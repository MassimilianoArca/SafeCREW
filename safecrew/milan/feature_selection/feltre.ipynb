{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid the verbose output of optuna optimization process\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"..\", \"utils\")\n",
    "\n",
    "data_folder = os.path.join(\"..\", \"..\", \"data\")\n",
    "clean_data_folder = os.path.join(data_folder, \"Clean Data\")\n",
    "metadata_folder = os.path.join(data_folder, \"Metadata\")\n",
    "plot_folder = os.path.join(data_folder, \"Plots\", \"Feltre\")\n",
    "\n",
    "sensor_folder = os.path.join(clean_data_folder, \"sensors\")\n",
    "\n",
    "feltre_sqlites_folder = 'feltre_sqlites_second'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part_df = pd.read_excel(os.path.join(clean_data_folder, 'Feltre', 'second_part.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variables = {\n",
    "    'ICC [1/mL]': 'ICC (1/mL)',\n",
    "    'HNAC [1/mL]': 'HNAC (1/mL)', \n",
    "    'LNAC [1/mL]': 'LNAC (1/mL)',\n",
    "    'HNAP [%]': 'HNAP (%)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = {\n",
    "    'Pressione [atm]': 'Pressione (atm)',\n",
    "    'TOCeq [mg/l]': 'TOCeq (mg/l)',\n",
    "    'DOCeq [mg/l]': 'DOCeq (mg/l)',\n",
    "    'Turbidity [FTU]': 'Turbidity (FTU)', \n",
    "    'Conductivity [uS/cm]': 'Conductivity (uS/cm)',\n",
    "    'Temperature [°C]': 'Temperature (°C)',\n",
    "    'pH': 'pH',\n",
    "    'Free Chlorine [mg/l]': 'Free Chlorine (mg/l)',\n",
    "    'Nitrate [mg/l]': 'Nitrate (mg/l)',\n",
    "    'UV254 [1/m]': 'UV254 (1/m)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part_df.rename(\n",
    "    columns=input_variables,\n",
    "    inplace=True\n",
    ")\n",
    "second_part_df.rename(\n",
    "    columns=target_variables,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "for target_variable in target_variables.values():\n",
    "    datasets[target_variable] = second_part_df[['DateTime', target_variable] + list(input_variables.values())].copy()\n",
    "    datasets[target_variable].set_index('DateTime', inplace=True)\n",
    "    datasets[target_variable].sort_index(inplace=True)\n",
    "    datasets[target_variable].dropna(inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# We are going to extend the features of the input variables for each target variable\n",
    "# -\n",
    "# We are going to add:\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "lags_in_hours = 3\n",
    "shifts_in_indexes = int(0.25 * 4 * lags_in_hours)\n",
    "rolling_window_in_hours = 6\n",
    "rolling_window = int(0.25 * 4 * rolling_window_in_hours)\n",
    "polyn_degree = 2\n",
    "\n",
    "ds = datasets.copy()\n",
    "lstm_datasets = {}\n",
    "\n",
    "for target_variable, df in datasets.items():\n",
    "    ds[target_variable] = df[list(input_variables.values())].copy(), df[target_variable].copy()\n",
    "    \n",
    "    X = ds[target_variable][0]\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "    \n",
    "    # uncomment based on the dataset you want to use\n",
    "    # X_extended = extend_features(X, lags_in_hours, rolling_window, polyn_degree)\n",
    "    X_extended = X\n",
    "    \n",
    "    y = ds[target_variable][1]\n",
    "    \n",
    "    # we are going to use the log1p of the target variable for the modelling to avoid instability\n",
    "    y = np.log1p(y)\n",
    "    \n",
    "    # need to change the name of target variable to avoid the / character\n",
    "    ds.pop(target_variable)\n",
    "    \n",
    "    target_variable = target_variable.replace(\"/\", \"_\")\n",
    "    \n",
    "    ds[target_variable] = X_extended, y\n",
    "    \n",
    "    # do not use the extended features for the LSTM model\n",
    "    lstm_datasets[target_variable] = X, y\n",
    "    \n",
    "datasets = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in datasets.items():\n",
    "    print(f\"Target variable: {target_variable}\")\n",
    "    # print number of nan values in X\n",
    "    print(f\"Number of nan values in X: {X.isna().sum().sum()}\")\n",
    "    # print number of nan values in y\n",
    "    print(f\"Number of nan values in y: {y.isna().sum().sum()}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "feature_combinations = []\n",
    "for i in range(1, len(input_variables.values()) + 1):\n",
    "    feature_combinations.extend(combinations(list(input_variables.values()), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_combinations = [list(comb) for comb in feature_combinations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "We are going to train different models:\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- QRNN\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in datasets.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    datasets[target_variable] = X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X_train, X_test, y_train, y_test) in datasets.items():\n",
    "    print(f\"Target variable: {target_variable}\")\n",
    "    # print number of nan values in X\n",
    "    print(f\"Number of nan values in X: {X_train.isna().sum().sum()}\")\n",
    "    # print number of nan values in y\n",
    "    print(f\"Number of nan values in y: {y_train.isna().sum().sum()}\")\n",
    "    print(\"-\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# perform mutual information regression to select the most important features\n",
    "target_variable = 'HNAC (1_mL)'\n",
    "X_train, X_test, y_train, y_test = datasets[target_variable]\n",
    "\n",
    "# perform mutual information regression to select the most important features\n",
    "mi_scores = mutual_info_regression(X_train, y_train)\n",
    "\n",
    "mi_scores = pd.Series(mi_scores, index=X_train.columns)\n",
    "\n",
    "mi_scores.sort_values(ascending=False).plot.bar(figsize=(10, 6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_folds)\n",
    "\n",
    "for target_variable, (X_train, X_test, y_train, y_test) in datasets.items():\n",
    "    X_cv = X_train.copy()\n",
    "    y_cv = y_train.copy()\n",
    "    \n",
    "    lr_results[target_variable] = {}\n",
    "    \n",
    "    for feature_combination in feature_combinations:\n",
    "        \n",
    "        X_comb = X_cv[feature_combination]\n",
    "\n",
    "        cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "        for i, (train_index, test_index) in enumerate(\n",
    "            cv.split(X_comb, y_cv)\n",
    "        ):\n",
    "            X_train_cv, X_test_cv = X_comb.iloc[train_index], X_comb.iloc[test_index]\n",
    "            y_train_cv, y_test_cv = y_cv.iloc[train_index], y_cv.iloc[test_index]\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train_cv, y_train_cv)\n",
    "            \n",
    "            y_pred_cv = model.predict(X_test_cv)\n",
    "            cv_rmse[i] = np.sqrt(mean_squared_error(y_test_cv, y_pred_cv)) \n",
    "        \n",
    "        lr_results[target_variable][str(feature_combination)] = {\n",
    "            \"mean_cv_rmse\": np.mean(cv_rmse),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_xgb_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = XGBRegressor(random_state=seed, **params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    if np.isnan(y_val).any() or np.isnan(y_val_pred).any():\n",
    "        print(f\"y_val: {y_val}\")\n",
    "        print(f\"y_val_pred: {y_val_pred}\")\n",
    "    return np.sqrt(mean_squared_error(y_val.values, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    eta = trial.suggest_float(\"eta\", 1e-5, 1, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1, log=True)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1, log=True)\n",
    "    learning_rate = trial.suggest_float(\n",
    "        \"learning_rate\", 1e-5, 1e-1, log=True\n",
    "    )\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 500)\n",
    "    updater = trial.suggest_categorical(\n",
    "        \"updater\", [\"shotgun\", \"coord_descent\"]\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gblinear\",\n",
    "        \"eta\": eta,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"updater\": updater,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "\n",
    "    cv = TimeSeriesSplit(n_splits=n_folds)\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_xgb_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            params,\n",
    "        )\n",
    "\n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "\n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_studies = {}\n",
    "\n",
    "for target_variable, (X_train, _, y_train, _) in datasets.items():\n",
    "    \n",
    "    if target_variable != \"HNAC (1_mL)\":\n",
    "        continue\n",
    "    \n",
    "    xgb_studies[target_variable] = {}\n",
    "    \n",
    "    for feature_combination in tqdm_notebook(feature_combinations, desc='Feature combination'):\n",
    "        \n",
    "        X_train_comb = X_train[feature_combination]\n",
    "        \n",
    "        path = f\"{feltre_sqlites_folder}/XGBoost - {target_variable}\" + str(feature_combination).replace('/', '_') + \".sqlite3\"\n",
    "        storage_path = f\"sqlite:///\" + path\n",
    "        study_name = \"Hyperparameter Tuning - XGBoost - \" + target_variable + str(feature_combination)\n",
    "\n",
    "        if os.path.exists(path):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=study_name,\n",
    "            storage=storage_path,\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=storage_path,\n",
    "                study_name=study_name,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            \n",
    "            print(f\"Optimizing XGBoost for {target_variable} with {feature_combination}\")\n",
    "            \n",
    "            study.optimize(lambda trial: objective(trial, X_train_comb, y_train), n_trials=100, show_progress_bar=False, )\n",
    "                \n",
    "        xgb_studies[target_variable][str(feature_combination)] = study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the studies\n",
    "best_studies = {}\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    \n",
    "    if cluster_name == 'cluster_1':\n",
    "        continue\n",
    "    \n",
    "    best_studies[cluster_name] = {}\n",
    "    \n",
    "    # get the best study for each model\n",
    "    pls_cluster = pls_studies[cluster_name]\n",
    "    pls_sorted = sorted(pls_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['pls'] = pls_sorted\n",
    "    \n",
    "    svr_cluster = svr_studies[cluster_name]\n",
    "    svr_sorted = sorted(svr_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['svr'] = svr_sorted\n",
    "    \n",
    "    qrnn_cluster = qrnn_studies[cluster_name]\n",
    "    qrnn_sorted = sorted(qrnn_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['qrnn'] = qrnn_sorted\n",
    "    \n",
    "    xgb_cluster = xgb_studies[cluster_name]\n",
    "    xgb_sorted = sorted(xgb_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['xgb'] = xgb_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the studies\n",
    "best_studies = {}\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    \n",
    "    if cluster_name == 'cluster_1':\n",
    "        continue\n",
    "    \n",
    "    best_studies[cluster_name] = {}\n",
    "    \n",
    "    # get the best study for each model\n",
    "    pls_cluster = pls_studies[cluster_name]\n",
    "    pls_sorted = sorted(pls_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['pls'] = pls_sorted\n",
    "    \n",
    "    svr_cluster = svr_studies[cluster_name]\n",
    "    svr_sorted = sorted(svr_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['svr'] = svr_sorted\n",
    "    \n",
    "    qrnn_cluster = qrnn_studies[cluster_name]\n",
    "    qrnn_sorted = sorted(qrnn_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['qrnn'] = qrnn_sorted\n",
    "    \n",
    "    xgb_cluster = xgb_studies[cluster_name]\n",
    "    xgb_sorted = sorted(xgb_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['xgb'] = xgb_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the studies\n",
    "best_studies = {}\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    \n",
    "    if cluster_name == 'cluster_1':\n",
    "        continue\n",
    "    \n",
    "    best_studies[cluster_name] = {}\n",
    "    \n",
    "    # get the best study for each model\n",
    "    pls_cluster = pls_studies[cluster_name]\n",
    "    pls_sorted = sorted(pls_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['pls'] = pls_sorted\n",
    "    \n",
    "    svr_cluster = svr_studies[cluster_name]\n",
    "    svr_sorted = sorted(svr_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['svr'] = svr_sorted\n",
    "    \n",
    "    qrnn_cluster = qrnn_studies[cluster_name]\n",
    "    qrnn_sorted = sorted(qrnn_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['qrnn'] = qrnn_sorted\n",
    "    \n",
    "    xgb_cluster = xgb_studies[cluster_name]\n",
    "    xgb_sorted = sorted(xgb_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['xgb'] = xgb_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the studies\n",
    "best_studies = {}\n",
    "\n",
    "for cluster_name in clusters.keys():\n",
    "    \n",
    "    if cluster_name == 'cluster_1':\n",
    "        continue\n",
    "    \n",
    "    best_studies[cluster_name] = {}\n",
    "    \n",
    "    # get the best study for each model\n",
    "    pls_cluster = pls_studies[cluster_name]\n",
    "    pls_sorted = sorted(pls_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['pls'] = pls_sorted\n",
    "    \n",
    "    svr_cluster = svr_studies[cluster_name]\n",
    "    svr_sorted = sorted(svr_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['svr'] = svr_sorted\n",
    "    \n",
    "    qrnn_cluster = qrnn_studies[cluster_name]\n",
    "    qrnn_sorted = sorted(qrnn_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['qrnn'] = qrnn_sorted\n",
    "    \n",
    "    xgb_cluster = xgb_studies[cluster_name]\n",
    "    xgb_sorted = sorted(xgb_cluster.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[cluster_name]['xgb'] = xgb_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_name in clusters.keys():\n",
    "    \n",
    "    if cluster_name == 'cluster_1':\n",
    "        continue\n",
    "    \n",
    "    print(f'Cluster {cluster_name}')\n",
    "    print('Best models:')\n",
    "    print('=='*50)\n",
    "    print('PLS')\n",
    "    print('=='*50)\n",
    "    # print the best 5 configurations for each model\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['pls'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['pls'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('SVR')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['svr'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['svr'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('XGB')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['xgb'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['xgb'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('QRNN')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['qrnn'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['qrnn'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cluster 0, the best model is XGBoost with the following variables:\n",
    "# ['Conductivity (uS/cm)', 'TOC (mg/L)', 'Temperature (°C)']\n",
    "\n",
    "# For cluster 1, the best model is XGBoost with the following variables:\n",
    "# ['Color (CU)', 'pH', 'Conductivity (uS/cm)', 'Temperature (°C)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_name in clusters.keys():\n",
    "    \n",
    "    if cluster_name == 'cluster_1':\n",
    "        continue\n",
    "    \n",
    "    print(f'Cluster {cluster_name}')\n",
    "    print('Best models:')\n",
    "    print('=='*50)\n",
    "    print('PLS')\n",
    "    print('=='*50)\n",
    "    # print the best 5 configurations for each model\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['pls'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['pls'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('SVR')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['svr'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['svr'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('XGB')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['xgb'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['xgb'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('QRNN')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['qrnn'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['qrnn'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cluster 0, the best model is XGBoost with the following variables:\n",
    "# ['Conductivity (uS/cm)', 'TOC (mg/L)', 'Temperature (°C)']\n",
    "\n",
    "# For cluster 1, the best model is XGBoost with the following variables:\n",
    "# ['Color (CU)', 'pH', 'Conductivity (uS/cm)', 'Temperature (°C)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_name in clusters.keys():\n",
    "    \n",
    "    if cluster_name == 'cluster_1':\n",
    "        continue\n",
    "    \n",
    "    print(f'Cluster {cluster_name}')\n",
    "    print('Best models:')\n",
    "    print('=='*50)\n",
    "    print('PLS')\n",
    "    print('=='*50)\n",
    "    # print the best 5 configurations for each model\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['pls'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['pls'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('SVR')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['svr'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['svr'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('XGB')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['xgb'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['xgb'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('QRNN')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['qrnn'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['qrnn'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cluster 0, the best model is XGBoost with the following variables:\n",
    "# ['Conductivity (uS/cm)', 'TOC (mg/L)', 'Temperature (°C)']\n",
    "\n",
    "# For cluster 1, the best model is XGBoost with the following variables:\n",
    "# ['Color (CU)', 'pH', 'Conductivity (uS/cm)', 'Temperature (°C)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_name in clusters.keys():\n",
    "    \n",
    "    if cluster_name == 'cluster_1':\n",
    "        continue\n",
    "    \n",
    "    print(f'Cluster {cluster_name}')\n",
    "    print('Best models:')\n",
    "    print('=='*50)\n",
    "    print('PLS')\n",
    "    print('=='*50)\n",
    "    # print the best 5 configurations for each model\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['pls'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['pls'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('SVR')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['svr'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['svr'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('XGB')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['xgb'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['xgb'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('QRNN')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[cluster_name]['qrnn'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[cluster_name]['qrnn'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cluster 0, the best model is XGBoost with the following variables:\n",
    "# ['Conductivity (uS/cm)', 'TOC (mg/L)', 'Temperature (°C)']\n",
    "\n",
    "# For cluster 1, the best model is XGBoost with the following variables:\n",
    "# ['Color (CU)', 'pH', 'Conductivity (uS/cm)', 'Temperature (°C)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_lgbm_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=seed,\n",
    "        linear_tree=True,\n",
    "    )\n",
    "\n",
    "    if params is not None:\n",
    "        model.set_params(**params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val.values, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_estimators\": trial.suggest_int(\n",
    "            \"n_estimators\", 1, 20, step=1\n",
    "        ),\n",
    "        \"learning_rate\": trial.suggest_float(\n",
    "            \"learning_rate\", 1e-5, 1e-1, log=True\n",
    "        ),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 16, step=1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 20, step=1),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\n",
    "            \"min_data_in_leaf\", 2, 50, step=1\n",
    "        ),\n",
    "        \"lambda_l1\": trial.suggest_float(\n",
    "            \"lambda_l1\", 1e-3, 10, log=True\n",
    "        ),\n",
    "        \"lambda_l2\": trial.suggest_float(\n",
    "            \"lambda_l2\", 1e-3, 10, log=True\n",
    "        ),\n",
    "        \"min_split_gain\": trial.suggest_float(\n",
    "            \"min_split_gain\", 0, 15, step=0.5\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"min_child_samples\": trial.suggest_int(\n",
    "            \"min_child_samples\", 20, 1000, log=True\n",
    "        ),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 10, 500, step=10),\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_lgbm_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_studies = {}\n",
    "\n",
    "for target_variable, (X_train, _, y_train, _) in datasets.items():\n",
    "    \n",
    "    if target_variable != \"HNAC (1_mL)\":\n",
    "        continue\n",
    "    \n",
    "    lgbm_studies[target_variable] = {}\n",
    "    \n",
    "    for feature_combination in tqdm_notebook(feature_combinations, desc='Feature combination'):\n",
    "        \n",
    "        X_train_comb = X_train[feature_combination]\n",
    "        \n",
    "        path = f\"{feltre_sqlites_folder}/LGBM - {target_variable}\" + str(feature_combination).replace('/', '_') + \".sqlite3\"\n",
    "        storage_path = f\"sqlite:///\" + path\n",
    "        study_name = \"Hyperparameter Tuning - LGBM - \" + target_variable + str(feature_combination)\n",
    "\n",
    "        if os.path.exists(path):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=study_name,\n",
    "            storage=storage_path,\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=storage_path,\n",
    "                study_name=study_name,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            \n",
    "            print(f\"Optimizing LGBM for {target_variable} with {feature_combination}\")\n",
    "            \n",
    "            study.optimize(lambda trial: objective(trial, X_train_comb, y_train), n_trials=100, show_progress_bar=False, )\n",
    "                \n",
    "        lgbm_studies[target_variable][str(feature_combination)] = study  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantnn.qrnn import QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.linspace(0.01, 0.99, 50)\n",
    "\n",
    "def fit_and_validate_qrnn_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index].to_numpy(), X.iloc[val_index].to_numpy()\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    n_layers = params[\"n_layers\"]\n",
    "    n_units = params[\"n_units\"]\n",
    "    activation = params[\"activation\"]\n",
    "\n",
    "    model = QRNN(\n",
    "        n_inputs=X_tr.shape[1],\n",
    "        quantiles=quantiles,\n",
    "        model=(n_layers, n_units, activation),\n",
    "    )\n",
    "    \n",
    "    n_epochs = 50\n",
    "    optimizer = torch.optim.AdamW(model.model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "    \n",
    "    model.train(\n",
    "        training_data=(X_tr, np.array(y_tr)),\n",
    "        validation_data=(X_val, np.array(y_val)),\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        n_epochs=n_epochs,\n",
    "        device=\"cpu\",\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        logger=None,\n",
    "        \n",
    "        \n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model.predict(X_val).numpy()\n",
    "    \n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val.values, y_val_pred.mean(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\n",
    "    \"elu\",\n",
    "    \"hardshrink\",\n",
    "    \"hardtanh\",\n",
    "    \"prelu\",\n",
    "    \"relu\",\n",
    "    \"selu\",\n",
    "    \"celu\",\n",
    "    \"sigmoid\",\n",
    "    \"softplus\",\n",
    "    \"softmin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"n_layers\": trial.suggest_int(\"n_layers\", 1, 3),\n",
    "        \"n_units\": trial.suggest_int(\"n_units\", 32, 512, log=True),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", activations),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "    }\n",
    "\n",
    "    cv = TimeSeriesSplit(n_splits=n_folds)\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_qrnn_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrnn_studies = {}\n",
    "\n",
    "for target_variable, (X_train, _, y_train, _) in datasets.items():\n",
    "    \n",
    "    if target_variable != \"HNAC (1_mL)\":\n",
    "        continue\n",
    "    \n",
    "    qrnn_studies[target_variable] = {}\n",
    "    \n",
    "    for feature_combination in tqdm_notebook(feature_combinations, desc='Feature combination'):\n",
    "        \n",
    "        X_train_comb = X_train[feature_combination]\n",
    "        \n",
    "        path = f\"{feltre_sqlites_folder}/QRNN - {target_variable}\" + str(feature_combination).replace('/', '_') + \".sqlite3\"\n",
    "        storage_path = f\"sqlite:///\" + path\n",
    "        study_name = \"Hyperparameter Tuning - QRNN - \" + target_variable + str(feature_combination)\n",
    "    \n",
    "        if os.path.exists(path):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=study_name,\n",
    "            storage=storage_path,\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=storage_path,\n",
    "                study_name=study_name,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            \n",
    "            print(f\"Optimizing QRNN for {target_variable} with {feature_combination}\")\n",
    "            \n",
    "            study.optimize(lambda trial: objective(trial, X_train_comb, y_train), n_trials=100, show_progress_bar=False, )\n",
    "        \n",
    "    qrnn_studies[target_variable][str(feature_combination)] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the LSTM model takes as input a tensor of shape (num_samples, time_steps, n_features)\n",
    "# we need to convert the pandas dataframe into a numpy array of shape (num_samples, time_steps, n_features)\n",
    "# each sample is a sequence of window_size time steps, containing the features and the target variable\n",
    "def create_sequences(X_df, y_df, window_size):\n",
    "    \"\"\"\n",
    "    Converts Pandas DataFrames into overlapping sequences for LSTM input.\n",
    "    \n",
    "    Returns:\n",
    "        X_seq: NumPy array of shape (num_samples - window_size, window_size, n_features)\n",
    "        y_seq: NumPy array of shape (num_samples - window_size, 1) with the last target value of each window\n",
    "        y_timestamps: List of timestamps corresponding to the predictions.\n",
    "    \"\"\"\n",
    "    timesteps = X_df.index\n",
    "    \n",
    "    X_values = X_df.to_numpy()\n",
    "    y_values = y_df.to_numpy()\n",
    "    \n",
    "    X_seq, y_seq, y_timestamps = [], [], []\n",
    "    \n",
    "    # Create sequences for X and corresponding y for only the last value of each window\n",
    "    for i in range(len(X_values) - window_size):\n",
    "        X_seq.append(X_values[i : i + window_size])  # Input sequence\n",
    "        y_seq.append(y_values[i + window_size - 1])  # Only the last value in the target window\n",
    "        y_timestamps.append(timesteps[i + window_size - 1])  # Timestamp for the last timestep\n",
    "        \n",
    "    return np.array(X_seq), np.array(y_seq), np.array(y_timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_lstm_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X[train_index], X[val_index]\n",
    "    y_tr, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(params[\"window_size\"], X_tr.shape[-1])))\n",
    "    model.add(LSTM(units=params[\"n_units_1\"], return_sequences=False, seed=seed))\n",
    "    model.add(Dropout(params[\"dropout_1\"], seed=seed))\n",
    "    # model.add(LSTM(units=params[\"n_units_2\"], return_sequences=False, seed=seed))\n",
    "    # model.add(Dropout(params[\"dropout_2\"], seed=seed))\n",
    "    model.add(Dense(params[\"n_neurons\"]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params[\"learning_rate\"]),\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError()],\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "    \n",
    "    _ = model.fit(X_tr, y_tr, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0, batch_size=params[\"batch_size\"])\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.squeeze(y_val_pred)\n",
    "    \n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_units_1\": trial.suggest_int(\"n_units_1\", low=16, high=64, step=1),\n",
    "        # \"n_units_2\": trial.suggest_int(\"n_units_2\", low=16, high=64, step=1),\n",
    "        \"n_neurons\": trial.suggest_int(\"n_neurons\", low=16, high=64, step=1),        \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-1),\n",
    "        \"window_size\": trial.suggest_int(\"window_size\", 1, 24, step=1),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128, 256]),\n",
    "        \"dropout_1\": trial.suggest_float(\"dropout_1\", 0.1, 0.5),\n",
    "        # \"dropout_2\": trial.suggest_float(\"dropout_2\", 0.1, 0.5),\n",
    "    }\n",
    "    \n",
    "    window_size = config[\"window_size\"]\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(X_cv, y_cv, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, _ = create_sequences(X_train, y_train, window_size)\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_train_seq, y_train_seq)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_lstm_model(\n",
    "            X_train_seq,\n",
    "            y_train_seq,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_studies = {}\n",
    "\n",
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    lstm_studies[target_variable] = {}\n",
    "    \n",
    "    for feature_combination in tqdm_notebook(feature_combinations, desc='Feature combination'):\n",
    "        \n",
    "        X_train_comb = X[feature_combination]\n",
    "        \n",
    "        path = f\"{feltre_sqlites_folder}/LSTM - {target_variable}\" + str(feature_combination).replace('/', '_') + \".sqlite3\"\n",
    "        storage_path = f\"sqlite:///\" + path\n",
    "        study_name = \"Hyperparameter Tuning - LSTM - \" + target_variable + str(feature_combination)\n",
    "        if os.path.exists(path):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=study_name,\n",
    "            storage=storage_path,\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=storage_path,\n",
    "                study_name=study_name,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            \n",
    "            print(f\"Optimizing LSTM for {target_variable} with {feature_combination}\")\n",
    "            study.optimize(lambda trial: objective(trial, X_train_comb, y_train), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        lstm_studies[target_variable][str(feature_combination)] = study  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare studies results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the studies\n",
    "best_studies = {}\n",
    "\n",
    "for target_variable in datasets.keys():\n",
    "    \n",
    "    if target_variable == 'HNAC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    best_studies[target_variable] = {}\n",
    "    \n",
    "    # get the best study for each model\n",
    "    xgb_target_variable = xgb_studies[target_variable]\n",
    "    xgb_sorted = sorted(xgb_target_variable.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[target_variable]['xgb'] = xgb_sorted\n",
    "    \n",
    "    lgbm_target_variable = lgbm_studies[target_variable]\n",
    "    lgbm_sorted = sorted(lgbm_target_variable.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[target_variable]['lgbm'] = lgbm_sorted\n",
    "    \n",
    "    qrnn_target_variable = qrnn_studies[target_variable]\n",
    "    qrnn_sorted = sorted(qrnn_target_variable.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[target_variable]['qrnn'] = qrnn_sorted\n",
    "    \n",
    "    lstm_target_variable = lstm_studies[target_variable]\n",
    "    lstm_sorted = sorted(lstm_target_variable.items(), key=lambda x: x[1].best_value)\n",
    "    best_studies[target_variable]['lstm'] = lstm_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable in datasets.keys():\n",
    "    \n",
    "    if target_variable == 'HNAC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    print(f'Target Variable: {target_variable}')\n",
    "    print('Best models:')\n",
    "    print('=='*50)\n",
    "    print('XGB')\n",
    "    print('=='*50)\n",
    "    # print the best 5 configurations for each model\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[target_variable]['xgb'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[target_variable]['xgb'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('LGBM')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[target_variable]['lgbm'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[target_variable]['lgbm'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('QRNN')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[target_variable]['qrnn'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[target_variable]['qrnn'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)\n",
    "    print('LSTM')\n",
    "    print('=='*50)\n",
    "    for i in range(5):\n",
    "        print('Parameters:', best_studies[target_variable]['lstm'][i][0])\n",
    "        print('Best Value:', np.round(best_studies[target_variable]['lstm'][i][1].best_value, 3))\n",
    "        print()\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HNAC (1_mL), the best model is LSTM with the following variables:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'LSTM' : {},\n",
    "    'XGBoost': {},\n",
    "    'LGBM': {},\n",
    "    'QRNN': {},\n",
    "    'GRU': {},\n",
    "    'BI_LSTM': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    # ==== LSTM ====\n",
    "    \n",
    "    predictions['LSTM'][target_variable] = {}\n",
    "    \n",
    "    window_size = lstm_studies[target_variable].best_trial.params[\"window_size\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, timesteps_train = create_sequences(X_train, y_train, window_size)\n",
    "    X_test_seq, y_test_seq, timesteps_test = create_sequences(X_test, y_test, window_size)\n",
    "    \n",
    "\n",
    "    n_units_1 = lstm_studies[target_variable].best_trial.params[\"n_units_1\"]\n",
    "    n_neurons = lstm_studies[target_variable].best_trial.params[\"n_neurons\"]\n",
    "    dropout_1 = lstm_studies[target_variable].best_trial.params[\"dropout_1\"]\n",
    "    learning_rate = lstm_studies[target_variable].best_trial.params[\"learning_rate\"]\n",
    "    batch_size = lstm_studies[target_variable].best_trial.params[\"batch_size\"] \n",
    "    n_units_2 = lstm_studies[target_variable].best_trial.params[\"n_units_2\"]\n",
    "    dropout_2 = lstm_studies[target_variable].best_trial.params[\"dropout_2\"]\n",
    "    \n",
    "    # fit the model 50 times to get a better estimate of the predictions and the uncertainty\n",
    "    n_iterations = 50\n",
    "    \n",
    "    y_pred_list = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(window_size, X_train_seq.shape[-1])))\n",
    "        model.add(LSTM(units=n_units_1, return_sequences=True, seed=seed))\n",
    "        model.add(Dropout(dropout_1, seed=seed))\n",
    "        model.add(LSTM(units=n_units_2, return_sequences=False, seed=seed))\n",
    "        model.add(Dropout(dropout_2, seed=seed))\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=MeanSquaredError(),\n",
    "            metrics=[RootMeanSquaredError()],\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        _ = model.fit(X_train_seq, y_train_seq, epochs=50, callbacks=[early_stopping], verbose=0, batch_size=batch_size)\n",
    "        \n",
    "        # Warm-up the model\n",
    "        warm_up_pred = model.predict(X_train_seq[-window_size - 1:])\n",
    "        warm_up_pred = np.squeeze(warm_up_pred)\n",
    "        \n",
    "        y_pred = model.predict(X_test_seq)\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "\n",
    "        # concatenate the warm-up predictions with the test predictions\n",
    "        y_pred = np.concatenate([warm_up_pred, y_pred])\n",
    "        \n",
    "        y_pred_list.append(y_pred)\n",
    "    \n",
    "    # get a timesteps_test as a one-dimensional array with no duplicates\n",
    "    timesteps_test = np.unique(timesteps_test)\n",
    "    timesteps_train = np.unique(timesteps_train)\n",
    "\n",
    "    predictions['LSTM'][target_variable][\"timesteps_test\"] = timesteps_test\n",
    "    predictions['LSTM'][target_variable][\"timesteps_train\"] = timesteps_train\n",
    "    predictions['LSTM'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['LSTM'][target_variable][\"y_train\"] = y_train\n",
    "    \n",
    "    mean_pred = np.mean(y_pred_list, axis=0)\n",
    "    std_pred = np.std(y_pred_list, axis=0)\n",
    "    \n",
    "    predictions['LSTM'][target_variable][\"mean_pred\"] = mean_pred\n",
    "    predictions['LSTM'][target_variable][\"std_pred\"] = std_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    # ==== GRU ====\n",
    "    \n",
    "    predictions['GRU'][target_variable] = {}\n",
    "    \n",
    "    window_size = lstm_studies[target_variable].best_trial.params[\"window_size\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, timesteps_train = create_sequences(X_train, y_train, window_size)\n",
    "    X_test_seq, y_test_seq, timesteps_test = create_sequences(X_test, y_test, window_size)\n",
    "    \n",
    "\n",
    "    n_units_1 = gru_studies[target_variable].best_trial.params[\"n_units_1\"]\n",
    "    n_neurons = gru_studies[target_variable].best_trial.params[\"n_neurons\"]\n",
    "    dropout_1 = gru_studies[target_variable].best_trial.params[\"dropout_1\"]\n",
    "    learning_rate = gru_studies[target_variable].best_trial.params[\"learning_rate\"]\n",
    "    batch_size = gru_studies[target_variable].best_trial.params[\"batch_size\"] \n",
    "    \n",
    "    # fit the model 50 times to get a better estimate of the predictions and the uncertainty\n",
    "    n_iterations = 50\n",
    "    \n",
    "    y_pred_list = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(window_size, X_train_seq.shape[-1])))\n",
    "        model.add(GRU(units=n_units_1, return_sequences=False, seed=42))\n",
    "        model.add(Dropout(dropout_1))\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=MeanSquaredError(),\n",
    "            metrics=[RootMeanSquaredError()],\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        _ = model.fit(X_train_seq, y_train_seq, epochs=50, callbacks=[early_stopping], verbose=0, batch_size=batch_size)\n",
    "        \n",
    "        # Warm-up the model\n",
    "        warm_up_pred = model.predict(X_train_seq[-window_size - 1:])\n",
    "        warm_up_pred = np.squeeze(warm_up_pred)\n",
    "        \n",
    "        y_pred = model.predict(X_test_seq)\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "        \n",
    "        # concatenate the warm-up predictions with the test predictions\n",
    "        y_pred = np.concatenate([warm_up_pred, y_pred])\n",
    "        \n",
    "        y_pred_list.append(y_pred)\n",
    "    \n",
    "    # get a timesteps_test as a one-dimensional array with no duplicates\n",
    "    timesteps_test = np.unique(timesteps_test)\n",
    "    timesteps_train = np.unique(timesteps_train)\n",
    "\n",
    "    predictions['GRU'][target_variable][\"timesteps_test\"] = timesteps_test\n",
    "    predictions['GRU'][target_variable][\"timesteps_train\"] = timesteps_train\n",
    "    predictions['GRU'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['GRU'][target_variable][\"y_train\"] = y_train\n",
    "    \n",
    "    mean_pred = np.mean(y_pred_list, axis=0)\n",
    "    std_pred = np.std(y_pred_list, axis=0)\n",
    "    \n",
    "    predictions['GRU'][target_variable][\"mean_pred\"] = mean_pred\n",
    "    predictions['GRU'][target_variable][\"std_pred\"] = std_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    # ==== BIDIRECTIONAL LSTM ====\n",
    "    \n",
    "    predictions['BI_LSTM'][target_variable] = {}\n",
    "    \n",
    "    window_size = bi_lstm_studies[target_variable].best_trial.params[\"window_size\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, timesteps_train = create_sequences(X_train, y_train, window_size)\n",
    "    X_test_seq, y_test_seq, timesteps_test = create_sequences(X_test, y_test, window_size)\n",
    "    \n",
    "\n",
    "    n_units_1 = bi_lstm_studies[target_variable].best_trial.params[\"n_units_1\"]\n",
    "    n_neurons = bi_lstm_studies[target_variable].best_trial.params[\"n_neurons\"]\n",
    "    dropout_1 = bi_lstm_studies[target_variable].best_trial.params[\"dropout_1\"]\n",
    "    learning_rate = bi_lstm_studies[target_variable].best_trial.params[\"learning_rate\"]\n",
    "    batch_size = bi_lstm_studies[target_variable].best_trial.params[\"batch_size\"] \n",
    "    \n",
    "    # fit the model 50 times to get a better estimate of the predictions and the uncertainty\n",
    "    n_iterations = 50\n",
    "    \n",
    "    y_pred_list = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(window_size, X_train_seq.shape[-1])))\n",
    "        model.add(Bidirectional(LSTM(units=n_units_1, return_sequences=False, seed=42)))\n",
    "        model.add(Dropout(dropout_1))\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=MeanSquaredError(),\n",
    "            metrics=[RootMeanSquaredError()],\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        _ = model.fit(X_train_seq, y_train_seq, epochs=50, callbacks=[early_stopping], verbose=0, batch_size=batch_size)\n",
    "        \n",
    "        # Warm-up the model\n",
    "        warm_up_pred = model.predict(X_train_seq[-window_size - 1:])\n",
    "        warm_up_pred = np.squeeze(warm_up_pred)\n",
    "        \n",
    "        y_pred = model.predict(X_test_seq)\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "        \n",
    "        # concatenate the warm-up predictions with the test predictions\n",
    "        y_pred = np.concatenate([warm_up_pred, y_pred])\n",
    "        \n",
    "        y_pred_list.append(y_pred)\n",
    "    \n",
    "    # get a timesteps_test as a one-dimensional array with no duplicates\n",
    "    timesteps_test = np.unique(timesteps_test)\n",
    "    timesteps_train = np.unique(timesteps_train)\n",
    "\n",
    "    predictions['BI_LSTM'][target_variable][\"timesteps_test\"] = timesteps_test\n",
    "    predictions['BI_LSTM'][target_variable][\"timesteps_train\"] = timesteps_train\n",
    "    predictions['BI_LSTM'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['BI_LSTM'][target_variable][\"y_train\"] = y_train\n",
    "    \n",
    "    mean_pred = np.mean(y_pred_list, axis=0)\n",
    "    std_pred = np.std(y_pred_list, axis=0)\n",
    "    \n",
    "    predictions['BI_LSTM'][target_variable][\"mean_pred\"] = mean_pred\n",
    "    predictions['BI_LSTM'][target_variable][\"std_pred\"] = std_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM PLOTS\n",
    "\n",
    "for target_variable in lstm_datasets.keys():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    timesteps_test = predictions['LSTM'][target_variable][\"timesteps_test\"]\n",
    "    timesteps_train = predictions['LSTM'][target_variable][\"timesteps_train\"]\n",
    "    y_train = predictions['LSTM'][target_variable][\"y_train\"]\n",
    "    y_test = predictions['LSTM'][target_variable][\"y_test\"]\n",
    "    \n",
    "    \n",
    "    y_pred_lstm = predictions['LSTM'][target_variable][\"mean_pred\"]\n",
    "    std_pred_lstm = predictions['LSTM'][target_variable][\"std_pred\"]    \n",
    "    \n",
    "    # y_pred_gru = predictions['GRU'][target_variable][\"mean_pred\"]\n",
    "    # std_pred_gru = predictions['GRU'][target_variable][\"std_pred\"]\n",
    "    \n",
    "    # y_pred_bi_lstm = predictions['BI_LSTM'][target_variable][\"mean_pred\"]\n",
    "    # std_pred_bi_lstm = predictions['BI_LSTM'][target_variable][\"std_pred\"]\n",
    "    \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_train,\n",
    "        y=np.expm1(y_train), \n",
    "        mode='lines',\n",
    "        name='True',\n",
    "        line=dict(color='blue'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_test),\n",
    "        mode='lines',\n",
    "        name='True',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm),\n",
    "        mode='lines',\n",
    "        name='LSTM',\n",
    "        line=dict(color='red')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        name='Upper Bound',\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm + 1.96 * std_pred_lstm),\n",
    "        mode='lines',\n",
    "        line=dict(width=0),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        name='Lower Bound',\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm - 1.96 * std_pred_lstm),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        fillcolor='rgba(255, 102, 102, 0.3)',  # light red color\n",
    "        fill='tonexty',\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter\n",
    "    # (\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_gru),\n",
    "    #     mode='lines',\n",
    "    #     name='GRU',\n",
    "    #     line=dict(color='green')\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Upper Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_gru + 1.96 * std_pred_gru),\n",
    "    #     mode='lines',\n",
    "    #     line=dict(width=0),\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Lower Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_gru - 1.96 * std_pred_gru),\n",
    "    #     line=dict(width=0),\n",
    "    #     mode='lines',\n",
    "    #     fillcolor='rgba(102, 255, 102, 0.3)',  # light green color\n",
    "    #     fill='tonexty',\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter\n",
    "    # (\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_bi_lstm),\n",
    "    #     mode='lines',\n",
    "    #     name='BI LSTM',\n",
    "    #     line=dict(color='orange')\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Upper Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_bi_lstm + 1.96 * std_pred_bi_lstm),\n",
    "    #     mode='lines',\n",
    "    #     line=dict(width=0),\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Lower Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_bi_lstm - 1.96 * std_pred_bi_lstm),\n",
    "    #     line=dict(width=0),\n",
    "    #     mode='lines',\n",
    "    #     fillcolor='rgba(255, 204, 102, 0.3)',  # light orange color\n",
    "    #     fill='tonexty',\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "\n",
    "    target_variable_name = f\"{target_variable.replace('_', '/')}\"\n",
    "    \n",
    "    # fig.update_yaxes(type=\"log\")\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f\"{target_variable_name}\",\n",
    "            'y':0.98,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=target_variable_name,\n",
    "        margin=dict(l=0, r=10, t=30, b=0),\n",
    "        font=dict(\n",
    "            size=14,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # put the legend at the top\n",
    "    fig.update_layout(legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    ))\n",
    "    \n",
    "    # fig.write_image(os.path.join(plot_folder, f\"LSTM - {target_variable}.png\"), scale=3)\n",
    "    \n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHER MODELS\n",
    "\n",
    "for target_variable, _ in lstm_datasets.items():\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = datasets[target_variable]\n",
    "    \n",
    "    # ==== XGBoost ====\n",
    "    \n",
    "    predictions['XGBoost'][target_variable] = {}\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gblinear\",\n",
    "        \"eta\": xgb_studies[target_variable].best_trial.params[\"eta\"],\n",
    "        \"reg_lambda\": xgb_studies[target_variable].best_trial.params[\"reg_lambda\"],\n",
    "        \"reg_alpha\": xgb_studies[target_variable].best_trial.params[\"reg_alpha\"],\n",
    "        \"learning_rate\": xgb_studies[target_variable].best_trial.params[\"learning_rate\"],\n",
    "        \"updater\": xgb_studies[target_variable].best_trial.params[\"updater\"],\n",
    "        \"n_estimators\": xgb_studies[target_variable].best_trial.params[\"n_estimators\"],\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(random_state=seed, **params)\n",
    "    \n",
    "    _ = model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions['XGBoost'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['XGBoost'][target_variable][\"y_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, _ in lstm_datasets.items():\n",
    "# ==== LGBM ====\n",
    "        \n",
    "    predictions['LGBM'][target_variable] = {}\n",
    "    \n",
    "    config = {\n",
    "        \"n_estimators\": lgbm_studies[target_variable].best_trial.params[\"n_estimators\"],\n",
    "        \"learning_rate\": lgbm_studies[target_variable].best_trial.params[\"learning_rate\"],\n",
    "        \"max_depth\": lgbm_studies[target_variable].best_trial.params[\"max_depth\"],\n",
    "        \"num_leaves\": lgbm_studies[target_variable].best_trial.params[\"num_leaves\"],\n",
    "        \"min_data_in_leaf\": lgbm_studies[target_variable].best_trial.params[\"min_data_in_leaf\"],\n",
    "        \"lambda_l1\": lgbm_studies[target_variable].best_trial.params[\"lambda_l1\"],\n",
    "        \"lambda_l2\": lgbm_studies[target_variable].best_trial.params[\"lambda_l2\"],\n",
    "        \"min_split_gain\": lgbm_studies[target_variable].best_trial.params[\"min_split_gain\"],\n",
    "        \"subsample\": lgbm_studies[target_variable].best_trial.params[\"subsample\"],\n",
    "        \"bagging_fraction\": lgbm_studies[target_variable].best_trial.params[\"bagging_fraction\"],\n",
    "        \"feature_fraction\": lgbm_studies[target_variable].best_trial.params[\"feature_fraction\"],\n",
    "        \"min_child_samples\": lgbm_studies[target_variable].best_trial.params[\"min_child_samples\"],\n",
    "        \"max_bin\": lgbm_studies[target_variable].best_trial.params[\"max_bin\"],\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=seed,\n",
    "        linear_tree=True,\n",
    "    )\n",
    "    \n",
    "    model.set_params(**config)\n",
    "    \n",
    "    _ = model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    predictions['LGBM'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['LGBM'][target_variable][\"y_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, _ in lstm_datasets.items():\n",
    "# ==== QRNN ====\n",
    "    \n",
    "    predictions['QRNN'][target_variable] = {}\n",
    "    \n",
    "    config = {\n",
    "        \"n_layers\": qrnn_studies[target_variable].best_trial.params[\"n_layers\"],\n",
    "        \"n_units\": qrnn_studies[target_variable].best_trial.params[\"n_units\"],\n",
    "        \"activation\": qrnn_studies[target_variable].best_trial.params[\"activation\"],\n",
    "        \"batch_size\": qrnn_studies[target_variable].best_trial.params[\"batch_size\"],\n",
    "    }\n",
    "    \n",
    "    model = QRNN(\n",
    "        n_inputs=X_train.shape[1],\n",
    "        quantiles=[0.05, 0.5, 0.95],\n",
    "        model=(config[\"n_layers\"], config[\"n_units\"], config[\"activation\"]),\n",
    "    )\n",
    "    \n",
    "    n_epochs = 50\n",
    "    optimizer = torch.optim.AdamW(model.model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "    \n",
    "    model.train(\n",
    "        training_data=(X_train.to_numpy(), np.array(y_train)),\n",
    "        validation_data=(X_test.to_numpy(), np.array(y_test)),\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        n_epochs=n_epochs,\n",
    "        device=\"cpu\",\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        logger=None,\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model.predict(X_test.to_numpy()).numpy()\n",
    "        \n",
    "        \n",
    "    predictions['QRNN'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['QRNN'][target_variable][\"y_pred_median\"] = y_pred[:, 1]\n",
    "    predictions['QRNN'][target_variable][\"y_pred_lower\"] = y_pred[:, 0]\n",
    "    predictions['QRNN'][target_variable][\"y_pred_upper\"] = y_pred[:, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL MODELS PLOTS\n",
    "\n",
    "# plot the predictions for each target variable\n",
    "for target_variable in datasets.keys():\n",
    "    \n",
    "    fig = go.Figure()\n",
    "        \n",
    "    fig.add_trace(go.Scatter(x=predictions['LGBM'][target_variable][\"y_test\"].index, y=predictions['LGBM'][target_variable][\"y_test\"], mode='lines', name='True'))\n",
    "        \n",
    "        \n",
    "    for model in predictions.keys():\n",
    "        \n",
    "        if model == 'QRNN':\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred_median\"], mode='lines', name='QRNN Predicted'))\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred_lower\"], mode='lines', name='Lower Bound'))\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred_upper\"], mode='lines', name='Upper Bound'))\n",
    "        \n",
    "        if model == 'LGBM':\n",
    "            fig.add_trace(go.Scatter\n",
    "                        (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred\"], mode='lines', name='LGBM Predicted'))\n",
    "            \n",
    "        if model == 'XGBoost':\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred\"], mode='lines', name='XGBoost Predicted'))\n",
    "            \n",
    "        if model == 'LSTM':\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"timesteps_test\"], y=predictions[model][target_variable][\"y_pred\"], mode='lines', name='LSTM Predicted'))\n",
    "            \n",
    "    fig.update_layout(title=f'{target_variable}', xaxis_title='Date', yaxis_title=target_variable)\n",
    "    fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
