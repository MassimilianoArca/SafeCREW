{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Points (Case dell'Acqua) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.path.join(\"..\", \"..\", \"data\"))\n",
    "raw_data_folder = os.path.join(data_folder, \"Raw Data\")\n",
    "intermediate_data_folder = os.path.join(data_folder, \"Intermediate Data\")\n",
    "metadata_folder = os.path.join(data_folder, \"Metadata\")\n",
    "\n",
    "reunion_folder = os.path.join(raw_data_folder, \"Riunione 24-04-2024\")\n",
    "plot_folder = os.path.join(data_folder, \"Plots\")\n",
    "sensor_folder = os.path.join(reunion_folder, \"Sensori\")\n",
    "\n",
    "clean_data_folder = os.path.join(data_folder, \"Clean Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df = pd.read_excel(\n",
    "    os.path.join(reunion_folder, \"Dati_case_Lab_Ingressi-Uscite.xlsx\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix LOD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(reunion_folder, \"columns_types.json\")) as f:\n",
    "    column_types = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns = column_types[\"metadata_columns\"]\n",
    "features_columns = column_types[\"features_columns\"]\n",
    "targets_columns = column_types[\"targets_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns = list(set(metadata_columns) & set(grab_samples_df.columns))\n",
    "features_columns = list(set(features_columns) & set(grab_samples_df.columns))\n",
    "targets_columns = list(set(targets_columns) & set(grab_samples_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def convert_string_values(s):\n",
    "    if isinstance(s, (int, float)):\n",
    "        return s\n",
    "    elif pd.isna(s):\n",
    "        return None\n",
    "    else:\n",
    "        if \",\" in s:\n",
    "            s = s.replace(\",\", \".\")\n",
    "        if \"<\" in s:\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) / 2 if number else None\n",
    "        elif \">\" in s:\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) if number else None\n",
    "        elif \"*\" in s or re.search(\"[a-zA-Z]\", s):\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) if number else None\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_label(value):\n",
    "    if pd.isna(value):\n",
    "        return \"NaN\"\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return \"Normal\"\n",
    "    elif \"<\" in value:\n",
    "        return \"Less than\"\n",
    "    elif \">\" in value:\n",
    "        return \"Greater than\"\n",
    "    else:\n",
    "        return \"NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all value columns in the mapping to the corresponding key column\n",
    "column_mapping = {\n",
    "    \"Concentrazione ioni idrogeno (unità pH)\": [\n",
    "        \"Concentr. ioni idrogeno (al prelievo) (unità pH)\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "for final_column, original_columns in column_mapping.items():\n",
    "    for original_column in original_columns:\n",
    "        grab_samples_df[final_column] = grab_samples_df[\n",
    "            final_column\n",
    "        ].combine_first(grab_samples_df[original_column])\n",
    "    grab_samples_df.drop(columns=original_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = [\n",
    "    \"Colore (Cu)\",\n",
    "    \"Torbidità (NTu)\",\n",
    "    \"Conduttività a 20°C (µS/cm)\",\n",
    "    \"Cloro residuo libero (al prelievo) (mg/L di Cl2)\",\n",
    "    \"Concentrazione ioni idrogeno (unità pH)\",\n",
    "    \"Temperatura (al prelievo) (°C)\",\n",
    "    \"Nitrati (mg/L)\",\n",
    "    \"TOC - carbonio organico totale (mg/L di C)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df = grab_samples_df[\n",
    "    metadata_columns + common_columns + targets_columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_columns + targets_columns:\n",
    "    label_column = column + \"_label\"\n",
    "    grab_samples_df.loc[:, label_column] = grab_samples_df[column].apply(\n",
    "        set_label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df[common_columns] = grab_samples_df[common_columns].map(\n",
    "    convert_string_values\n",
    ")\n",
    "\n",
    "grab_samples_df[targets_columns] = grab_samples_df[targets_columns].map(\n",
    "    convert_string_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df = pd.DataFrame()\n",
    "\n",
    "for sensor_file in os.listdir(sensor_folder):\n",
    "    xls = pd.ExcelFile(os.path.join(sensor_folder, sensor_file))\n",
    "    supply_point = sensor_file.split(\"-\")[0]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # first join the sheets in a single dataframe\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.concat([df, xls.parse(sheet_name, header=1)], axis=0)\n",
    "\n",
    "    df[\"Code\"] = supply_point\n",
    "\n",
    "    # then join to  sensor_df\n",
    "    sensor_df = pd.concat([sensor_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the supply point named 'Preapli' to 'Prealpi'\n",
    "sensor_df[\"Code\"] = sensor_df[\"Code\"].map(\n",
    "    lambda x: \"Prealpi\" if x == \"Preapli\" else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df = sensor_df[\n",
    "    sensor_df.columns[~sensor_df.columns.str.contains(\"Status\")]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.rename(\n",
    "    columns={\n",
    "        \"Measurement interval=900[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "        \"COLORtrue - Measured value [Hazen-eq.] (Limit:0.00-300.00)\": \"Color\",\n",
    "        \"TOCeq - Measured value [mg/l] (Limit:0.00-22.00)\": \"TOC\",\n",
    "        \"NO3eq - Measured value [mg/l] (Limit:0.00-88.00)\": \"Nitrate\",\n",
    "        \"UV254t - Measured value [Abs/m] (Limit:0.00-71.00)\": \"Absorbance\",\n",
    "        \"Turbidity - Measured value [FTUeq] (Limit:0.00-170.00)\": \"Turbidity\",\n",
    "        \"pH - Measured value (Limit:0.00-14.00)\": \"pH\",\n",
    "        \"Temperature - Measured value [C] (Limit:-5.00-100.00)\": \"Temperature\",\n",
    "        \"Conductivity - Measured value [uS/cm] (Limit:0.10-600000.00)\": \"Conductivity\",\n",
    "        \"Free Chlorine - Measured value [mg/l] (Limit:0.00-2.00)\": \"Free Chlorine\",\n",
    "        \"Flow - Measured value (Limit:0.00-1.00)\": \"Flow\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each value of Supply Point column, make it caps\n",
    "sensor_df[\"Code\"] = \"HOUSE_\" + sensor_df[\"Code\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.drop(columns=[\"Tag\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df[\"DateTime\"] = pd.to_datetime(\n",
    "    sensor_df[\"DateTime\"], format=\"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Grab Samples\n",
    "\n",
    "We only keep the variables measured from the sensors to assess similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df.drop(\n",
    "    columns=[\n",
    "        \"Campagna\",\n",
    "        \"ZONA\",\n",
    "        \"Rapporto di prova\",\n",
    "        \"Punto di prelievo\",\n",
    "        \"Analisi programmate\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df.rename(\n",
    "    columns={\n",
    "        \"Data di prelievo\": \"DateTime\",\n",
    "        \"Colore (Cu)\": \"Color\",\n",
    "        \"Torbidità (NTu)\": \"Turbidity\",\n",
    "        \"Conduttività a 20°C (µS/cm)\": \"Conductivity\",\n",
    "        \"Cloro residuo libero (al prelievo) (mg/L di Cl2)\": \"Free Chlorine\",\n",
    "        \"Concentrazione ioni idrogeno (unità pH)\": \"pH\",\n",
    "        \"Temperatura (al prelievo) (°C)\": \"Temperature\",\n",
    "        \"Nitrati (mg/L)\": \"Nitrate\",\n",
    "        \"TOC - carbonio organico totale (mg/L di C)\": \"TOC\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df[\"DateTime\"] = pd.to_datetime(\n",
    "    grab_samples_df[\"DateTime\"], format=\"%Y/%m/%d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by code, type, date\n",
    "grab_samples_df.sort_values(\n",
    "    by=[\"Codice punto di prelievo\", \"Tipologia\", \"DateTime\"], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df.rename(\n",
    "    columns={\n",
    "        \"Codice punto di prelievo\": \"Code\",\n",
    "        \"Tipologia\": \"Type\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of non null values for each code\n",
    "for (code, type_), group in grab_samples_df.groupby([\"Code\", \"Type\"]):\n",
    "    print(code, type_)\n",
    "\n",
    "    print(group.notnull().sum())\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (code, type_), group in grab_samples_df.groupby([\"Code\", \"Type\"]):\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Scatter(\n",
    "                x=group[\"DateTime\"],\n",
    "                y=group[\"Color\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Color\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=group[\"DateTime\"],\n",
    "                y=group[\"Turbidity\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Turbidity\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=group[\"DateTime\"],\n",
    "                y=group[\"Conductivity\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Conductivity\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=group[\"DateTime\"],\n",
    "                y=group[\"Free Chlorine\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Free Chlorine\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=group[\"DateTime\"], y=group[\"pH\"], mode=\"lines\", name=\"pH\"\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=group[\"DateTime\"],\n",
    "                y=group[\"Temperature\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Temperature\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=group[\"DateTime\"],\n",
    "                y=group[\"Nitrate\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Nitrate\",\n",
    "            ),\n",
    "            go.Scatter(\n",
    "                x=group[\"DateTime\"], y=group[\"TOC\"], mode=\"lines\", name=\"TOC\"\n",
    "            ),\n",
    "        ],\n",
    "        layout=go.Layout(\n",
    "            title=f\"{code} - {type_}\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first resample the sensor_df to have a frequency of 15 minutes\n",
    "sensor_res_df = (\n",
    "    sensor_df.groupby(\"Code\")\n",
    "    .resample(\"15min\", on=\"DateTime\")\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the sensor_res_df with sensor_df\n",
    "for code in sensor_df[\"Code\"].unique():\n",
    "    for feature in sensor_df.columns.difference([\"Code\", \"DateTime\"]):\n",
    "        df = sensor_df[sensor_df[\"Code\"] == code][[\"Code\", \"DateTime\", feature]]\n",
    "        res_df = sensor_res_df[sensor_res_df[\"Code\"] == code][\n",
    "            [\"Code\", \"DateTime\", feature]\n",
    "        ]\n",
    "\n",
    "        common_dates = df[df[\"Code\"] == code][\"DateTime\"].isin(\n",
    "            res_df[res_df[\"Code\"] == code][\"DateTime\"]\n",
    "        )\n",
    "\n",
    "        merged_df = pd.merge(\n",
    "            df[df[\"Code\"] == code],\n",
    "            res_df[res_df[\"Code\"] == code],\n",
    "            on=[\"Code\", \"DateTime\"],\n",
    "            suffixes=(\"_sensor\", \"_sensor_res\"),\n",
    "        )\n",
    "\n",
    "        # get indexes where the values are not NaN\n",
    "        not_nan_indexes = (\n",
    "            merged_df[feature + \"_sensor\"].notna()\n",
    "            & merged_df[feature + \"_sensor_res\"].notna()\n",
    "        )\n",
    "        merged_df = merged_df[not_nan_indexes]\n",
    "\n",
    "        # compute normalized mean absolute error\n",
    "        mae = mean_absolute_error(\n",
    "            merged_df[feature + \"_sensor\"].dropna(),\n",
    "            merged_df[feature + \"_sensor_res\"].dropna(),\n",
    "        )\n",
    "\n",
    "        mae = mae / df[df[\"Code\"] == code][feature].mean()\n",
    "\n",
    "        if mae > 0:\n",
    "            go.Figure(\n",
    "                data=[\n",
    "                    go.Scatter(\n",
    "                        x=df[df[\"Code\"] == code][\"DateTime\"],\n",
    "                        y=df[df[\"Code\"] == code][feature],\n",
    "                        mode=\"lines\",\n",
    "                        name=\"Original\",\n",
    "                    ),\n",
    "                    go.Scatter(\n",
    "                        x=res_df[res_df[\"Code\"] == code][\"DateTime\"],\n",
    "                        y=res_df[res_df[\"Code\"] == code][feature],\n",
    "                        mode=\"lines\",\n",
    "                        name=\"Resampled\",\n",
    "                    ),\n",
    "                ],\n",
    "                layout=go.Layout(\n",
    "                    title=f\"{code} - {feature} - MAE: {mae}\",\n",
    "                    xaxis_title=\"Date\",\n",
    "                    yaxis_title=\"Value\",\n",
    "                ),\n",
    "            ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df = sensor_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Data Based on MM Considerations\n",
    "\n",
    "Considerations:\n",
    "-\tI dati della temperatura in uscita dei grab sample sono refrigerati e non possono essere messi in considerazione con i dati dei sensori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bande Nere\n",
    "\n",
    "Manutenzione sonde Torbidità e UV 03/06/2023-08/06/2023 – Valori Torbidità e UV254 anomali da non considerare dal 03/06/2023-08/06/2023\n",
    "\n",
    "Manutenzione e allineamento TOC e Nitrati dal 22/12/2023 – Valori precedenti da non considerare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data from 03/06/2023 to 08/06/2023 where Code is HOUSE_BANDE NERE\n",
    "mask = (\n",
    "    (sensor_df[\"Code\"] == \"HOUSE_BANDE NERE\")\n",
    "    & (sensor_df[\"DateTime\"].dt.date >= pd.to_datetime(\"2023-06-03\").date())\n",
    "    & (sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-06-08\").date())\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"Turbidity\", \"Absorbance\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_BANDE NERE\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-12-22\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"TOC\", \"Nitrate\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df[\"Code\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Berna\n",
    "\n",
    "Manutenzione e allineamento TOC e Nitrati dal 20/11/2023 – Valori precedenti da non considerare\n",
    "\n",
    "Manutenzione e sostituzione elettrodo Cloro 31/01/2024 – Valore precedenti da non considerare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_BERNA\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-11-20\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"TOC\", \"Nitrate\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_BERNA\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-31\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"Free Chlorine\"]] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chiostergi\n",
    "\n",
    "Manutenzione e allineamento TOC e Nitrati dal 20/12/2023 – Valori precedenti da non considerare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_CHIOSTERGI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-12-20\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"TOC\", \"Nitrate\"]] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fortunato\n",
    "\n",
    "Manutenzione e sostituzione elettrodo Cloro 24/01/2024 – Valore precedenti da non considerare\n",
    "\n",
    "Manutenzione e sostituzione elettrodo PH 24/01/2024 – Valori da non considerare dal 04/01/2024 fino al 24/01/2024\n",
    "\n",
    "Manutenzione e allineamento Nitrati dal 23/11/2023 – Valori precedenti da non considerare\n",
    "\n",
    "Manutenzione e allineamento TOC dal 20/12/2023 – Valori precedenti da non considerare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_FORTUNATO\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-24\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"Free Chlorine\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (sensor_df[\"Code\"] == \"HOUSE_FORTUNATO\")\n",
    "    & (sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-24\").date())\n",
    "    & (sensor_df[\"DateTime\"].dt.date >= pd.to_datetime(\"2024-01-04\").date())\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"pH\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_FORTUNATO\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-11-23\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"Nitrate\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_FORTUNATO\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-12-20\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"TOC\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gramsci\n",
    "\n",
    "Manutenzione e allineamento TOC e Nitrati dal 23/11/2023 – Valori precedenti da non considerare\n",
    "\n",
    "Manutenzione e sostituzione elettrodo Cloro 07/02/2024 – Valore precedenti da non considerare\n",
    "\n",
    "Manutenzione e sostituzione elettrodo PH 23/12/2023 – Valori da non considerare dal 27/11/2023 fino al 23/12/2023\n",
    "\n",
    "Manutenzione e allineamento SACUV dal 23/01/2024 – Valori precedenti da non considerare dal 20/10/2023 fino al 23/01/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_GRAMSCI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-11-23\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"TOC\", \"Nitrate\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_GRAMSCI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-02-07\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"Free Chlorine\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (sensor_df[\"Code\"] == \"HOUSE_GRAMSCI\")\n",
    "    & (sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-12-23\").date())\n",
    "    & (sensor_df[\"DateTime\"].dt.date >= pd.to_datetime(\"2023-11-27\").date())\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"pH\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (sensor_df[\"Code\"] == \"HOUSE_GRAMSCI\")\n",
    "    & (sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-23\").date())\n",
    "    & (sensor_df[\"DateTime\"].dt.date >= pd.to_datetime(\"2023-10-20\").date())\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"Absorbance\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Montevideo\n",
    "\n",
    "Manutenzione e allineamento TOC e Nitrati dal 20/11/2023 – Valori precedenti da non considerare\n",
    "\n",
    "Manutenzione e sostituzione elettrodo PH 29/01/2024 – Valori segnati in rosso da non considerare e gli altri poco affidabili fino al 29/01/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_MONTEVIDEO\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-11-20\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"TOC\", \"Nitrate\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_MONTEVIDEO\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-29\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"pH\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prealpi\n",
    "\n",
    "Manutenzione e allineamento TOC e Nitrati dal 23/11/2023 – Valori precedenti da non considerare\n",
    "\n",
    "Manutenzione e sostituzione elettrodo PH 29/01/2024 – Valori segnati in rosso da non considerare e gli altri poco affidabili fino al 29/01/2024\n",
    "\n",
    "Manutenzione Torbidità il 22/01/2024– Valori da non considerare dal 05/09/2023 al 22/01/2024\n",
    "\n",
    "Manutenzione torbidità il 20/03/2024- Valori da non considerare dal 12/02/2024 al 20/03/2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_PREALPI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-11-23\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"TOC\", \"Nitrate\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_PREALPI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-29\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"pH\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (sensor_df[\"Code\"] == \"HOUSE_PREALPI\")\n",
    "    & (sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-22\").date())\n",
    "    & (sensor_df[\"DateTime\"].dt.date >= pd.to_datetime(\"2023-09-05\").date())\n",
    "    & (sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-03-20\").date())\n",
    "    & (sensor_df[\"DateTime\"].dt.date >= pd.to_datetime(\"2024-02-12\").date())\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"Turbidity\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabacchi\n",
    "\n",
    "Manutenzione e allineamento TOC e Nitrati dal 21/11/2023 – Valori precedenti da non considerare\n",
    "\n",
    "Manutenzione e sostituzione elettrodo Cloro 26/01/2024 – Valore precedenti da non considerare\n",
    "\n",
    "Manutenzione e sostituzione elettrodo PH 25/01/2024 – Valori segnati in rosso da non considerare e gli altri poco affidabili fino al 25/01/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_TABACCHI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-11-21\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"TOC\", \"Nitrate\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_TABACCHI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-26\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"Free Chlorine\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_TABACCHI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-25\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"pH\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tognazzi\n",
    "\n",
    "Manutenzione e allineamento TOC e Nitrati dal 23/11/2023 – Valori precedenti da non considerare\n",
    "\n",
    "Manutenzione e sostituzione elettrodo PH 17/01/2024 – Tutti i valori precedenti da non considerare assolutamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_TOGNAZZI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2023-11-23\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, [\"TOC\", \"Nitrate\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sensor_df[\"Code\"] == \"HOUSE_TOGNAZZI\") & (\n",
    "    sensor_df[\"DateTime\"].dt.date <= pd.to_datetime(\"2024-01-17\").date()\n",
    ")\n",
    "\n",
    "sensor_df.loc[mask, \"pH\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in sensor_df[\"Code\"].unique():\n",
    "    print(code)\n",
    "    for feature in sensor_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        df = sensor_df[sensor_df[\"Code\"] == code][feature]\n",
    "        print(f\"{feature}: \" + str(df.isna().sum() / df.shape[0] * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame\n",
    "missing_values_df = pd.DataFrame()\n",
    "\n",
    "for code in sensor_df[\"Code\"].unique():\n",
    "    # Initialize a dictionary to store the information for the current code\n",
    "    info_dict = {}\n",
    "    for feature in sensor_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        df = sensor_df[sensor_df[\"Code\"] == code][feature]\n",
    "        # Store the percentage of missing values in the dictionary\n",
    "\n",
    "        perc = df.isna().sum() / df.shape[0] * 100\n",
    "        perc = round(perc, 2)\n",
    "        perc = str(perc) + \"%\"\n",
    "        info_dict[feature] = perc\n",
    "    # Add the dictionary to the DataFrame\n",
    "    missing_values_df[code] = pd.Series(info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_df.to_excel(\n",
    "    os.path.join(metadata_folder, \"Riunione 24-04-2024\", \"Missing Values.xlsx\"),\n",
    "    index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in sensor_df[\"Code\"].unique():\n",
    "    for feature in sensor_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        fig = go.Figure()\n",
    "\n",
    "        df = sensor_df[sensor_df[\"Code\"] == code][[\"DateTime\", feature]].copy()\n",
    "        df[\"is_missing\"] = df[feature].isna()\n",
    "        missing_values_perc = (df[\"is_missing\"].sum() / df.shape[0]) * 100\n",
    "        missing_values_perc = missing_values_perc.round(2)\n",
    "\n",
    "        # Create a boolean mask to identify NaN values\n",
    "        mask = df[feature].isna()\n",
    "\n",
    "        # Create a new column to identify consecutive NaNs\n",
    "        df[\"group\"] = (mask & (~mask).shift()).cumsum()\n",
    "\n",
    "        # Group by the 'group' column and find the start and end dates for each group\n",
    "        nan_periods = (\n",
    "            df[mask]\n",
    "            .groupby(\"group\")[\"DateTime\"]\n",
    "            .agg([\"min\", \"max\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Rename the columns for better readability\n",
    "        nan_periods.columns = [\"start_date\", \"end_date\"]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"DateTime\"],\n",
    "                y=df[feature],\n",
    "                mode=\"lines\",\n",
    "                name=feature,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for _, row in nan_periods.iterrows():\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=row[\"start_date\"],\n",
    "                y0=df[feature].min(),\n",
    "                x1=row[\"end_date\"],\n",
    "                y1=df[feature].max(),\n",
    "                fillcolor=\"red\",\n",
    "                line=dict(color=\"red\"),\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature} - Missing Values: {missing_values_perc}%\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\",\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(\n",
    "            os.path.join(\n",
    "                plot_folder, \"Riunione 24-04-2024\", \"Missing Values\", code\n",
    "            )\n",
    "        ):\n",
    "            os.makedirs(\n",
    "                os.path.join(\n",
    "                    plot_folder, \"Riunione 24-04-2024\", \"Missing Values\", code\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # # save the plot\n",
    "        # fig.write_image(\n",
    "        #     os.path.join(\n",
    "        #         plot_folder,\n",
    "        #         \"Riunione 24-04-2024\",\n",
    "        #         \"Missing Values\",\n",
    "        #         code,\n",
    "        #         f\"{feature}.png\",\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = sensor_df.copy()\n",
    "\n",
    "copy_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in copy_df[\"Code\"].unique():\n",
    "    print(code)\n",
    "    for feature in copy_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        df = copy_df[copy_df[\"Code\"] == code][feature]\n",
    "        print(f\"{feature}: \" + str(df.isna().sum() / df.shape[0] * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = (\n",
    "    copy_df.groupby(\"Code\")\n",
    "    .resample(\"15min\", on=\"DateTime\")\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in copy_df[\"Code\"].unique():\n",
    "    print(code)\n",
    "    for feature in copy_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        df = copy_df[copy_df[\"Code\"] == code][feature]\n",
    "        print(f\"{feature}: \" + str(df.isna().sum() / df.shape[0] * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in copy_df[\"Code\"].unique():\n",
    "    for feature in copy_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        fig = go.Figure()\n",
    "\n",
    "        df = copy_df[copy_df[\"Code\"] == code][[\"DateTime\", feature]].copy()\n",
    "\n",
    "        df[\"is_missing\"] = df[feature].isna()\n",
    "        missing_values_perc = (df[\"is_missing\"].sum() / df.shape[0]) * 100\n",
    "        missing_values_perc = missing_values_perc.round(2)\n",
    "\n",
    "        # Create a boolean mask to identify NaN values\n",
    "        mask = df[feature].isna()\n",
    "\n",
    "        # Create a new column to identify consecutive NaNs\n",
    "        df[\"group\"] = (mask & (~mask).shift()).cumsum()\n",
    "\n",
    "        # Group by the 'group' column and find the start and end dates for each group\n",
    "        nan_periods = (\n",
    "            df[mask]\n",
    "            .groupby(\"group\")[\"DateTime\"]\n",
    "            .agg([\"min\", \"max\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Rename the columns for better readability\n",
    "        nan_periods.columns = [\"start_date\", \"end_date\"]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"DateTime\"],\n",
    "                y=df[feature],\n",
    "                mode=\"lines\",\n",
    "                name=feature,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for _, row in nan_periods.iterrows():\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=row[\"start_date\"],\n",
    "                y0=df[feature].min(),\n",
    "                x1=row[\"end_date\"],\n",
    "                y1=df[feature].max(),\n",
    "                fillcolor=\"red\",\n",
    "                line=dict(color=\"red\"),\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature} - Missing Values: {missing_values_perc}%\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\",\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df.set_index(\"DateTime\", inplace=True)\n",
    "for code in copy_df[\"Code\"].unique():\n",
    "    df = copy_df[copy_df[\"Code\"] == code].copy()\n",
    "\n",
    "    for feature in copy_df.columns.difference([\"Code\"]):\n",
    "        df[feature] = df[feature].interpolate(method=\"time\")\n",
    "\n",
    "    copy_df[copy_df[\"Code\"] == code] = df\n",
    "\n",
    "copy_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame\n",
    "date_range_df = pd.DataFrame()\n",
    "\n",
    "for code in copy_df[\"Code\"].unique():\n",
    "    df = copy_df[copy_df[\"Code\"] == code]\n",
    "\n",
    "    min_date = df[\"DateTime\"].min().date()\n",
    "    max_date = df[\"DateTime\"].max().date()\n",
    "    date_range = f\"{min_date} - {max_date}\"\n",
    "\n",
    "    date_range_df[code] = pd.Series({\"Date Range\": date_range})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range_df.to_excel(\n",
    "    os.path.join(metadata_folder, \"Riunione 24-04-2024\", \"Date Range.xlsx\"),\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in copy_df[\"Code\"].unique():\n",
    "    for feature in copy_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        fig = go.Figure()\n",
    "\n",
    "        df = copy_df[copy_df[\"Code\"] == code][[\"DateTime\", feature]].copy()\n",
    "\n",
    "        df[\"is_missing\"] = df[feature].isna()\n",
    "        missing_values_perc = (df[\"is_missing\"].sum() / df.shape[0]) * 100\n",
    "        missing_values_perc = missing_values_perc.round(2)\n",
    "\n",
    "        # Create a boolean mask to identify NaN values\n",
    "        mask = df[feature].isna()\n",
    "\n",
    "        # Create a new column to identify consecutive NaNs\n",
    "        df[\"group\"] = (mask & (~mask).shift()).cumsum()\n",
    "\n",
    "        # Group by the 'group' column and find the start and end dates for each group\n",
    "        nan_periods = (\n",
    "            df[mask]\n",
    "            .groupby(\"group\")[\"DateTime\"]\n",
    "            .agg([\"min\", \"max\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Rename the columns for better readability\n",
    "        nan_periods.columns = [\"start_date\", \"end_date\"]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"DateTime\"],\n",
    "                y=df[feature],\n",
    "                mode=\"lines\",\n",
    "                name=feature,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for _, row in nan_periods.iterrows():\n",
    "            fig.add_shape(\n",
    "                type=\"rect\",\n",
    "                x0=row[\"start_date\"],\n",
    "                y0=df[feature].min(),\n",
    "                x1=row[\"end_date\"],\n",
    "                y1=df[feature].max(),\n",
    "                fillcolor=\"red\",\n",
    "                line=dict(color=\"red\"),\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature} - Missing Values: {missing_values_perc}%\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\",\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(\n",
    "            os.path.join(\n",
    "                plot_folder,\n",
    "                \"Riunione 24-04-2024\",\n",
    "                \"Clean Data\",\n",
    "                \"No Removed Outliers\",\n",
    "                code,\n",
    "            )\n",
    "        ):\n",
    "            os.makedirs(\n",
    "                os.path.join(\n",
    "                    plot_folder,\n",
    "                    \"Riunione 24-04-2024\",\n",
    "                    \"Clean Data\",\n",
    "                    \"No Removed Outliers\",\n",
    "                    code,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                plot_folder,\n",
    "                \"Riunione 24-04-2024\",\n",
    "                \"Clean Data\",\n",
    "                \"No Removed Outliers\",\n",
    "                code,\n",
    "                f\"{feature}.png\",\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Outliers (or Anomalies) are detected by using the STL method, which uses LOESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df = copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_z_score(df, window=5):\n",
    "    rolling_mean = df.rolling(window=window).mean()\n",
    "    rolling_std = df.rolling(window=window).std()\n",
    "\n",
    "    upper_threshold = rolling_mean + 2 * rolling_std\n",
    "    lower_threshold = rolling_mean - 2 * rolling_std\n",
    "\n",
    "    df[\"upper_threshold\"] = upper_threshold\n",
    "    df[\"lower_threshold\"] = lower_threshold\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = (\n",
    "    copy_df.groupby(\"Code\").resample(\"D\", on=\"DateTime\").median().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df.set_index(\"DateTime\", inplace=True)\n",
    "for code in copy_df[\"Code\"].unique():\n",
    "    df = copy_df[copy_df[\"Code\"] == code].copy()\n",
    "\n",
    "    for feature in copy_df.columns.difference([\"Code\"]):\n",
    "        df[feature] = df[feature].interpolate(method=\"time\")\n",
    "\n",
    "    copy_df[copy_df[\"Code\"] == code] = df\n",
    "\n",
    "copy_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_df\n",
    "for code in copy_df[\"Code\"].unique():\n",
    "    for feature in copy_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=copy_df[copy_df[\"Code\"] == code][\"DateTime\"],\n",
    "                y=copy_df[copy_df[\"Code\"] == code][feature],\n",
    "                mode=\"lines\",\n",
    "                name=\"Original\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature}\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\",\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(\n",
    "            os.path.join(\n",
    "                plot_folder, \"Riunione 24-04-2024\", \"Clean Data\", \"Daily\", code\n",
    "            )\n",
    "        ):\n",
    "            os.makedirs(\n",
    "                os.path.join(\n",
    "                    plot_folder,\n",
    "                    \"Riunione 24-04-2024\",\n",
    "                    \"Clean Data\",\n",
    "                    \"Daily\",\n",
    "                    code,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                plot_folder,\n",
    "                \"Riunione 24-04-2024\",\n",
    "                \"Clean Data\",\n",
    "                \"Daily\",\n",
    "                code,\n",
    "                f\"{feature}.png\",\n",
    "            ),\n",
    "            width=5,\n",
    "            height=2,\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in sensor_df[\"Code\"].unique():\n",
    "    for feature in sensor_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        df = sensor_df[sensor_df[\"Code\"] == code][[\"DateTime\", feature]].copy()\n",
    "\n",
    "        df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        stl = STL(df, period=96 * 20, robust=True)\n",
    "        result = stl.fit()\n",
    "        seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df.index,\n",
    "                y=df[feature],\n",
    "                mode=\"lines\",\n",
    "                name=\"Original\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trend.index,\n",
    "                y=trend,\n",
    "                mode=\"lines\",\n",
    "                name=\"Trend\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        resid_mean = np.mean(resid)\n",
    "        resid_std = np.std(resid)\n",
    "\n",
    "        lower_bound = resid_mean - 2 * resid_std\n",
    "        upper_bound = resid_mean + 2 * resid_std\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trend.index,\n",
    "                y=trend - 2 * resid_std,\n",
    "                mode=\"lines\",\n",
    "                name=\"Lower Bound\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trend.index,\n",
    "                y=trend + 2 * resid_std,\n",
    "                mode=\"lines\",\n",
    "                name=\"Upper Bound\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        # fig.add_shape(\n",
    "        #     type=\"line\",\n",
    "        #     x0=df.index.min(),\n",
    "        #     y0=lower_bound,\n",
    "        #     x1=df.index.max(),\n",
    "        #     y1=lower_bound,\n",
    "        #     line=dict(\n",
    "        #         color=\"red\",\n",
    "        #         width=1,\n",
    "        #     ),\n",
    "        #     row=2,\n",
    "        #     col=1,\n",
    "        # )\n",
    "\n",
    "        # fig.add_shape(\n",
    "        #     type=\"line\",\n",
    "        #     x0=df.index.min(),\n",
    "        #     y0=upper_bound,\n",
    "        #     x1=df.index.max(),\n",
    "        #     y1=upper_bound,\n",
    "        #     line=dict(\n",
    "        #         color=\"red\",\n",
    "        #         width=1,\n",
    "        #     ),\n",
    "        #     row=2,\n",
    "        #     col=1,\n",
    "        # )\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         x=df.index,\n",
    "        #         y=resid,\n",
    "        #         mode=\"lines\",\n",
    "        #         name=\"Residual\",\n",
    "        #     ),\n",
    "        #     row=2,\n",
    "        #     col=1,\n",
    "        # )\n",
    "\n",
    "        outliers = df[\n",
    "            (df[feature] < trend - 2 * resid_std)\n",
    "            | (df[feature] > trend + 2 * resid_std)\n",
    "        ]\n",
    "        # outliers = df[df[\"DateTime\"].isin(outliers.index)][[\"DateTime\", feature]]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=outliers.index,\n",
    "                y=outliers[feature],\n",
    "                mode=\"markers\",\n",
    "                name=\"Outliers\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature}\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\",\n",
    "        )\n",
    "\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in sensor_df[\"Code\"].unique():\n",
    "    for feature in sensor_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "        df = sensor_df[sensor_df[\"Code\"] == code][[\"DateTime\", feature]].copy()\n",
    "\n",
    "        df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        # Calculate the moving average and standard deviation\n",
    "        df[\"moving_avg\"] = df[feature].rolling(window=96 * 20).mean()\n",
    "        df[\"moving_std\"] = df[feature].rolling(window=96 * 20).std()\n",
    "\n",
    "        # Calculate the z-score\n",
    "        df[\"z_score\"] = (df[feature] - df[\"moving_avg\"]) / df[\"moving_std\"]\n",
    "\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"DateTime\"],\n",
    "                y=df[feature],\n",
    "                mode=\"lines\",\n",
    "                name=\"Original\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"DateTime\"],\n",
    "                y=df[\"moving_avg\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Moving Average\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        lower_bound = -3\n",
    "        upper_bound = 3\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"DateTime\"],\n",
    "                y=df[\"moving_avg\"] - 3 * df[\"moving_std\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Lower Bound\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"DateTime\"],\n",
    "                y=df[\"moving_avg\"] + 3 * df[\"moving_std\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Upper Bound\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"DateTime\"],\n",
    "                y=df[\"z_score\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Z-Score\",\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        outliers = df[\n",
    "            (df[\"z_score\"] < lower_bound) | (df[\"z_score\"] > upper_bound)\n",
    "        ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=outliers[\"DateTime\"],\n",
    "                y=outliers[feature],\n",
    "                mode=\"markers\",\n",
    "                name=\"Outliers\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature}\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\",\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every feature, compare every code with the others\n",
    "feature_dict = {}\n",
    "for feature in copy_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    codes = copy_df[\"Code\"].unique()\n",
    "\n",
    "    for i in range(len(codes)):\n",
    "        for j in range(i + 1, len(codes)):\n",
    "            df = copy_df[copy_df[\"Code\"] == codes[i]][\n",
    "                [\"DateTime\", feature]\n",
    "            ].copy()\n",
    "            other_df = copy_df[copy_df[\"Code\"] == codes[j]][\n",
    "                [\"DateTime\", feature]\n",
    "            ].copy()\n",
    "\n",
    "            # get the common timerange between the two dataframes\n",
    "            common_dates = df[df[\"DateTime\"].isin(other_df[\"DateTime\"])][\n",
    "                \"DateTime\"\n",
    "            ]\n",
    "            df = df[df[\"DateTime\"].isin(common_dates)]\n",
    "            other_df = other_df[other_df[\"DateTime\"].isin(common_dates)]\n",
    "\n",
    "            # sort the dataframes by DateTime\n",
    "            df.sort_values(by=\"DateTime\", inplace=True)\n",
    "            other_df.sort_values(by=\"DateTime\", inplace=True)\n",
    "\n",
    "            # normalize the dataframes\n",
    "            # scaler = MinMaxScaler()\n",
    "            # df[feature] = scaler.fit_transform(df[[feature]])\n",
    "            # other_df[feature] = scaler.fit_transform(other_df[[feature]])\n",
    "\n",
    "            # compute the correlation between the two dataframes\n",
    "            correlation = stats.pearsonr(df[feature], other_df[feature])[0]\n",
    "            feature_df.loc[codes[i], codes[j]] = correlation\n",
    "\n",
    "    feature_dict[feature] = feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\n",
    "    os.path.join(\n",
    "        metadata_folder, \"Riunione 24-04-2024\", \"comparison\", \"pearson.xlsx\"\n",
    "    )\n",
    ") as writer:\n",
    "    for key, value in feature_dict.items():\n",
    "        value.to_excel(writer, sheet_name=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every feature, compare every code with the others\n",
    "feature_dict = {}\n",
    "for feature in copy_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    codes = sensor_df[\"Code\"].unique()\n",
    "\n",
    "    for i in range(len(codes)):\n",
    "        for j in range(i + 1, len(codes)):\n",
    "            df = copy_df[copy_df[\"Code\"] == codes[i]][\n",
    "                [\"DateTime\", feature]\n",
    "            ].copy()\n",
    "            other_df = copy_df[copy_df[\"Code\"] == codes[j]][\n",
    "                [\"DateTime\", feature]\n",
    "            ].copy()\n",
    "\n",
    "            # get the common timerange between the two dataframes\n",
    "            common_dates = df[df[\"DateTime\"].isin(other_df[\"DateTime\"])][\n",
    "                \"DateTime\"\n",
    "            ]\n",
    "            df = df[df[\"DateTime\"].isin(common_dates)]\n",
    "            other_df = other_df[other_df[\"DateTime\"].isin(common_dates)]\n",
    "\n",
    "            # sort the dataframes by DateTime\n",
    "            df.sort_values(by=\"DateTime\", inplace=True)\n",
    "            other_df.sort_values(by=\"DateTime\", inplace=True)\n",
    "\n",
    "            # normalize the dataframes\n",
    "            # scaler = MinMaxScaler()\n",
    "            # df[feature] = scaler.fit_transform(df[[feature]])\n",
    "            # other_df[feature] = scaler.fit_transform(other_df[[feature]])\n",
    "\n",
    "            # compute mae between the two dataframes\n",
    "            mae = mean_absolute_error(df[feature], other_df[feature])\n",
    "            feature_df.loc[codes[i], codes[j]] = mae\n",
    "\n",
    "    feature_dict[feature] = feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\n",
    "    os.path.join(\n",
    "        metadata_folder, \"Riunione 24-04-2024\", \"comparison\", \"mae.xlsx\"\n",
    "    )\n",
    ") as writer:\n",
    "    for key, value in feature_dict.items():\n",
    "        value.to_excel(writer, sheet_name=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = AffinityPropagation(affinity=\"precomputed\")\n",
    "\n",
    "# List of unique house codes\n",
    "house_codes = copy_df[\"Code\"].unique()\n",
    "\n",
    "# Compute the affinity matrix\n",
    "for feature in copy_df.columns.difference([\"DateTime\", \"Code\"]):\n",
    "    affinity_matrix = np.zeros((len(house_codes), len(house_codes)))\n",
    "    for i, house1 in enumerate(house_codes):\n",
    "        for j, house2 in enumerate(house_codes):\n",
    "            if house1 == house2:\n",
    "                continue\n",
    "            affinity_matrix[i, j] = stats.wasserstein_distance(\n",
    "                copy_df[copy_df[\"Code\"] == house1][feature].dropna(),\n",
    "                copy_df[copy_df[\"Code\"] == house2][feature].dropna(),\n",
    "            )\n",
    "\n",
    "    # normalize the affinity matrix to have similarity values between 0 and 1\n",
    "    affinity_matrix = 1 - (affinity_matrix - affinity_matrix.min()) / (\n",
    "        affinity_matrix.max() - affinity_matrix.min()\n",
    "    )\n",
    "\n",
    "    # set the nan values to 0\n",
    "    affinity_matrix[np.isnan(affinity_matrix)] = 0\n",
    "\n",
    "    prop.fit(affinity_matrix)\n",
    "\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"Number of clusters: {len(prop.cluster_centers_indices_)}\")\n",
    "\n",
    "    # Print cluster centers\n",
    "    print(\"Cluster centers:\")\n",
    "    for center_index in prop.cluster_centers_indices_:\n",
    "        print(house_codes[center_index])\n",
    "\n",
    "    # Print houses in each cluster\n",
    "    for cluster_id in range(len(prop.cluster_centers_indices_)):\n",
    "        print(f\"Cluster {cluster_id}:\")\n",
    "        for i, label in enumerate(prop.labels_):\n",
    "            if label == cluster_id:\n",
    "                print(house_codes[i])\n",
    "\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab vs Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df[\"Code\"] = grab_samples_df[\"Code\"].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Code HOUSE_BANDE NERE to HOUSE_BANDENERE\n",
    "sensor_df[\"Code\"] = sensor_df[\"Code\"].str.replace(\n",
    "    \"HOUSE_BANDE NERE\", \"HOUSE_BANDENERE\"\n",
    ")\n",
    "copy_df[\"Code\"] = copy_df[\"Code\"].str.replace(\n",
    "    \"HOUSE_BANDE NERE\", \"HOUSE_BANDENERE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor_df: 15 min sampling rate\n",
    "\n",
    "for code in [\"HOUSE_TABACCHI\"]:\n",
    "    for feature in grab_samples_df.columns.difference(\n",
    "        [\"DateTime\", \"Code\", \"Type\"]\n",
    "    ):\n",
    "        df = sensor_df[sensor_df[\"Code\"] == code][[\"DateTime\", feature]].copy()\n",
    "\n",
    "        df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        stl = STL(df, period=96 * 20, robust=True)\n",
    "        result = stl.fit()\n",
    "        seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df.index,\n",
    "                y=df[feature],\n",
    "                mode=\"lines\",\n",
    "                name=\"Sensor\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trend.index,\n",
    "                y=trend,\n",
    "                mode=\"lines\",\n",
    "                name=\"Trend\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        resid_mean = np.mean(resid)\n",
    "        resid_std = np.std(resid)\n",
    "\n",
    "        lower_bound = resid_mean - 3 * resid_std\n",
    "        upper_bound = resid_mean + 3 * resid_std\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trend.index,\n",
    "                y=trend - 3 * resid_std,\n",
    "                mode=\"lines\",\n",
    "                name=\"Lower Bound\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trend.index,\n",
    "                y=trend + 3 * resid_std,\n",
    "                mode=\"lines\",\n",
    "                name=\"Upper Bound\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        outliers = df[\n",
    "            (df[feature] < trend - 3 * resid_std)\n",
    "            | (df[feature] > trend + 3 * resid_std)\n",
    "        ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=outliers.index,\n",
    "                y=outliers[feature],\n",
    "                mode=\"markers\",\n",
    "                name=\"Outliers\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        # add grab samples\n",
    "        grab_df = grab_samples_df[grab_samples_df[\"Code\"] == code][\n",
    "            [\"DateTime\", \"Type\", feature]\n",
    "        ].copy()\n",
    "\n",
    "        # get common dates between the two dataframes\n",
    "        common_dates = df[df.index.isin(grab_df[\"DateTime\"])]\n",
    "\n",
    "        grab_df = grab_df[grab_df[\"DateTime\"].isin(common_dates.index)]\n",
    "        grab_df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        for type in grab_df[\"Type\"].unique():\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=grab_df[grab_df[\"Type\"] == type].index,\n",
    "                    y=grab_df[grab_df[\"Type\"] == type][feature],\n",
    "                    mode=\"markers\",\n",
    "                    name=f\"Grab: {feature} - {type}\",\n",
    "                    marker_symbol=\"x\",\n",
    "                    marker_size=10,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature}\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\",\n",
    "        )\n",
    "\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        if not os.path.exists(\n",
    "            os.path.join(\n",
    "                plot_folder, \"Riunione 24-04-2024\", \"Comparison\", \"15min\", code\n",
    "            )\n",
    "        ):\n",
    "            os.makedirs(\n",
    "                os.path.join(\n",
    "                    plot_folder,\n",
    "                    \"Riunione 24-04-2024\",\n",
    "                    \"Comparison\",\n",
    "                    \"15min\",\n",
    "                    code,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                plot_folder,\n",
    "                \"Riunione 24-04-2024\",\n",
    "                \"Comparison\",\n",
    "                \"15min\",\n",
    "                code,\n",
    "                f\"{feature}.jpeg\",\n",
    "            ),\n",
    "            scale=5,\n",
    "            width=8,\n",
    "            height=2,\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_samples_df[\"Code\"].unique():\n",
    "    for feature in grab_samples_df.columns.difference(\n",
    "        [\"DateTime\", \"Code\", \"Type\"]\n",
    "    ):\n",
    "        df = copy_df[copy_df[\"Code\"] == code][[\"DateTime\", feature]].copy()\n",
    "\n",
    "        df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        stl = STL(df, period=20, robust=True)\n",
    "        result = stl.fit()\n",
    "        seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
    "\n",
    "        fig = make_subplots(rows=1, cols=1)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df.index,\n",
    "                y=df[feature],\n",
    "                mode=\"lines\",\n",
    "                name=\"Sensor\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trend.index,\n",
    "                y=trend,\n",
    "                mode=\"lines\",\n",
    "                name=\"Trend\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        resid_mean = np.mean(resid)\n",
    "        resid_std = np.std(resid)\n",
    "\n",
    "        lower_bound = resid_mean - 3 * resid_std\n",
    "        upper_bound = resid_mean + 3 * resid_std\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trend.index,\n",
    "                y=trend - 3 * resid_std,\n",
    "                mode=\"lines\",\n",
    "                name=\"Lower Bound\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trend.index,\n",
    "                y=trend + 3 * resid_std,\n",
    "                mode=\"lines\",\n",
    "                name=\"Upper Bound\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        outliers = df[\n",
    "            (df[feature] < trend - 3 * resid_std)\n",
    "            | (df[feature] > trend + 3 * resid_std)\n",
    "        ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=outliers.index,\n",
    "                y=outliers[feature],\n",
    "                mode=\"markers\",\n",
    "                name=\"Outliers\",\n",
    "                marker_size=20,\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        # add grab samples\n",
    "        grab_df = grab_samples_df[grab_samples_df[\"Code\"] == code][\n",
    "            [\"DateTime\", \"Type\", feature]\n",
    "        ].copy()\n",
    "\n",
    "        # get common dates between the two dataframes\n",
    "        common_dates = df[df.index.isin(grab_df[\"DateTime\"])]\n",
    "\n",
    "        grab_df = grab_df[grab_df[\"DateTime\"].isin(common_dates.index)]\n",
    "        grab_df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        for type in grab_df[\"Type\"].unique():\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=grab_df[grab_df[\"Type\"] == type].index,\n",
    "                    y=grab_df[grab_df[\"Type\"] == type][feature],\n",
    "                    mode=\"markers\",\n",
    "                    name=f\"Grab: {feature} - {type}\",\n",
    "                    marker_symbol=\"x\",\n",
    "                    marker_size=20,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature}\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Value\",\n",
    "            font=dict(size=20),\n",
    "        )\n",
    "\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        if not os.path.exists(\n",
    "            os.path.join(\n",
    "                plot_folder, \"Riunione 24-04-2024\", \"Comparison\", \"Daily\", code\n",
    "            )\n",
    "        ):\n",
    "            os.makedirs(\n",
    "                os.path.join(\n",
    "                    plot_folder,\n",
    "                    \"Riunione 24-04-2024\",\n",
    "                    \"Comparison\",\n",
    "                    \"Daily\",\n",
    "                    code,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                plot_folder,\n",
    "                \"Riunione 24-04-2024\",\n",
    "                \"Comparison\",\n",
    "                \"Daily\",\n",
    "                code,\n",
    "                f\"{feature}.jpeg\",\n",
    "            ),\n",
    "            scale=5,\n",
    "            width=8,\n",
    "            height=2,\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in copy_df.columns.difference([\"DateTime\", \"Code\", \"Type\"]):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for code in sensor_df[\"Code\"].unique():\n",
    "        df = sensor_df[sensor_df[\"Code\"] == code][[\"DateTime\", feature]].copy()\n",
    "\n",
    "        df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "        # remove wrong values\n",
    "        if feature == \"Absorbance\":\n",
    "            df = df[df[feature] < 5000]\n",
    "\n",
    "        elif feature == \"Color\":\n",
    "            df = df[df[feature] < 6]\n",
    "\n",
    "        elif feature == \"Free Chlorine\":\n",
    "            df = df[df[feature] < 400]\n",
    "\n",
    "        elif feature == \"TOC\":\n",
    "            df = df[df[feature] < 3]\n",
    "\n",
    "        elif feature == \"Turbidity\":\n",
    "            df = df[df[feature] < 2000]\n",
    "\n",
    "        elif feature == \"Flow\":\n",
    "            df = df[df[feature] > 0]\n",
    "\n",
    "        fig.add_trace(go.Box(y=df[feature], name=code, showlegend=False))\n",
    "\n",
    "        if feature != \"Absorbance\" and feature != \"Flow\":\n",
    "            # add grab samples\n",
    "            grab_df = grab_samples_df[grab_samples_df[\"Code\"] == code][\n",
    "                [\"DateTime\", \"Type\", feature]\n",
    "            ].copy()\n",
    "\n",
    "            # get common dates between the two dataframes\n",
    "            common_dates = df[df.index.isin(grab_df[\"DateTime\"])]\n",
    "\n",
    "            grab_df = grab_df[grab_df[\"DateTime\"].isin(common_dates.index)]\n",
    "            grab_df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "            for type in grab_df[\"Type\"].unique():\n",
    "                # add points to the boxplots for the grab samples\n",
    "\n",
    "                if type == \"Ingresso\":\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=[code],\n",
    "                            y=grab_df[grab_df[\"Type\"] == type][feature],\n",
    "                            mode=\"markers\",\n",
    "                            name=f\"Grab: {type}\",\n",
    "                            marker=dict(symbol=\"x\", size=10, color=\"blue\"),\n",
    "                            showlegend=False,\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=[code],\n",
    "                            y=grab_df[grab_df[\"Type\"] == type][feature],\n",
    "                            mode=\"markers\",\n",
    "                            name=f\"Grab: {type}\",\n",
    "                            marker=dict(symbol=\"x\", size=10, color=\"red\"),\n",
    "                            showlegend=False,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{feature}\",\n",
    "            xaxis_title=\"House\",\n",
    "            yaxis_title=\"Value\",\n",
    "            font=dict(size=18),\n",
    "        )\n",
    "\n",
    "        # set legend to the right and set the x blue mark as Ingresso and the red mark as Uscita\n",
    "        fig.update_layout(\n",
    "            legend=dict(\n",
    "                orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # if not os.path.exists(\n",
    "        #     os.path.join(\n",
    "        #         plot_folder, \"Riunione 24-04-2024\", \"Comparison\", code\n",
    "        #     )\n",
    "        # ):\n",
    "        #     os.makedirs(\n",
    "        #         os.path.join(\n",
    "        #             plot_folder, \"Riunione 24-04-2024\", \"Comparison\", code\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        # fig.write_image(\n",
    "        #     os.path.join(\n",
    "        #         plot_folder,\n",
    "        #         \"Riunione 24-04-2024\",\n",
    "        #         \"Comparison\",\n",
    "        #         code,\n",
    "        #         f\"{feature}.jpeg\",\n",
    "        #     ),\n",
    "        #     scale=5,\n",
    "        #     width=8,\n",
    "        #     height=2\n",
    "        # )\n",
    "\n",
    "    # Add dummy traces for custom legend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(symbol=\"x\", size=10, color=\"blue\"),\n",
    "            name=\"Ingresso\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(symbol=\"x\", size=10, color=\"red\"),\n",
    "            name=\"Uscita\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df.rename(\n",
    "    columns={\n",
    "        \"Color\": \"Color (CU)\",\n",
    "        \"Turbidity\": \"Turbidity (NTU)\",\n",
    "        \"Free Chlorine\": \"Free Chlorine (mg/L)\",\n",
    "        \"Conductivity\": \"Conductivity (uS/cm)\",\n",
    "        \"pH\": \"pH\",\n",
    "        \"TOC\": \"TOC (mg/L)\",\n",
    "        \"Nitrate\": \"Nitrate (mg/L)\",\n",
    "        \"Temperature\": \"Temperature (°C)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df.to_excel(\n",
    "    os.path.join(clean_data_folder, \"Riunione 24-04-2024\", \"Grab Samples.xlsx\"),\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.rename(\n",
    "    columns={\n",
    "        \"Color\": \"Color (CU)\",\n",
    "        \"Turbidity\": \"Turbidity (NTU)\",\n",
    "        \"Free Chlorine\": \"Free Chlorine (mg/L)\",\n",
    "        \"Conductivity\": \"Conductivity (uS/cm)\",\n",
    "        \"pH\": \"pH\",\n",
    "        \"TOC\": \"TOC (mg/L)\",\n",
    "        \"Nitrate\": \"Nitrate (mg/L)\",\n",
    "        \"Temperature\": \"Temperature (°C)\",\n",
    "        \"Absorbance\": \"UVA254 (1/m)\",\n",
    "        \"Flow\": \"Flow Rate (m³/s)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.to_excel(\n",
    "    os.path.join(clean_data_folder, \"Riunione 24-04-2024\", \"Sensor Data.xlsx\"),\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
