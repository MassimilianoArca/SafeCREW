{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Points (Case dell'Acqua) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"..\", \"utils\")\n",
    "\n",
    "with open(os.path.join(utils_folder, \"onedrive.txt\"), \"r\") as f:\n",
    "    cloud_data_folder = os.path.join(f.readline().strip(), \"Case dell'acqua\")\n",
    "\n",
    "grab_samples_folder = os.path.join(cloud_data_folder, \"Grab Samples\")\n",
    "sensors_folder = os.path.join(cloud_data_folder, \"Sensori\")\n",
    "\n",
    "local_data_folder = os.path.join(\"..\", \"..\", \"data\")\n",
    "clean_data_folder = os.path.join(local_data_folder, \"Clean Data\")\n",
    "raw_data_folder = os.path.join(local_data_folder, \"Raw Data\")\n",
    "\n",
    "plot_folder = os.path.join(local_data_folder, \"Plots\")\n",
    "\n",
    "all_grab_samples_path = os.path.join(\n",
    "    raw_data_folder, \"Tutti punti - Grab Samples\"\n",
    ")\n",
    "\n",
    "grab_samples_supply_points_path = os.path.join(\n",
    "    raw_data_folder,\n",
    "    \"Case dell'acqua - Grab Samples (main)/0. Case acqua - 2010-2023.xlsx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tra i grab non c'è l'ORP, mentre\n",
    "# tra i sensori non c'è DOC (c'è il TOC) e L'UVA254\n",
    "\n",
    "# Quindi in comune abbiamo:\n",
    "# Color, TOC, Nitrati, Turbidity, pH, Temperature, Conductivity, Free Chlorine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapping = {\n",
    "    'Cloro attivo libero (al prelievo)': 'Free Chlorine (mg/L)',\n",
    "    'Colore': 'Color (CU)',\n",
    "    'Concentrazione ioni idrogeno (al prelievo)': 'pH',\n",
    "    'Conducibilità\\xa0 elettrica 20°C': 'Conductivity (uS/cm)',\n",
    "    'Carbonio organico totale': 'TOC (mg/L)',\n",
    "    'Temperatura (al prelievo)': 'Temperature (°C)',\n",
    "    'Nitrati': 'Nitrate (mg/L)',\n",
    "    'Dibromoclorometano': 'Dibromochloromethane (ug/L)',\n",
    "    'Bromoformio': 'Bromoform (ug/L)',\n",
    "    'Cloroformio': 'Chloroform (ug/L)',\n",
    "    'di cloro bromometano': 'Dichlorobromomethane (ug/L)',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.DataFrame()\n",
    "for filename in os.listdir(grab_samples_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        if grab_df.empty:\n",
    "            grab_df = pd.read_csv(os.path.join(grab_samples_folder, filename), sep=\";\")\n",
    "        else:\n",
    "            df = pd.read_csv(os.path.join(grab_samples_folder, filename), sep=';')\n",
    "            grab_df = pd.concat([grab_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(columns_mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.drop(\n",
    "    columns=[\n",
    "        'CAMPIONE',\n",
    "        'ANALITA',\n",
    "        'VALORE',\n",
    "        'UNITA\\' DI MISURA',\n",
    "    ],\n",
    "    inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = grab_df[grab_df['COMPONENTE'].isin(columns_mapping.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['COMPONENTE'] = grab_df['COMPONENTE'].map(columns_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = grab_df.pivot(\n",
    "    index=['DATA CAMPIONAMENTO', 'INDIRIZZO'],\n",
    "    columns='COMPONENTE',\n",
    "    values='VALORE TESTO'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['DATA CAMPIONAMENTO'] = pd.to_datetime(grab_df['DATA CAMPIONAMENTO'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.sort_values(\n",
    "    by=['DATA CAMPIONAMENTO'],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix LOD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_string_values(s):\n",
    "    \n",
    "    try:\n",
    "        s = s.strip()\n",
    "        s = float(s)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    if isinstance(s, (int, float)):\n",
    "        return s\n",
    "    elif pd.isna(s):\n",
    "        return None\n",
    "    else:\n",
    "        if \",\" in s:\n",
    "            s = s.replace(\",\", \".\")\n",
    "        if \"<\" in s:\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) / 2 if number else None\n",
    "        elif \">\" in s:\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) if number else None\n",
    "        elif \"*\" in s or re.search(\"[a-zA-Z]\", s):\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) if number else None\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_label(value):\n",
    "    \n",
    "    try:\n",
    "        value = value.strip()\n",
    "        value = float(value)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    if pd.isna(value):\n",
    "        return \"NaN\"\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return \"Normal\"\n",
    "    elif \"<\" in value:\n",
    "        return \"Less than\"\n",
    "    elif \">\" in value:\n",
    "        return \"Greater than\"\n",
    "    else:\n",
    "        return \"NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns:\n",
    "    label_column = column + \"_label\"\n",
    "    grab_df.loc[:, label_column] = grab_df[column].apply(set_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[columns] = grab_df[columns].map(\n",
    "    convert_string_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapping = {\n",
    "    \"Measurement interval=900[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "    \"Measurement interval=999[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "    \"Measurement interval=0[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "    \"COLORtrue - Measured value [Hazen-eq.] (Limit:0.00-300.00)\": \"Color (CU)\",\n",
    "    \"TOCeq - Measured value [mg/l] (Limit:0.00-22.00)\": \"TOC (mg/L)\",\n",
    "    \"NO3eq - Measured value [mg/l] (Limit:0.00-88.00)\": \"Nitrate (mg/L)\",\n",
    "    # \"UV254t - Measured value [Abs/m] (Limit:0.00-71.00)\": \"UVA254 (1/m)\",\n",
    "    \"pH - Measured value (Limit:0.00-14.00)\": \"pH\",\n",
    "    \"Temperature - Measured value [C] (Limit:-5.00-100.00)\": \"Temperature (°C)\",\n",
    "    \"Conductivity - Measured value [uS/cm] (Limit:0.10-600000.00)\": \"Conductivity (uS/cm)\",\n",
    "    \"Free Chlorine - Measured value [mg/l] (Limit:0.00-2.00)\": \"Free Chlorine (mg/L)\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict = {}\n",
    "\n",
    "for sensor_file in os.listdir(sensors_folder):\n",
    "    if sensor_file == \".DS_Store\":\n",
    "        continue\n",
    "\n",
    "    sensor_folder = os.path.join(sensors_folder, sensor_file)\n",
    "    for filename in os.listdir(sensor_folder):\n",
    "        if not filename.endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        house_code = filename.split(\"_\")[0]\n",
    "        \n",
    "        if filename.endswith(\".csv\"):\n",
    "            if house_code not in sensor_dict:\n",
    "                \n",
    "                df = pd.read_csv(\n",
    "                    os.path.join(sensor_folder, filename), header=1, sep=\";\"\n",
    "                )\n",
    "                df.rename(columns=columns_mapping, inplace=True)\n",
    "                \n",
    "                sensor_dict[house_code] = df\n",
    "            else:\n",
    "                df = pd.read_csv(os.path.join(sensor_folder, filename), header=1, sep=\";\")\n",
    "                \n",
    "                df.rename(columns=columns_mapping, inplace=True)\n",
    "                \n",
    "                sensor_dict[house_code] = pd.concat([sensor_dict[house_code], df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns = set(columns_mapping.values())\n",
    "\n",
    "for house_code, df in sensor_dict.items():\n",
    "    sensor_dict[house_code] = sensor_dict[house_code][list(sensor_columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.rename(\n",
    "    columns={\n",
    "        \"DATA CAMPIONAMENTO\": \"DateTime\",\n",
    "        \"INDIRIZZO\": \"Code\",\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name of Punta di prelievo values to match codes\n",
    "def change_name(name):\n",
    "    if \"Tognazzi\" in name:\n",
    "        return \"Tognazzi\"\n",
    "    elif \"Tabacchi\" in name:\n",
    "        return \"Tabacchi\"\n",
    "    elif \"Gramsci\" in name:\n",
    "        return \"Gramsci\"\n",
    "    elif \"Berna\" in name:\n",
    "        return \"Berna\"\n",
    "    elif \"Bande Nere\" in name or \"Piazzale Giovanni\" in name:\n",
    "        return \"Bande Nere\"\n",
    "    elif \"Prealpi\" in name:\n",
    "        return \"Prealpi\"\n",
    "    elif \"Chiostergi\" in name:\n",
    "        return \"Chiostergi\"\n",
    "    elif \"Montevideo\" in name or \"Montevid\" in name:\n",
    "        return \"Montevideo\"\n",
    "    elif \"Fortunato\" in name:\n",
    "        return \"Fortunato\"\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[\"Code\"] = grab_df[\"Code\"].map(change_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name of the keys to match the names in the grab_df\n",
    "sensor_dict[\"Tabacchi\"] = sensor_dict.pop(\"via TABACCHI\")\n",
    "sensor_dict[\"Tognazzi\"] = sensor_dict.pop(\"via Tognazzi\")\n",
    "sensor_dict[\"Prealpi\"] = sensor_dict.pop(\"Piazza Prealpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    sensor_df = sensor_dict.pop(code)\n",
    "    sensor_df[\"DateTime\"] = pd.to_datetime(sensor_df[\"DateTime\"])\n",
    "    sensor_df.set_index(\"DateTime\", inplace=True)\n",
    "    sensor_dict[code] = sensor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of missing values for each column\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    code_df = grab_df[grab_df[\"Code\"] == code]\n",
    "    for column in columns:\n",
    "        # count the number of missing values\n",
    "        missing_values = code_df[column].isna().sum()\n",
    "        if missing_values > 0:\n",
    "            print(\n",
    "                f\"{code} has {missing_values} missing values in column {column}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of rows that have at least one missing value\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    code_df = grab_df[grab_df[\"Code\"] == code]\n",
    "\n",
    "    missing_values = (\n",
    "        code_df[columns]\n",
    "        .isna()\n",
    "        .any(axis=1)\n",
    "        .sum()\n",
    "    )\n",
    "    if missing_values > 0:\n",
    "        print(f\"{code} has {missing_values} rows with missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the moment no imputation is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict[code]\n",
    "    for column in sensor_df.columns:\n",
    "        missing_values = sensor_df[column].isna().sum()\n",
    "        if missing_values > 0:\n",
    "            print(\n",
    "                f\"{code} has {missing_values} missing values in column {column}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of missing values is very low, so we can do implicit imputation with time interpolation\n",
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict.pop(code)\n",
    "    sensor_df.interpolate(method=\"time\", inplace=True)\n",
    "    sensor_dict[code] = sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.sort_values(by=\"DateTime\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Further Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df = pd.read_excel(\n",
    "    os.path.join(clean_data_folder, \"Riunione 24-04-2024\", \"Sensor Data.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df.Code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping = {\n",
    "    'HOUSE_BANDENERE': 'Bande Nere',\n",
    "    'HOUSE_BERNA': 'Berna',\n",
    "    'HOUSE_CHIOSTERGI': 'Chiostergi',\n",
    "    'HOUSE_FORTUNATO': 'Fortunato',\n",
    "    'HOUSE_GRAMSCI': 'Gramsci',\n",
    "    'HOUSE_MONTEVIDEO': 'Montevideo',\n",
    "    'HOUSE_PREALPI': 'Prealpi',\n",
    "    'HOUSE_TOGNAZZI': 'Tognazzi',\n",
    "    'HOUSE_TABACCHI': 'Tabacchi',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df[\"Code\"] = first_batch_sensor_df[\"Code\"].map(code_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in first_batch_sensor_df[\"Code\"].unique():\n",
    "    df = first_batch_sensor_df[first_batch_sensor_df[\"Code\"] == code].copy()\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "    df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "    df.drop(columns=[\"Code\"], inplace=True)\n",
    "\n",
    "    sensor_df = sensor_dict.pop(code)\n",
    "\n",
    "    sensor_df = pd.concat([sensor_df, df])\n",
    "    sensor_df.sort_index(inplace=True)\n",
    "    sensor_dict[code] = sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict[\"Berna\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Tabacchi Late Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_folder = os.path.join(raw_data_folder, \"Tabacchi Late\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(tabacchi_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        if tabacchi_df.empty:\n",
    "            tabacchi_df = pd.read_csv(os.path.join(tabacchi_folder, filename), sep=\";\")\n",
    "        else:\n",
    "            df = pd.read_csv(os.path.join(tabacchi_folder, filename), sep=\";\")\n",
    "            tabacchi_df = pd.concat([tabacchi_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df['Tag Name'] = tabacchi_df['Tag Name'].apply(lambda x: x.split(\"_\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df['Tag Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df.drop(\n",
    "    columns=[\n",
    "        'Chart',\n",
    "        'Historian Tag Name',\n",
    "        'Quality',\n",
    "    ],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color, TOC, Nitrati, Turbidity, pH, Temperature, Conductivity, Free Chlorine are the ones we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapping = {\n",
    "    'NO3eq': 'Nitrate (mg/L)',\n",
    "    'Colore': 'Color (CU)',\n",
    "    'TOCeq': 'TOC (mg/L)',\n",
    "    'pH': 'pH',\n",
    "    'Turbidity': 'Turbidity (NTU)',\n",
    "    'Temperature': 'Temperature (°C)',\n",
    "    'Conductivity': 'Conductivity (uS/cm)',\n",
    "    'Chlorine': 'Free Chlorine (mg/L)',\n",
    "    'SAC254': 'UVA254 (1/m)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df = tabacchi_df.pivot(\n",
    "    index=['TimeStamp'],\n",
    "    columns='Tag Name',\n",
    "    values='Value'\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = tabacchi_df.columns.difference(list(columns_mapping.keys()) + ['TimeStamp']) \n",
    "tabacchi_df.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df.rename(columns=columns_mapping, inplace=True)\n",
    "tabacchi_df.rename(\n",
    "    columns={\n",
    "        'TimeStamp': 'DateTime',\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df['DateTime'] = pd.to_datetime(tabacchi_df['DateTime'], format='mixed')\n",
    "# remove the time zone information\n",
    "tabacchi_df['DateTime'] = tabacchi_df['DateTime'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df.sort_values(\n",
    "    by=['DateTime'],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabacchi_df.set_index(\"DateTime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the df to sensor_dict\n",
    "df = sensor_dict.pop(\"Tabacchi\")\n",
    "df = pd.concat([df, tabacchi_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)\n",
    "sensor_dict[\"Tabacchi\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "\n",
    "# - GRAMSCI\n",
    "# Turbidity selected upper threshold is 1.5\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 0.8\n",
    "# UVA254 selected upper threshold is 1.5\n",
    "\n",
    "\n",
    "# - BERNA\n",
    "# Turbidity selected upper threshold is 1.5\n",
    "# Temperature selected upper threshold is 19.5\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 0.2\n",
    "\n",
    "# - BANDE NERE\n",
    "# Turbidity selected upper threshold is 1\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Nitrate selected lower threshold is 20\n",
    "# UVA254 selected upper threshold is 0.4\n",
    "\n",
    "# - CHIOSTREGI\n",
    "# free chlorine selected upper threshold is 0.06\n",
    "\n",
    "# - FORTUNATO\n",
    "# Turbidity selected upper threshold is 1\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Nitrate selected lower threshold is 25\n",
    "# UVA254 selected upper threshold is 0.4\n",
    "\n",
    "# - MONTEVIDEO\n",
    "# Color selected upper threshold is 4\n",
    "# Turbidity selected upper threshold is 1\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 1\n",
    "# Nitrate selected lower threshold is 20\n",
    "# TOC selected upper threshold is 1\n",
    "# UVA254 selected upper threshold is 4\n",
    "\n",
    "# - PREALPI\n",
    "# Turbidity selected upper threshold is 0.7\n",
    "# UVA254 selected upper threshold is 1.5\n",
    "\n",
    "# - TABACCHI\n",
    "\n",
    "# - TOGNAZZI\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 0.4\n",
    "\n",
    "thresholds = {\n",
    "    \"Gramsci\": {\n",
    "        \"Turbidity (NTU)\": 1.5,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        # \"Free Chlorine (mg/l)\": 0.8,\n",
    "        \"UVA254 (1/m)\": 1.5,\n",
    "    },\n",
    "    \"Berna\": {\n",
    "        \"Turbidity (NTU)\": 1.5,\n",
    "        \"Temperature (°C)\": 19.5,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        # \"Free Chlorine (mg/L)\": 0.2,\n",
    "        \"UVA254 (1/m)\": 1.5,\n",
    "    },\n",
    "    \"Bande Nere\": {\n",
    "        \"Turbidity (NTU)\": 1,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        \"Nitrate (mg/L)\": 20,\n",
    "        \"UVA254 (1/m)\": 0.4,\n",
    "    },\n",
    "    \"Chiostergi\": {\n",
    "        # \"Free Chlorine (mg/L)\": 0.06,\n",
    "    },\n",
    "    \"Fortunato\": {\n",
    "        \"Turbidity (NTU)\": 1,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        \"Free Chlorine (mg/L)\": 1.0,\n",
    "        \"Nitrate (mg/L)\": 25,\n",
    "        \"UVA254 (1/m)\": 0.4,\n",
    "        \"TOC (mg/L)\": 1,\n",
    "    },\n",
    "    \"Montevideo\": {\n",
    "        \"Color (CU)\": 4,\n",
    "        \"Turbidity (NTU)\": 1,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        \"Free Chlorine (mg/L)\": 1,\n",
    "        \"Nitrate (mg/L)\": 20,\n",
    "        \"TOC (mg/L)\": 1,\n",
    "        \"UVA254 (1/m)\": 4,\n",
    "    },\n",
    "    \"Prealpi\": {\n",
    "        \"Turbidity (NTU)\": 0.7,\n",
    "        \"UVA254 (1/m)\": 1.5,\n",
    "    },\n",
    "    \"Tabacchi\": {\n",
    "        \"Nitrate (mg/L)\": 14,\n",
    "        \"Temperature (°C)\": 10,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        \"pH\": 6,\n",
    "        \n",
    "    },\n",
    "    \"Tognazzi\": {\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        \"Free Chlorine (mg/L)\": 0.4,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fixare questo poichè ora con Tabacchi non c'è più consistenza tra variabili e lower thrshold e upper threshold\n",
    "\n",
    "figsize = (30, 20)\n",
    "plt.rcParams.update({\"font.size\": 22})\n",
    "\n",
    "for code in sensor_dict.keys():\n",
    "    \n",
    "    sensor_df = sensor_dict[code]\n",
    "    for column in sensor_df.columns:\n",
    "        df = sensor_df[column].copy()\n",
    "\n",
    "        # drop rows with duplicated index\n",
    "        df = df[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "        # plot the data with the thresholds for the variables that have them\n",
    "        # and compare the distribution of the values with the thresholds\n",
    "\n",
    "        if column in thresholds[code]:\n",
    "            threshold = thresholds[code][column]\n",
    "            fig, ax = plt.subplots(2, 2, figsize=figsize)\n",
    "            sns.lineplot(x=df.index, y=df, ax=ax[0, 0])\n",
    "            ax[0, 0].set_title(f\"Raw Data\")\n",
    "            ax[0, 0].set_ylabel(column)\n",
    "            ax[0, 0].set_xlabel(\"DateTime\")\n",
    "            ax[0, 0].grid()\n",
    "\n",
    "            fig_hist = sns.histplot(\n",
    "                df, bins=50, kde=True, stat=\"probability\", ax=ax[1, 0]\n",
    "            )\n",
    "            ax[1, 0].set_title(f\"Raw Data\")\n",
    "            ax[1, 0].set_ylabel(\"Probability\")\n",
    "            ax[1, 0].set_xlabel(column)\n",
    "            ax[1, 0].grid()\n",
    "\n",
    "            if (column not in [\"Conductivity (uS/cm)\", \"Nitrate (mg/L)\"]) and (code != \"Tabacchi\"):\n",
    "                ax[0, 0].axhline(\n",
    "                    y=threshold,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"dashed\",\n",
    "                    label=\"Upper Threshold\",\n",
    "                )\n",
    "                ax[0, 0].text(\n",
    "                    df.index[0],\n",
    "                    threshold,\n",
    "                    f\"Upper Threshold: {threshold}\",\n",
    "                    color=\"r\",\n",
    "                    va=\"bottom\",\n",
    "                )\n",
    "                ax[1, 0].axvline(\n",
    "                    x=threshold,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"dashed\",\n",
    "                    label=\"Upper Threshold\",\n",
    "                )\n",
    "                ax[1, 0].text(\n",
    "                    threshold,\n",
    "                    fig_hist.get_ylim()[1],\n",
    "                    f\"Upper Threshold: {threshold}\",\n",
    "                    color=\"r\",\n",
    "                    rotation=90,\n",
    "                    ha=\"right\",\n",
    "                    va=\"top\",\n",
    "                )\n",
    "                df = df[df <= threshold]\n",
    "            else:\n",
    "                ax[0, 0].axhline(\n",
    "                    y=threshold,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"dashed\",\n",
    "                    label=\"Lower Threshold\",\n",
    "                )\n",
    "                ax[0, 0].text(\n",
    "                    df.index[0],\n",
    "                    threshold,\n",
    "                    f\"Lower Threshold: {threshold}\",\n",
    "                    color=\"r\",\n",
    "                    va=\"bottom\",\n",
    "                )\n",
    "                ax[1, 0].axvline(\n",
    "                    x=threshold,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"dashed\",\n",
    "                    label=\"Lower Threshold\",\n",
    "                )\n",
    "                ax[1, 0].text(\n",
    "                    threshold,\n",
    "                    fig_hist.get_ylim()[1],\n",
    "                    f\"Lower Threshold: {threshold}\",\n",
    "                    color=\"r\",\n",
    "                    rotation=90,\n",
    "                    ha=\"right\",\n",
    "                    va=\"top\",\n",
    "                )\n",
    "                df = df[df >= threshold]\n",
    "\n",
    "            sns.lineplot(x=df.index, y=df, ax=ax[0, 1], color=\"g\")\n",
    "            ax[0, 1].set_title(f\"Filtered Data\")\n",
    "            ax[0, 1].set_ylabel(column)\n",
    "            ax[0, 1].set_xlabel(\"DateTime\")\n",
    "            ax[0, 1].grid()\n",
    "\n",
    "            sns.histplot(\n",
    "                df,\n",
    "                bins=50,\n",
    "                kde=True,\n",
    "                stat=\"probability\",\n",
    "                ax=ax[1, 1],\n",
    "                color=\"g\",\n",
    "            )\n",
    "            ax[1, 1].set_title(f\"Filtered Data\")\n",
    "            ax[1, 1].set_ylabel(\"Probability\")\n",
    "            ax[1, 1].set_xlabel(column)\n",
    "            ax[1, 1].grid()\n",
    "        else:\n",
    "            plt.figure(figsize=figsize)\n",
    "            sns.lineplot(x=df.index, y=df)\n",
    "            plt.ylabel(column)\n",
    "            plt.xlabel(\"DateTime\")\n",
    "            plt.grid()\n",
    "\n",
    "        plt.suptitle(f\"{code} - {column}\", fontsize=30)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        column_ = column.replace(\"/\", \"_\")\n",
    "\n",
    "        path = os.path.join(plot_folder, \"Clean Data\", \"Removed Outliers\", code)\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # plt.savefig(\n",
    "        #     os.path.join(\n",
    "        #         plot_folder,\n",
    "        #         \"Clean Data\",\n",
    "        #         \"Removed Outliers\",\n",
    "        #         code,\n",
    "        #         f\"{column_}.png\",\n",
    "        #     ),\n",
    "        #     dpi=300,\n",
    "        # )\n",
    "        # plt.close()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows that have values outside the thresholds\n",
    "for code in thresholds.keys():\n",
    "    sensor_df = sensor_dict[code].copy()\n",
    "\n",
    "    for column in thresholds[code].keys():\n",
    "        threshold = thresholds[code][column]\n",
    "        df = sensor_df[column].copy()\n",
    "\n",
    "        df = (\n",
    "            df[df > threshold]\n",
    "            if column not in [\"Conductivity (uS/cm)\", \"Nitrate (mg/L)\"]\n",
    "            else df[df < threshold]\n",
    "        )\n",
    "\n",
    "        sensor_df.loc[df.index, column] = np.nan\n",
    "\n",
    "    sensor_df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    sensor_dict.update({code: sensor_df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.to_excel(os.path.join(clean_data_folder, \"grab.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(clean_data_folder, \"sensors\")):\n",
    "    os.mkdir(os.path.join(clean_data_folder, \"sensors\"))\n",
    "\n",
    "for code in sensor_dict.keys():\n",
    "    sensor_dict[code].to_excel(\n",
    "        os.path.join(clean_data_folder, \"sensors\", f\"{code}.xlsx\"), index=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
