{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Points (Case dell'Acqua) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"..\", \"utils\")\n",
    "\n",
    "with open(os.path.join(utils_folder, \"onedrive.txt\"), \"r\") as f:\n",
    "    cloud_data_folder = os.path.join(f.readline().strip(), \"Case dell'acqua\")\n",
    "\n",
    "grab_samples_folder = os.path.join(cloud_data_folder, \"Grab Samples\")\n",
    "sensors_folder = os.path.join(cloud_data_folder, \"Sensori\")\n",
    "\n",
    "local_data_folder = os.path.join(\"..\", \"..\", \"data\")\n",
    "intermediate_data_folder = os.path.join(local_data_folder, \"Intermediate Data\")\n",
    "clean_data_folder = os.path.join(local_data_folder, \"Clean Data\")\n",
    "raw_data_folder = os.path.join(local_data_folder, \"Raw Data\")\n",
    "\n",
    "plot_folder = os.path.join(local_data_folder, \"Plots\")\n",
    "\n",
    "all_grab_samples_path = os.path.join(\n",
    "    raw_data_folder, \"Tutti punti - Grab Samples\"\n",
    ")\n",
    "\n",
    "grab_samples_supply_points_path = os.path.join(\n",
    "    raw_data_folder,\n",
    "    \"Case dell'acqua - Grab Samples (main)/0. Case acqua - 2010-2023.xlsx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tra i grab non c'è l'ORP, mentre\n",
    "# tra i sensori non c'è DOC (c'è il TOC) e L'UVA254\n",
    "\n",
    "# Quindi in comune abbiamo:\n",
    "# Color, TOC, Nitrati, Turbidity, pH, Temperature, Conductivity, Free Chlorine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.DataFrame()\n",
    "for filename in os.listdir(grab_samples_folder):\n",
    "    if grab_df.empty:\n",
    "        grab_df = pd.read_excel(os.path.join(grab_samples_folder, filename))\n",
    "    else:\n",
    "        df = pd.read_excel(os.path.join(grab_samples_folder, filename))\n",
    "        grab_df = pd.concat([grab_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(utils_folder, \"columns_types.json\")) as f:\n",
    "    column_types = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns = column_types[\"metadata_columns\"]\n",
    "features_columns = column_types[\"features_columns\"]\n",
    "targets_columns = column_types[\"targets_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_metadata_columns = list(\n",
    "    set(metadata_columns).intersection(grab_df.columns)\n",
    ")\n",
    "common_features_columns = list(\n",
    "    set(features_columns).intersection(grab_df.columns)\n",
    ")\n",
    "common_targets_columns = list(\n",
    "    set(targets_columns).intersection(grab_df.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that are not in the column_types.json file\n",
    "grab_df = grab_df[\n",
    "    common_metadata_columns + common_features_columns + common_targets_columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix LOD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def convert_string_values(s):\n",
    "    if isinstance(s, (int, float)):\n",
    "        return s\n",
    "    elif pd.isna(s):\n",
    "        return None\n",
    "    else:\n",
    "        if \",\" in s:\n",
    "            s = s.replace(\",\", \".\")\n",
    "        if \"<\" in s:\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) / 2 if number else None\n",
    "        elif \">\" in s:\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) if number else None\n",
    "        elif \"*\" in s or re.search(\"[a-zA-Z]\", s):\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) if number else None\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_label(value):\n",
    "    if pd.isna(value):\n",
    "        return \"NaN\"\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return \"Normal\"\n",
    "    elif \"<\" in value:\n",
    "        return \"Less than\"\n",
    "    elif \">\" in value:\n",
    "        return \"Greater than\"\n",
    "    else:\n",
    "        return \"NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add TTHMs columns as the sum of the four TTHMs columns\n",
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_features_columns + common_targets_columns:\n",
    "    label_column = column + \"_label\"\n",
    "    grab_df.loc[:, label_column] = grab_df[column].apply(set_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[common_features_columns] = grab_df[common_features_columns].map(\n",
    "    convert_string_values\n",
    ")\n",
    "\n",
    "grab_df[common_targets_columns] = grab_df[common_targets_columns].map(\n",
    "    convert_string_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict = {}\n",
    "\n",
    "for sensor_file in os.listdir(sensors_folder):\n",
    "    if sensor_file == \".DS_Store\":\n",
    "        continue\n",
    "\n",
    "    sensor_folder = os.path.join(sensors_folder, sensor_file)\n",
    "    for filename in os.listdir(sensor_folder):\n",
    "        if not filename.endswith(\".xlsx\"):\n",
    "            continue\n",
    "\n",
    "        house_code = filename.split(\"_\")[0]\n",
    "        if house_code not in sensor_dict:\n",
    "            sensor_dict[house_code] = pd.read_excel(\n",
    "                os.path.join(sensor_folder, filename), header=1\n",
    "            )\n",
    "        else:\n",
    "            df = pd.read_excel(os.path.join(sensor_folder, filename), header=1)\n",
    "            sensor_dict[house_code] = pd.concat([sensor_dict[house_code], df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict[\"via TABACCHI\"].columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapping = {\n",
    "    \"Measurement interval=900[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "    \"Measurement interval=999[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "    \"Measurement interval=0[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "    \"COLORtrue - Measured value [Hazen-eq.] (Limit:0.00-300.00)\": \"Color (CU)\",\n",
    "    \"TOCeq - Measured value [mg/l] (Limit:0.00-22.00)\": \"TOC (mg/l)\",\n",
    "    \"NO3eq - Measured value [mg/l] (Limit:0.00-88.00)\": \"Nitrate (mg/l)\",\n",
    "    \"UV254t - Measured value [Abs/m] (Limit:0.00-71.00)\": \"UVA254 (1/m)\",\n",
    "    \"Turbidity - Measured value [FTUeq] (Limit:0.00-170.00)\": \"Turbidity (FTU)\",\n",
    "    \"pH - Measured value (Limit:0.00-14.00)\": \"pH\",\n",
    "    \"Temperature - Measured value [C] (Limit:-5.00-100.00)\": \"Temperature (°C)\",\n",
    "    \"Conductivity - Measured value [uS/cm] (Limit:0.10-600000.00)\": \"Conductivity (μS/cm)\",\n",
    "    \"Free Chlorine - Measured value [mg/l] (Limit:0.00-2.00)\": \"Free Chlorine (mg/l)\",\n",
    "}\n",
    "\n",
    "\n",
    "for house_code, df in sensor_dict.items():\n",
    "    sensor_dict[house_code] = df.rename(columns=columns_mapping)\n",
    "\n",
    "    # set to get unique values\n",
    "    columns = set(columns_mapping.values())\n",
    "\n",
    "    sensor_dict[house_code] = sensor_dict[house_code][list(columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.drop(\n",
    "    columns=[\n",
    "        \"Codice punto di prelievo\",\n",
    "        \"Rapporto di prova\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[\"Punto di prelievo\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name of Punta di prelievo values to match codes\n",
    "def change_name(name):\n",
    "    if \"Tognazzi\" in name:\n",
    "        return \"Tognazzi\"\n",
    "    elif \"Tabacchi\" in name:\n",
    "        return \"Tabacchi\"\n",
    "    elif \"Gramsci\" in name:\n",
    "        return \"Gramsci\"\n",
    "    elif \"Berna\" in name:\n",
    "        return \"Berna\"\n",
    "    elif \"Bande Nere\" in name or \"Piazzale Giovanni\" in name:\n",
    "        return \"Bande Nere\"\n",
    "    elif \"Prealpi\" in name:\n",
    "        return \"Prealpi\"\n",
    "    elif \"Chiostergi\" in name:\n",
    "        return \"Chiostergi\"\n",
    "    elif \"Montevideo\" in name or \"Montevid\" in name:\n",
    "        return \"Montevideo\"\n",
    "    elif \"Fortunato\" in name:\n",
    "        return \"Fortunato\"\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[\"Punto di prelievo\"] = grab_df[\"Punto di prelievo\"].map(change_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[\"Data di prelievo\"] = pd.to_datetime(grab_df[\"Data di prelievo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name of the keys to match the names in the grab_df\n",
    "sensor_dict[\"Tabacchi\"] = sensor_dict.pop(\"via TABACCHI\")\n",
    "sensor_dict[\"Tognazzi\"] = sensor_dict.pop(\"via Tognazzi\")\n",
    "sensor_dict[\"Prealpi\"] = sensor_dict.pop(\"Piazza Prealpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Punto di prelievo\"].unique():\n",
    "    sensor_df = sensor_dict.pop(code)\n",
    "    sensor_df[\"DateTime\"] = pd.to_datetime(sensor_df[\"DateTime\"])\n",
    "    sensor_df.set_index(\"DateTime\", inplace=True)\n",
    "    sensor_dict[code] = sensor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of missing values for each column\n",
    "for code in grab_df[\"Punto di prelievo\"].unique():\n",
    "    code_df = grab_df[grab_df[\"Punto di prelievo\"] == code]\n",
    "    for column in common_features_columns + common_targets_columns:\n",
    "        # count the number of missing values\n",
    "        missing_values = code_df[column].isna().sum()\n",
    "        if missing_values > 0:\n",
    "            print(\n",
    "                f\"{code} has {missing_values} missing values in column {column}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of rows that have at least one missing value\n",
    "for code in grab_df[\"Punto di prelievo\"].unique():\n",
    "    code_df = grab_df[grab_df[\"Punto di prelievo\"] == code]\n",
    "\n",
    "    missing_values = (\n",
    "        code_df[common_features_columns + common_targets_columns]\n",
    "        .isna()\n",
    "        .any(axis=1)\n",
    "        .sum()\n",
    "    )\n",
    "    if missing_values > 0:\n",
    "        print(f\"{code} has {missing_values} rows with missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the Berna rows with missing values\n",
    "row_index = grab_df[\n",
    "    (grab_df[\"Punto di prelievo\"] == \"Berna\")\n",
    "    & (\n",
    "        grab_df[common_features_columns + common_targets_columns]\n",
    "        .isna()\n",
    "        .any(axis=1)\n",
    "    )\n",
    "].index\n",
    "\n",
    "grab_df.drop(row_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the moment no imputation is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict[code]\n",
    "    for column in sensor_df.columns:\n",
    "        missing_values = sensor_df[column].isna().sum()\n",
    "        if missing_values > 0:\n",
    "            print(\n",
    "                f\"{code} has {missing_values} missing values in column {column}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of missing values is very low, so we can do implicit imputation with time interpolation\n",
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict.pop(code)\n",
    "    sensor_df.interpolate(method=\"time\", inplace=True)\n",
    "    sensor_dict[code] = sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Historical Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples = []\n",
    "\n",
    "for file in os.listdir(all_grab_samples_path):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(all_grab_samples_path, file), header=11)\n",
    "    else:\n",
    "        df = pd.read_excel(os.path.join(all_grab_samples_path, file), header=15)\n",
    "    common_cols = list(\n",
    "        set(df.columns.to_list())\n",
    "        & set(metadata_columns + features_columns + targets_columns)\n",
    "    )\n",
    "    df = df[common_cols]\n",
    "    grab_samples.append(df)\n",
    "\n",
    "grab_samples_df = pd.concat(grab_samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = \"CS, CT\"\n",
    "\n",
    "meta_supply_points_df = pd.read_excel(\n",
    "    grab_samples_supply_points_path, usecols=column_list, header=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_supply_points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df = grab_samples_df.merge(\n",
    "    meta_supply_points_df,\n",
    "    left_on=[\"Punto di prelievo\", \"Codice punto di prelievo\"],\n",
    "    right_on=[\"filtro 1\", \"filtro 2\"],\n",
    "    how=\"inner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supply_points_df.drop(columns=[\"filtro 1\", \"filtro 2\"], inplace=True)\n",
    "hist_grab_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all value columns in the mapping to the corresponding key column\n",
    "column_mapping = {\n",
    "    \"Temperatura (°C)\": [\n",
    "        \"Temperatura - °C\",\n",
    "        \"Temperatura (al prelievo) (°C)\",\n",
    "    ],\n",
    "    \"Cloro residuo libero (mg/L di Cl2)\": [\n",
    "        \"Cloro residuo libero (al prelievo) (mg/L di Cl2)\",\n",
    "    ],\n",
    "    \"Torbidità (NTU)\": [\n",
    "        \"Torbidità (NTu)\",\n",
    "    ],\n",
    "    \"Batteri coliformi a 37°C (MPN/100 mL)\": [\n",
    "        \"Batteri coliformi a 37°C (MPN / 100 mL)\",\n",
    "    ],\n",
    "    \"Colore (CU)\": [\n",
    "        \"Colore (Cu)\",\n",
    "    ],\n",
    "    \"Escherichia coli (MPN/100 mL)\": [\n",
    "        \"Escherichia Coli (MPN / 100mL)\",\n",
    "    ],\n",
    "    \"Enterococchi (MPN/100 mL)\": [\n",
    "        \"Enterococchi (MPN / 100mL)\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for final_column, original_columns in column_mapping.items():\n",
    "    for original_column in original_columns:\n",
    "        hist_grab_df[final_column] = hist_grab_df[final_column].combine_first(\n",
    "            hist_grab_df[original_column]\n",
    "        )\n",
    "    hist_grab_df.drop(columns=original_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns = column_types[\"metadata_columns\"]\n",
    "features_columns = column_types[\"features_columns\"]\n",
    "targets_columns = column_types[\"targets_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_common_metadata_columns = list(\n",
    "    set(metadata_columns).intersection(hist_grab_df.columns)\n",
    ")\n",
    "hist_common_features_columns = list(\n",
    "    set(features_columns).intersection(hist_grab_df.columns)\n",
    ")\n",
    "hist_common_targets_columns = list(\n",
    "    set(targets_columns).intersection(hist_grab_df.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_columns(title, columns):\n",
    "    print(f\"{title}:\")\n",
    "    for col in columns:\n",
    "        print(f\"  - {col}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print_columns(\n",
    "    \"Historical Common Metadata Columns\", hist_common_metadata_columns\n",
    ")\n",
    "print_columns(\"Common Metadata Columns\", common_metadata_columns)\n",
    "print_columns(\n",
    "    \"Historical Common Features Columns\", hist_common_features_columns\n",
    ")\n",
    "print_columns(\"Common Features Columns\", common_features_columns)\n",
    "print_columns(\"Historical Common Targets Columns\", hist_common_targets_columns)\n",
    "print_columns(\"Common Targets Columns\", common_targets_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a mapping of the hist_common_features_columns to the common_features_columns (sorted)\n",
    "# and the hist_common_targets_columns to the common_targets_columns (sorted)\n",
    "\n",
    "# the mapping is done by sorting the columns and then zipping them together\n",
    "mapping_features = dict(\n",
    "    zip(sorted(hist_common_features_columns), sorted(common_features_columns))\n",
    ")\n",
    "mapping_targets = dict(\n",
    "    zip(sorted(hist_common_targets_columns), sorted(common_targets_columns))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.rename(columns=mapping_features, inplace=True)\n",
    "hist_grab_df.rename(columns=mapping_targets, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.drop(\n",
    "    columns=[\n",
    "        \"filtro 1\",\n",
    "        \"filtro 2\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the hist_grab_df rows that have the Punto di prelievo containing the grab_df Punto di prelievo\n",
    "hist_grab_df = hist_grab_df[\n",
    "    (\n",
    "        hist_grab_df[\"Punto di prelievo\"].str.contains(\n",
    "            \"|\".join(grab_df[\"Punto di prelievo\"].unique()),\n",
    "            case=False,\n",
    "            na=False,\n",
    "        )\n",
    "    )\n",
    "    | (\n",
    "        hist_grab_df[\"Codice punto di prelievo\"].str.contains(\n",
    "            \"|\".join(grab_df[\"Punto di prelievo\"].unique()),\n",
    "            case=False,\n",
    "            na=False,\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[\"Punto di prelievo\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import contains\n",
    "\n",
    "\n",
    "# do a function that for a value, if an item of grab_df['Punto di prelievo'].unique() is contained in the value, then change the value to the item\n",
    "def change_name(value):\n",
    "    for name in grab_df[\"Punto di prelievo\"].unique():\n",
    "        if contains(value, name):\n",
    "            return name\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df[\"Punto di prelievo\"] = hist_grab_df[\"Punto di prelievo\"].map(\n",
    "    change_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_features_columns + common_targets_columns:\n",
    "    if column not in hist_grab_df.columns:\n",
    "        continue\n",
    "    label_column = column + \"_label\"\n",
    "    hist_grab_df.loc[:, label_column] = hist_grab_df[column].apply(set_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_features_columns + common_targets_columns:\n",
    "    if column not in hist_grab_df.columns:\n",
    "        continue\n",
    "    hist_grab_df[column] = hist_grab_df[column].map(convert_string_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.shape, hist_grab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me the columns that are in grab_df but not in hist_grab_df\n",
    "for column in grab_df.columns:\n",
    "    if column not in hist_grab_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in hist_grab_df.columns:\n",
    "    if column not in grab_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.drop(\n",
    "    columns=[\n",
    "        \"Rapporto di prova\",\n",
    "        \"Codice punto di prelievo\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.concat([grab_df, hist_grab_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "# FIXME this piece of code needs to be rearranged\n",
    "columns_mapping = {\n",
    "    \"Data di prelievo\": \"DateTime\",\n",
    "    \"Punto di prelievo\": \"Code\",\n",
    "    \"Colore (Cu)\": \"Color (CU)\",\n",
    "    \"Cloro residuo libero (al prelievo) (mg/L di Cl2)\": \"Free Chlorine (mg/L)\",\n",
    "    \"Concentrazione ioni idrogeno (unità pH)\": \"pH\",\n",
    "    \"Conduttività a 20°C (µS/cm)\": \"Conductivity (uS/cm)\",\n",
    "    \"TOC - carbonio organico totale (mg/L di C)\": \"TOC (mg/L)\",\n",
    "    \"Temperatura (al prelievo) (°C)\": \"Temperature (°C)\",\n",
    "    \"Torbidità (NTu)\": \"Turbidity (NTU)\",\n",
    "    \"Nitrati (mg/L)\": \"Nitrate (mg/L)\",\n",
    "}\n",
    "\n",
    "grab_df.rename(columns=columns_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with First Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df = pd.read_excel(\n",
    "    os.path.join(clean_data_folder, \"Riunione 24-04-2024\", \"Grab Samples.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df = first_batch_grab_df[\n",
    "    first_batch_grab_df[\"Type\"] == \"Ingresso\"\n",
    "]\n",
    "first_batch_grab_df.drop(columns=[\"Type\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df[\"Code\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[\"Code\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping = {\n",
    "    \"HOUSE_BANDENERE\": \"Bande Nere\",\n",
    "    \"HOUSE_BERNA\": \"Berna\",\n",
    "    \"HOUSE_CHIOSTERGI\": \"Chiostergi\",\n",
    "    \"HOUSE_FORTUNATO\": \"Fortunato\",\n",
    "    \"HOUSE_GRAMSCI\": \"Gramsci\",\n",
    "    \"HOUSE_MONTEVIDEO\": \"Montevideo\",\n",
    "    \"HOUSE_PREALPI\": \"Prealpi\",\n",
    "    \"HOUSE_TABACCHI\": \"Tabacchi\",\n",
    "    \"HOUSE_TOGNAZZI\": \"Tognazzi\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df[\"Code\"] = first_batch_grab_df[\"Code\"].map(code_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.shape, first_batch_grab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in grab_df.columns:\n",
    "    if column not in first_batch_grab_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in first_batch_grab_df.columns:\n",
    "    if column not in grab_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataframes\n",
    "grab_df = pd.concat([grab_df, first_batch_grab_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.sort_values(by=\"DateTime\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first rename the columns of the current sensors_dfs\n",
    "\n",
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict[code].copy()\n",
    "\n",
    "    sensor_df.rename(\n",
    "        columns={\n",
    "            \"Conductivity (μS/cm)\": \"Conductivity (uS/cm)\",\n",
    "            \"TOC (mg/l)\": \"TOC (mg/L)\",\n",
    "            \"Nitrate (mg/l)\": \"Nitrate (mg/L)\",\n",
    "            \"Free Chlorine (mg/l)\": \"Free Chlorine (mg/L)\",\n",
    "            \"Turbidity (FTU)\": \"Turbidity (NTU)\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    sensor_dict.update({code: sensor_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df = pd.read_excel(\n",
    "    os.path.join(clean_data_folder, \"Riunione 24-04-2024\", \"Sensor Data.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df[\"Code\"] = first_batch_sensor_df[\"Code\"].map(code_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df.drop(columns=[\"Flow Rate (m³/s)\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict[\"Bande Nere\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in first_batch_sensor_df[\"Code\"].unique():\n",
    "    df = first_batch_sensor_df[first_batch_sensor_df[\"Code\"] == code].copy()\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "    df.set_index(\"DateTime\", inplace=True)\n",
    "\n",
    "    df.drop(columns=[\"Code\"], inplace=True)\n",
    "\n",
    "    sensor_df = sensor_dict.pop(code)\n",
    "\n",
    "    sensor_df = pd.concat([sensor_df, df])\n",
    "    sensor_df.sort_index(inplace=True)\n",
    "    sensor_dict[code] = sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict[\"Berna\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.rename(\n",
    "    columns={\n",
    "        \"Conductivity (μS/cm)\": \"Conductivity (uS/cm)\",\n",
    "        \"TOC (mg/l)\": \"TOC (mg/L)\",\n",
    "        \"Nitrate (mg/l)\": \"Nitrate (mg/L)\",\n",
    "        \"Free Chlorine (mg/l)\": \"Free Chlorine (mg/L)\",\n",
    "        \"Turbidity (FTU)\": \"Turbidity (NTU)\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "\n",
    "# - GRAMSCI\n",
    "# Turbidity selected upper threshold is 1.5\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 0.8\n",
    "# UVA254 selected upper threshold is 1.5\n",
    "\n",
    "\n",
    "# - BERNA\n",
    "# Turbidity selected upper threshold is 1.5\n",
    "# Temperature selected upper threshold is 19.5\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 0.2\n",
    "\n",
    "# - BANDE NERE\n",
    "# Turbidity selected upper threshold is 1\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Nitrate selected lower threshold is 20\n",
    "# UVA254 selected upper threshold is 0.4\n",
    "\n",
    "# - CHIOSTREGI\n",
    "# free chlorine selected upper threshold is 0.06\n",
    "\n",
    "# - FORTUNATO\n",
    "# Turbidity selected upper threshold is 1\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Nitrate selected lower threshold is 25\n",
    "# UVA254 selected upper threshold is 0.4\n",
    "\n",
    "# - MONTEVIDEO\n",
    "# Color selected upper threshold is 4\n",
    "# Turbidity selected upper threshold is 1\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 1\n",
    "# Nitrate selected lower threshold is 20\n",
    "# TOC selected upper threshold is 1\n",
    "# UVA254 selected upper threshold is 4\n",
    "\n",
    "# - PREALPI\n",
    "# Turbidity selected upper threshold is 0.7\n",
    "# UVA254 selected upper threshold is 1.5\n",
    "\n",
    "# - TABACCHI\n",
    "\n",
    "# - TOGNAZZI\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 0.4\n",
    "\n",
    "thresholds = {\n",
    "    \"Gramsci\": {\n",
    "        \"Turbidity (NTU)\": 1.5,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        # \"Free Chlorine (mg/l)\": 0.8,\n",
    "        \"UVA254 (1/m)\": 1.5,\n",
    "    },\n",
    "    \"Berna\": {\n",
    "        \"Turbidity (NTU)\": 1.5,\n",
    "        \"Temperature (°C)\": 19.5,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        # \"Free Chlorine (mg/L)\": 0.2,\n",
    "        \"UVA254 (1/m)\": 1.5,\n",
    "    },\n",
    "    \"Bande Nere\": {\n",
    "        \"Turbidity (NTU)\": 1,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        \"Nitrate (mg/L)\": 20,\n",
    "        \"UVA254 (1/m)\": 0.4,\n",
    "    },\n",
    "    \"Chiostergi\": {\n",
    "        \"Free Chlorine (mg/L)\": 0.06,\n",
    "    },\n",
    "    \"Fortunato\": {\n",
    "        \"Turbidity (NTU)\": 1,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        \"Nitrate (mg/L)\": 25,\n",
    "        \"UVA254 (1/m)\": 0.4,\n",
    "    },\n",
    "    \"Montevideo\": {\n",
    "        \"Color (CU)\": 4,\n",
    "        \"Turbidity (NTU)\": 1,\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        \"Free Chlorine (mg/L)\": 1,\n",
    "        \"Nitrate (mg/L)\": 20,\n",
    "        \"TOC (mg/L)\": 1,\n",
    "        \"UVA254 (1/m)\": 4,\n",
    "    },\n",
    "    \"Prealpi\": {\n",
    "        \"Turbidity (NTU)\": 0.7,\n",
    "        \"UVA254 (1/m)\": 1.5,\n",
    "    },\n",
    "    \"Tabacchi\": {},\n",
    "    \"Tognazzi\": {\n",
    "        \"Conductivity (uS/cm)\": 400,\n",
    "        \"Free Chlorine (mg/L)\": 0.4,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (30, 20)\n",
    "plt.rcParams.update({\"font.size\": 22})\n",
    "\n",
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict[code]\n",
    "    for column in sensor_df.columns:\n",
    "        df = sensor_df[column].copy()\n",
    "\n",
    "        # drop rows with duplicated index\n",
    "        df = df[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "        # plot the data with the thresholds for the variables that have them\n",
    "        # and compare the distribution of the values with the thresholds\n",
    "\n",
    "        if column in thresholds[code]:\n",
    "            threshold = thresholds[code][column]\n",
    "            fig, ax = plt.subplots(2, 2, figsize=figsize)\n",
    "            sns.lineplot(x=df.index, y=df, ax=ax[0, 0])\n",
    "            ax[0, 0].set_title(f\"Raw Data\")\n",
    "            ax[0, 0].set_ylabel(column)\n",
    "            ax[0, 0].set_xlabel(\"DateTime\")\n",
    "            ax[0, 0].grid()\n",
    "\n",
    "            fig_hist = sns.histplot(\n",
    "                df, bins=50, kde=True, stat=\"probability\", ax=ax[1, 0]\n",
    "            )\n",
    "            ax[1, 0].set_title(f\"Raw Data\")\n",
    "            ax[1, 0].set_ylabel(\"Probability\")\n",
    "            ax[1, 0].set_xlabel(column)\n",
    "            ax[1, 0].grid()\n",
    "\n",
    "            if column not in [\"Conductivity (uS/cm)\", \"Nitrate (mg/L)\"]:\n",
    "                ax[0, 0].axhline(\n",
    "                    y=threshold,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"dashed\",\n",
    "                    label=\"Upper Threshold\",\n",
    "                )\n",
    "                ax[0, 0].text(\n",
    "                    df.index[0],\n",
    "                    threshold,\n",
    "                    f\"Upper Threshold: {threshold}\",\n",
    "                    color=\"r\",\n",
    "                    va=\"bottom\",\n",
    "                )\n",
    "                ax[1, 0].axvline(\n",
    "                    x=threshold,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"dashed\",\n",
    "                    label=\"Upper Threshold\",\n",
    "                )\n",
    "                ax[1, 0].text(\n",
    "                    threshold,\n",
    "                    fig_hist.get_ylim()[1],\n",
    "                    f\"Upper Threshold: {threshold}\",\n",
    "                    color=\"r\",\n",
    "                    rotation=90,\n",
    "                    ha=\"right\",\n",
    "                    va=\"top\",\n",
    "                )\n",
    "                df = df[df <= threshold]\n",
    "            else:\n",
    "                ax[0, 0].axhline(\n",
    "                    y=threshold,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"dashed\",\n",
    "                    label=\"Lower Threshold\",\n",
    "                )\n",
    "                ax[0, 0].text(\n",
    "                    df.index[0],\n",
    "                    threshold,\n",
    "                    f\"Lower Threshold: {threshold}\",\n",
    "                    color=\"r\",\n",
    "                    va=\"bottom\",\n",
    "                )\n",
    "                ax[1, 0].axvline(\n",
    "                    x=threshold,\n",
    "                    color=\"r\",\n",
    "                    linestyle=\"dashed\",\n",
    "                    label=\"Lower Threshold\",\n",
    "                )\n",
    "                ax[1, 0].text(\n",
    "                    threshold,\n",
    "                    fig_hist.get_ylim()[1],\n",
    "                    f\"Lower Threshold: {threshold}\",\n",
    "                    color=\"r\",\n",
    "                    rotation=90,\n",
    "                    ha=\"right\",\n",
    "                    va=\"top\",\n",
    "                )\n",
    "                df = df[df >= threshold]\n",
    "\n",
    "            sns.lineplot(x=df.index, y=df, ax=ax[0, 1], color=\"g\")\n",
    "            ax[0, 1].set_title(f\"Filtered Data\")\n",
    "            ax[0, 1].set_ylabel(column)\n",
    "            ax[0, 1].set_xlabel(\"DateTime\")\n",
    "            ax[0, 1].grid()\n",
    "\n",
    "            sns.histplot(\n",
    "                df,\n",
    "                bins=50,\n",
    "                kde=True,\n",
    "                stat=\"probability\",\n",
    "                ax=ax[1, 1],\n",
    "                color=\"g\",\n",
    "            )\n",
    "            ax[1, 1].set_title(f\"Filtered Data\")\n",
    "            ax[1, 1].set_ylabel(\"Probability\")\n",
    "            ax[1, 1].set_xlabel(column)\n",
    "            ax[1, 1].grid()\n",
    "        else:\n",
    "            plt.figure(figsize=figsize)\n",
    "            sns.lineplot(x=df.index, y=df)\n",
    "            plt.ylabel(column)\n",
    "            plt.xlabel(\"DateTime\")\n",
    "            plt.grid()\n",
    "\n",
    "        plt.suptitle(f\"{code} - {column}\", fontsize=30)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        column_ = column.replace(\"/\", \"_\")\n",
    "\n",
    "        path = os.path.join(plot_folder, \"Clean Data\", \"Removed Outliers\", code)\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        plt.savefig(\n",
    "            os.path.join(\n",
    "                plot_folder,\n",
    "                \"Clean Data\",\n",
    "                \"Removed Outliers\",\n",
    "                code,\n",
    "                f\"{column_}.png\",\n",
    "            ),\n",
    "            dpi=300,\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        # plt.show()\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows that have values outside the thresholds\n",
    "for code in thresholds.keys():\n",
    "    sensor_df = sensor_dict[code].copy()\n",
    "\n",
    "    for column in thresholds[code].keys():\n",
    "        threshold = thresholds[code][column]\n",
    "        df = sensor_df[column].copy()\n",
    "\n",
    "        df = (\n",
    "            df[df > threshold]\n",
    "            if column not in [\"Conductivity (uS/cm)\", \"Nitrate (mg/L)\"]\n",
    "            else df[df < threshold]\n",
    "        )\n",
    "\n",
    "        sensor_df.loc[df.index, column] = np.nan\n",
    "\n",
    "    sensor_df.interpolate(method=\"time\", inplace=True)\n",
    "\n",
    "    sensor_dict.update({code: sensor_df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.to_excel(os.path.join(clean_data_folder, \"grab.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(clean_data_folder, \"sensors\")):\n",
    "    os.mkdir(os.path.join(clean_data_folder, \"sensors\"))\n",
    "\n",
    "for code in sensor_dict.keys():\n",
    "    sensor_dict[code].to_excel(\n",
    "        os.path.join(clean_data_folder, \"sensors\", f\"{code}.xlsx\"), index=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
