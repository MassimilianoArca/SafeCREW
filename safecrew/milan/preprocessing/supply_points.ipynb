{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Points (Case dell'Acqua) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join('..', '..', 'utils')\n",
    "\n",
    "with open(os.path.join(utils_folder, 'onedrive.txt'), 'r') as f:\n",
    "    cloud_data_folder = os.path.join(f.readline().strip(), 'Case dell\\'acqua')\n",
    "\n",
    "grab_samples_folder = os.path.join(cloud_data_folder, \"Grab Samples\")\n",
    "sensors_folder = os.path.join(cloud_data_folder, \"Sensori\") \n",
    "\n",
    "local_data_folder = os.path.join('..', '..', 'data')\n",
    "intermediate_data_folder = os.path.join(local_data_folder, \"Intermediate Data\")\n",
    "clean_data_folder = os.path.join(local_data_folder, \"Clean Data\")\n",
    "raw_data_folder = os.path.join(local_data_folder, \"Raw Data\")\n",
    "\n",
    "all_grab_samples_path = os.path.join(\n",
    "    raw_data_folder, \"Tutti punti - Grab Samples\"\n",
    ")\n",
    "\n",
    "grab_samples_supply_points_path = os.path.join(\n",
    "    raw_data_folder,\n",
    "    \"Case dell'acqua - Grab Samples (main)/0. Case acqua - 2010-2023.xlsx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tra i grab non c'è l'ORP, mentre\n",
    "# tra i sensori non c'è DOC (c'è il TOC) e L'UVA254\n",
    "\n",
    "# Quindi in comune abbiamo:\n",
    "# Color, TOC, Nitrati, Turbidity, pH, Temperature, Conductivity, Free Chlorine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.DataFrame()\n",
    "for filename in os.listdir(grab_samples_folder):\n",
    "    if grab_df.empty:\n",
    "        grab_df = pd.read_excel(os.path.join(grab_samples_folder, filename))\n",
    "    else:\n",
    "        df = pd.read_excel(os.path.join(grab_samples_folder, filename))\n",
    "        grab_df = pd.concat([grab_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(utils_folder, \"columns_types.json\")) as f:\n",
    "    column_types = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns = column_types[\"metadata_columns\"]\n",
    "features_columns = column_types[\"features_columns\"]\n",
    "targets_columns = column_types[\"targets_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_metadata_columns = list(set(metadata_columns).intersection(grab_df.columns))\n",
    "common_features_columns = list(set(features_columns).intersection(grab_df.columns))\n",
    "common_targets_columns = list(set(targets_columns).intersection(grab_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that are not in the column_types.json file\n",
    "grab_df = grab_df[common_metadata_columns + common_features_columns + common_targets_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix LOD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def convert_string_values(s):\n",
    "    if isinstance(s, (int, float)):\n",
    "        return s\n",
    "    elif pd.isna(s):\n",
    "        return None\n",
    "    else:\n",
    "        if \",\" in s:\n",
    "            s = s.replace(\",\", \".\")\n",
    "        if \"<\" in s:\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) / 2 if number else None\n",
    "        elif \">\" in s:\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) if number else None\n",
    "        elif \"*\" in s or re.search(\"[a-zA-Z]\", s):\n",
    "            number = re.findall(r\"\\d+\\.?\\d*\", s)\n",
    "            return float(number[0]) if number else None\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_label(value):\n",
    "    if pd.isna(value):\n",
    "        return \"NaN\"\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return \"Normal\"\n",
    "    elif \"<\" in value:\n",
    "        return \"Less than\"\n",
    "    elif \">\" in value:\n",
    "        return \"Greater than\"\n",
    "    else:\n",
    "        return \"NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_features_columns + common_targets_columns:\n",
    "    label_column = column + \"_label\"\n",
    "    grab_df.loc[:, label_column] = grab_df[column].apply(set_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[common_features_columns] = grab_df[common_features_columns].map(\n",
    "    convert_string_values\n",
    ")\n",
    "\n",
    "grab_df[common_targets_columns] = grab_df[common_targets_columns].map(\n",
    "    convert_string_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict = {}\n",
    "\n",
    "for sensor_file in os.listdir(sensors_folder):\n",
    "    if sensor_file == \".DS_Store\":\n",
    "        continue\n",
    "    \n",
    "    sensor_folder = os.path.join(sensors_folder, sensor_file)\n",
    "    for filename in os.listdir(sensor_folder): \n",
    "        \n",
    "        if not filename.endswith(\".xlsx\"):\n",
    "            continue\n",
    "        \n",
    "        house_code = filename.split(\"_\")[0]\n",
    "        if house_code not in sensor_dict:\n",
    "            sensor_dict[house_code] = pd.read_excel(os.path.join(sensor_folder, filename), header=1)\n",
    "        else:\n",
    "            df = pd.read_excel(os.path.join(sensor_folder, filename), header=1)\n",
    "            sensor_dict[house_code] = pd.concat([sensor_dict[house_code], df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict['via TABACCHI'].columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapping = {\n",
    "    \"Measurement interval=900[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "    \"Measurement interval=999[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "    \"Measurement interval=0[sec] (Export-Aggregation disabled)\": \"DateTime\",\n",
    "    \"COLORtrue - Measured value [Hazen-eq.] (Limit:0.00-300.00)\": \"Color (CU)\",\n",
    "    \"TOCeq - Measured value [mg/l] (Limit:0.00-22.00)\": \"TOC (mg/l)\",\n",
    "    \"NO3eq - Measured value [mg/l] (Limit:0.00-88.00)\": \"Nitrate (mg/l)\",\n",
    "    \"UV254t - Measured value [Abs/m] (Limit:0.00-71.00)\": \"UVA254 (1/m)\",\n",
    "    \"Turbidity - Measured value [FTUeq] (Limit:0.00-170.00)\": \"Turbidity (FTU)\",\n",
    "    \"pH - Measured value (Limit:0.00-14.00)\": \"pH\",\n",
    "    \"Temperature - Measured value [C] (Limit:-5.00-100.00)\": \"Temperature (°C)\",\n",
    "    \"Conductivity - Measured value [uS/cm] (Limit:0.10-600000.00)\": \"Conductivity (μS/cm)\",\n",
    "    \"Free Chlorine - Measured value [mg/l] (Limit:0.00-2.00)\": \"Free Chlorine (mg/l)\",\n",
    "}\n",
    "\n",
    "\n",
    "for house_code, df in sensor_dict.items():\n",
    "    sensor_dict[house_code] = df.rename(columns=columns_mapping)\n",
    "    \n",
    "    # set to get unique values\n",
    "    columns = set(columns_mapping.values())\n",
    "    \n",
    "    \n",
    "    sensor_dict[house_code] = sensor_dict[house_code][list(columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.drop(\n",
    "    columns=[\n",
    "        'Codice punto di prelievo',\n",
    "        'Rapporto di prova',\n",
    "    ],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['Punto di prelievo'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name of Punta di prelievo values to match codes\n",
    "def change_name(name):\n",
    "    if \"Tognazzi\" in name:\n",
    "        return \"Tognazzi\"\n",
    "    elif \"Tabacchi\" in name:\n",
    "        return \"Tabacchi\"\n",
    "    elif \"Gramsci\" in name:\n",
    "        return \"Gramsci\"\n",
    "    elif \"Berna\" in name:\n",
    "        return \"Berna\"\n",
    "    elif \"Bande Nere\" in name or \"Piazzale Giovanni\" in name:\n",
    "        return \"Bande Nere\"\n",
    "    elif \"Prealpi\" in name:\n",
    "        return \"Prealpi\"\n",
    "    elif \"Chiostergi\" in name:\n",
    "        return \"Chiostergi\"\n",
    "    elif \"Montevideo\" in name or \"Montevid\" in name:\n",
    "        return \"Montevideo\"\n",
    "    elif \"Fortunato\" in name:\n",
    "        return \"Fortunato\"\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['Punto di prelievo'] = grab_df['Punto di prelievo'].map(change_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['Data di prelievo'] = pd.to_datetime(grab_df['Data di prelievo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name of the keys to match the names in the grab_df\n",
    "sensor_dict['Tabacchi'] = sensor_dict.pop('via TABACCHI')\n",
    "sensor_dict['Tognazzi'] = sensor_dict.pop('via Tognazzi')\n",
    "sensor_dict['Prealpi'] = sensor_dict.pop('Piazza Prealpi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df['Punto di prelievo'].unique():\n",
    "    sensor_df = sensor_dict.pop(code)\n",
    "    sensor_df['DateTime'] = pd.to_datetime(sensor_df['DateTime'])\n",
    "    sensor_df.set_index('DateTime', inplace=True)\n",
    "    sensor_dict[code] = sensor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of missing values for each column\n",
    "for code in grab_df['Punto di prelievo'].unique():\n",
    "    code_df = grab_df[grab_df['Punto di prelievo'] == code]\n",
    "    for column in common_features_columns + common_targets_columns:\n",
    "        # count the number of missing values\n",
    "        missing_values = code_df[column].isna().sum()\n",
    "        if missing_values > 0:\n",
    "            print(f\"{code} has {missing_values} missing values in column {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute number of rows that have at least one missing value\n",
    "for code in grab_df['Punto di prelievo'].unique():\n",
    "    code_df = grab_df[grab_df['Punto di prelievo'] == code]\n",
    "    \n",
    "    missing_values = code_df[common_features_columns + common_targets_columns].isna().any(axis=1).sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"{code} has {missing_values} rows with missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the Berna rows with missing values\n",
    "row_index = grab_df[\n",
    "    (grab_df['Punto di prelievo'] == \"Berna\") & (grab_df[common_features_columns + common_targets_columns].isna().any(axis=1))\n",
    "].index\n",
    "\n",
    "grab_df.drop(row_index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the moment no imputation is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict[code]\n",
    "    for column in sensor_df.columns:\n",
    "        missing_values = sensor_df[column].isna().sum()\n",
    "        if missing_values > 0:\n",
    "            print(f\"{code} has {missing_values} missing values in column {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of missing values is very low, so we can do implicit imputation with time interpolation\n",
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict.pop(code)\n",
    "    sensor_df.interpolate(method='time', inplace=True)\n",
    "    sensor_dict[code] = sensor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "\n",
    "# - GRAMSCI\n",
    "# Turbidity selected upper threshold is 1.5\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 0.8\n",
    "# UVA254 selected upper threshold is 1.5\n",
    "\n",
    "\n",
    "# - BERNA\n",
    "# Turbidity selected upper threshold is 1.5\n",
    "# Temperature selected upper threshold is 19.5\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 0.2\n",
    "\n",
    "# - BANDE NERE\n",
    "# Turbidity selected upper threshold is 1\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Nitrate selected lower threshold is 20\n",
    "# UVA254 selected upper threshold is 0.4\n",
    "\n",
    "# - CHIOSTREGI\n",
    "# free chlorine selected upper threshold is 0.06\n",
    "\n",
    "# - FORTUNATO\n",
    "# Turbidity selected upper threshold is 1\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Nitrate selected lower threshold is 25\n",
    "# UVA254 selected upper threshold is 0.4\n",
    "\n",
    "# - MONTEVIDEO\n",
    "# Color selected upper threshold is 4\n",
    "# Turbidity selected upper threshold is 1\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 1\n",
    "# Nitrate selected lower threshold is 20\n",
    "# TOC selected upper threshold is 1\n",
    "# UVA254 selected upper threshold is 4\n",
    "\n",
    "# - PREALPI\n",
    "# Turbidity selected upper threshold is 0.7\n",
    "# UVA254 selected upper threshold is 1.5\n",
    "\n",
    "# - TABACCHI\n",
    "\n",
    "# - TOGNAZZI\n",
    "# Conductivity selected lower threshold is 400\n",
    "# Free Chlorine selected upper threshold is 0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict[code]\n",
    "    for column in sensor_df.columns:\n",
    "        \n",
    "        # plot the data with the thresholds\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=sensor_df.index, y=sensor_df[column], mode='lines', name=column))\n",
    "        \n",
    "        if code == \"Gramsci\":\n",
    "            if column == \"Turbidity (FTU)\":\n",
    "                fig.add_hline(y=1.5, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"Conductivity (μS/cm)\":\n",
    "                fig.add_hline(y=400, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Lower Threshold\")\n",
    "            elif column == \"Free Chlorine (mg/l)\":\n",
    "                fig.add_hline(y=0.8, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"UVA254 (1/m)\":\n",
    "                fig.add_hline(y=1.5, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "        elif code == \"Berna\":\n",
    "            if column == \"Turbidity (FTU)\":\n",
    "                fig.add_hline(y=1.5, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"Temperature (°C)\":\n",
    "                fig.add_hline(y=19.5, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"Conductivity (μS/cm)\":\n",
    "                fig.add_hline(y=400, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Lower Threshold\")\n",
    "            elif column == \"Free Chlorine (mg/l)\":\n",
    "                fig.add_hline(y=0.2, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "        elif code == \"Bande Nere\":\n",
    "            if column == \"Turbidity (FTU)\":\n",
    "                fig.add_hline(y=1, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"Conductivity (μS/cm)\":\n",
    "                fig.add_hline(y=400, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Lower Threshold\")\n",
    "            elif column == \"Nitrate (mg/l)\":\n",
    "                fig.add_hline(y=20, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Lower Threshold\")\n",
    "            elif column == \"UVA254 (1/m)\":\n",
    "                fig.add_hline(y=0.4, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "        elif code == \"Chiostergi\":\n",
    "            if column == \"Free Chlorine (mg/l)\":\n",
    "                fig.add_hline(y=0.06, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "        elif code == \"Fortunato\":\n",
    "            if column == \"Turbidity (FTU)\":\n",
    "                fig.add_hline(y=1, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"Conductivity (μS/cm)\":\n",
    "                fig.add_hline(y=400, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Lower Threshold\")\n",
    "            elif column == \"Nitrate (mg/l)\":\n",
    "                fig.add_hline(y=25, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Lower Threshold\")\n",
    "            elif column == \"UVA254 (1/m)\":\n",
    "                fig.add_hline(y=0.4, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "        elif code == \"Montevideo\":\n",
    "            if column == \"Color (CU)\":\n",
    "                fig.add_hline(y=4, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"Turbidity (FTU)\":\n",
    "                fig.add_hline(y=1, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"Conductivity (μS/cm)\":\n",
    "                fig.add_hline(y=400, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Lower Threshold\")\n",
    "            elif column == \"Free Chlorine (mg/l)\":\n",
    "                fig.add_hline(y=1, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"Nitrate (mg/l)\":\n",
    "                fig.add_hline(y=20, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Lower Threshold\")\n",
    "            elif column == \"TOC (mg/l)\":\n",
    "                fig.add_hline(y=1, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"UVA254 (1/m)\":\n",
    "                fig.add_hline(y=4, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "        elif code == \"Prealpi\":\n",
    "            if column == \"Turbidity (FTU)\":\n",
    "                fig.add_hline(y=0.7, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "            elif column == \"UVA254 (1/m)\":\n",
    "                fig.add_hline(y=1.5, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "        elif code == \"Tabacchi\":\n",
    "            pass\n",
    "        elif code == \"Tognazzi\":\n",
    "            if column == \"Conductivity (μS/cm)\":\n",
    "                fig.add_hline(y=400, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Lower Threshold\")\n",
    "            elif column == \"Free Chlorine (mg/l)\":\n",
    "                fig.add_hline(y=0.4, line_dash=\"dot\", line_color=\"red\", annotation_text=\"Upper Threshold\")\n",
    "                \n",
    "        fig.update_layout(title=f\"{code} - {column}\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows that have values outside the thresholds\n",
    "for code in sensor_dict.keys():\n",
    "    sensor_df = sensor_dict[code].copy()\n",
    "    \n",
    "    if code == \"Gramsci\":\n",
    "        row_index = sensor_df[\n",
    "            (sensor_df[\"Turbidity (FTU)\"] > 1.5) |\n",
    "            (sensor_df[\"Conductivity (μS/cm)\"] < 400) |\n",
    "            (sensor_df[\"Free Chlorine (mg/l)\"] > 0.8) |\n",
    "            (sensor_df[\"UVA254 (1/m)\"] > 1.5)\n",
    "        ].index\n",
    "    elif code == \"Berna\":\n",
    "        row_index = sensor_df[\n",
    "            (sensor_df[\"Turbidity (FTU)\"] > 1.5) |\n",
    "            (sensor_df[\"Temperature (°C)\"] > 19.5) |\n",
    "            (sensor_df[\"Conductivity (μS/cm)\"] < 400) |\n",
    "            (sensor_df[\"Free Chlorine (mg/l)\"] > 0.2)\n",
    "        ].index\n",
    "    elif code == \"Bande Nere\":\n",
    "        row_index = sensor_df[\n",
    "            (sensor_df[\"Turbidity (FTU)\"] > 1) |\n",
    "            (sensor_df[\"Conductivity (μS/cm)\"] < 400) |\n",
    "            (sensor_df[\"Nitrate (mg/l)\"] < 20) |\n",
    "            (sensor_df[\"UVA254 (1/m)\"] > 0.4)\n",
    "        ].index\n",
    "    elif code == \"Chiostergi\":\n",
    "        row_index = sensor_df[\n",
    "            (sensor_df[\"Free Chlorine (mg/l)\"] > 0.06)\n",
    "        ].index\n",
    "    elif code == \"Fortunato\":\n",
    "        row_index = sensor_df[\n",
    "            (sensor_df[\"Turbidity (FTU)\"] > 1) |\n",
    "            (sensor_df[\"Conductivity (μS/cm)\"] < 400) |\n",
    "            (sensor_df[\"Nitrate (mg/l)\"] < 25) |\n",
    "            (sensor_df[\"UVA254 (1/m)\"] > 0.4)\n",
    "        ].index\n",
    "    elif code == \"Montevideo\":\n",
    "        row_index = sensor_df[\n",
    "            (sensor_df[\"Color (CU)\"] > 4) |\n",
    "            (sensor_df[\"Turbidity (FTU)\"] > 1) |\n",
    "            (sensor_df[\"Conductivity (μS/cm)\"] < 400) |\n",
    "            (sensor_df[\"Free Chlorine (mg/l)\"] > 1) |\n",
    "            (sensor_df[\"Nitrate (mg/l)\"] < 20) |\n",
    "            (sensor_df[\"TOC (mg/l)\"] > 1) |\n",
    "            (sensor_df[\"UVA254 (1/m)\"] > 4)\n",
    "        ].index\n",
    "    elif code == \"Prealpi\":\n",
    "        row_index = sensor_df[\n",
    "            (sensor_df[\"Turbidity (FTU)\"] > 0.7) |\n",
    "            (sensor_df[\"UVA254 (1/m)\"] > 1.5)\n",
    "        ].index\n",
    "    elif code == \"Tabacchi\":\n",
    "        pass\n",
    "    elif code == \"Tognazzi\":\n",
    "        row_index = sensor_df[\n",
    "            (sensor_df[\"Conductivity (μS/cm)\"] < 400) |\n",
    "            (sensor_df[\"Free Chlorine (mg/l)\"] > 0.4)\n",
    "        ].index\n",
    "        \n",
    "    sensor_df.drop(row_index, inplace=True)\n",
    "    \n",
    "    sensor_df.rename(\n",
    "        columns={\n",
    "        'Conductivity (μS/cm)': 'Conductivity (uS/cm)',\n",
    "        'TOC (mg/l)': 'TOC (mg/L)',\n",
    "        'Nitrate (mg/l)': 'Nitrate (mg/L)',\n",
    "        'Free Chlorine (mg/l)': 'Free Chlorine (mg/L)',\n",
    "        'Turbidity (FTU)': 'Turbidity (NTU)',\n",
    "        },\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    sensor_dict.update({code: sensor_df})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Historical Grab Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples = []\n",
    "\n",
    "for file in os.listdir(all_grab_samples_path):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(os.path.join(all_grab_samples_path, file), header=11)\n",
    "    else:\n",
    "        df = pd.read_excel(os.path.join(all_grab_samples_path, file), header=15)\n",
    "    common_cols = list(set(df.columns.to_list()) & set(metadata_columns + features_columns + targets_columns))\n",
    "    df = df[common_cols]\n",
    "    grab_samples.append(df)\n",
    "\n",
    "grab_samples_df = pd.concat(grab_samples, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = \"CS, CT\"\n",
    "\n",
    "meta_supply_points_df = pd.read_excel(\n",
    "    grab_samples_supply_points_path, usecols=column_list, header=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_supply_points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df = grab_samples_df.merge(\n",
    "    meta_supply_points_df,\n",
    "    left_on=[\"Punto di prelievo\", \"Codice punto di prelievo\"],\n",
    "    right_on=[\"filtro 1\", \"filtro 2\"],\n",
    "    how=\"inner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supply_points_df.drop(columns=[\"filtro 1\", \"filtro 2\"], inplace=True)\n",
    "hist_grab_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all value columns in the mapping to the corresponding key column\n",
    "column_mapping = {\n",
    "    \"Temperatura (°C)\": [\n",
    "        \"Temperatura - °C\",\n",
    "        \"Temperatura (al prelievo) (°C)\",\n",
    "    ],\n",
    "    \"Cloro residuo libero (mg/L di Cl2)\": [\n",
    "        \"Cloro residuo libero (al prelievo) (mg/L di Cl2)\",\n",
    "    ],\n",
    "    \"Torbidità (NTU)\": [\n",
    "        \"Torbidità (NTu)\",\n",
    "    ],\n",
    "    \"Batteri coliformi a 37°C (MPN/100 mL)\": [\n",
    "        \"Batteri coliformi a 37°C (MPN / 100 mL)\",\n",
    "    ],\n",
    "    \"Colore (CU)\": [\n",
    "        \"Colore (Cu)\",\n",
    "    ],\n",
    "    \"Escherichia coli (MPN/100 mL)\": [\n",
    "        \"Escherichia Coli (MPN / 100mL)\",\n",
    "    ],\n",
    "    \"Enterococchi (MPN/100 mL)\": [\n",
    "        \"Enterococchi (MPN / 100mL)\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for final_column, original_columns in column_mapping.items():\n",
    "    for original_column in original_columns:\n",
    "        hist_grab_df[final_column] = hist_grab_df[\n",
    "            final_column\n",
    "        ].combine_first(hist_grab_df[original_column])\n",
    "    hist_grab_df.drop(columns=original_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns = column_types[\"metadata_columns\"]\n",
    "features_columns = column_types[\"features_columns\"]\n",
    "targets_columns = column_types[\"targets_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_common_metadata_columns = list(set(metadata_columns).intersection(hist_grab_df.columns))\n",
    "hist_common_features_columns = list(set(features_columns).intersection(hist_grab_df.columns))\n",
    "hist_common_targets_columns = list(set(targets_columns).intersection(hist_grab_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_columns(title, columns):\n",
    "    print(f\"{title}:\")\n",
    "    for col in columns:\n",
    "        print(f\"  - {col}\")\n",
    "    print()\n",
    "\n",
    "print_columns(\"Historical Common Metadata Columns\", hist_common_metadata_columns)\n",
    "print_columns(\"Common Metadata Columns\", common_metadata_columns)\n",
    "print_columns(\"Historical Common Features Columns\", hist_common_features_columns)\n",
    "print_columns(\"Common Features Columns\", common_features_columns)\n",
    "print_columns(\"Historical Common Targets Columns\", hist_common_targets_columns)\n",
    "print_columns(\"Common Targets Columns\", common_targets_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a mapping of the hist_common_features_columns to the common_features_columns (sorted)\n",
    "# and the hist_common_targets_columns to the common_targets_columns (sorted)\n",
    "\n",
    "# the mapping is done by sorting the columns and then zipping them together\n",
    "mapping_features = dict(zip(sorted(hist_common_features_columns), sorted(common_features_columns)))\n",
    "mapping_targets = dict(zip(sorted(hist_common_targets_columns), sorted(common_targets_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.rename(columns=mapping_features, inplace=True)\n",
    "hist_grab_df.rename(columns=mapping_targets, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.drop(\n",
    "    columns=[\n",
    "        'filtro 1',\n",
    "        'filtro 2',\n",
    "    ],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the hist_grab_df rows that have the Punto di prelievo containing the grab_df Punto di prelievo\n",
    "hist_grab_df = hist_grab_df[(hist_grab_df['Punto di prelievo'].str.contains(\"|\".join(grab_df['Punto di prelievo'].unique()), case=False, na=False)) | (hist_grab_df['Codice punto di prelievo'].str.contains(\"|\".join(grab_df['Punto di prelievo'].unique()), case=False, na=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['Punto di prelievo'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import contains\n",
    "\n",
    "\n",
    "# do a function that for a value, if an item of grab_df['Punto di prelievo'].unique() is contained in the value, then change the value to the item\n",
    "def change_name(value):\n",
    "    for name in grab_df['Punto di prelievo'].unique():\n",
    "        if contains(value, name):\n",
    "            return name\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df['Punto di prelievo'] = hist_grab_df['Punto di prelievo'].map(change_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_features_columns + common_targets_columns:\n",
    "    if column not in hist_grab_df.columns:\n",
    "        continue\n",
    "    label_column = column + \"_label\"\n",
    "    hist_grab_df.loc[:, label_column] = hist_grab_df[column].apply(set_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_features_columns + common_targets_columns:\n",
    "    if column not in hist_grab_df.columns:\n",
    "        continue\n",
    "    hist_grab_df[column] = hist_grab_df[column].map(convert_string_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.shape, hist_grab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me the columns that are in grab_df but not in hist_grab_df\n",
    "for column in grab_df.columns:\n",
    "    if column not in hist_grab_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in hist_grab_df.columns:\n",
    "    if column not in grab_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grab_df.drop(\n",
    "    columns=[\n",
    "        'Rapporto di prova',\n",
    "        'Codice punto di prelievo',\n",
    "    ],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.concat([grab_df, hist_grab_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "# FIXME this piece of code needs to be rearranged\n",
    "columns_mapping = {\n",
    "    \"Data di prelievo\": \"DateTime\",\n",
    "    \"Punto di prelievo\": \"Code\",\n",
    "    \"Colore (Cu)\": \"Color (CU)\",\n",
    "    \"Cloro residuo libero (al prelievo) (mg/L di Cl2)\": \"Free Chlorine (mg/L)\",\n",
    "    \"Concentrazione ioni idrogeno (unità pH)\": \"pH\",\n",
    "    \"Conduttività a 20°C (µS/cm)\": \"Conductivity (uS/cm)\",\n",
    "    \"TOC - carbonio organico totale (mg/L di C)\": \"TOC (mg/L)\",\n",
    "    \"Temperatura (al prelievo) (°C)\": \"Temperature (°C)\",\n",
    "    \"Torbidità (NTu)\": \"Turbidity (NTU)\",\n",
    "    \"Nitrati (mg/L)\": \"Nitrate (mg/L)\",\n",
    "}\n",
    "\n",
    "grab_df.rename(columns=columns_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with First Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df = pd.read_excel(\n",
    "    os.path.join(clean_data_folder, \"Riunione 24-04-2024\", \"Grab Samples.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df = first_batch_grab_df[first_batch_grab_df['Type'] == 'Ingresso']\n",
    "first_batch_grab_df.drop(columns=['Type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df['Code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['Code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping = {\n",
    "    'HOUSE_BANDENERE': 'Bande Nere',\n",
    "    'HOUSE_BERNA': 'Berna',\n",
    "    'HOUSE_CHIOSTERGI': 'Chiostergi',\n",
    "    'HOUSE_FORTUNATO': 'Fortunato',\n",
    "    'HOUSE_GRAMSCI': 'Gramsci',\n",
    "    'HOUSE_MONTEVIDEO': 'Montevideo',\n",
    "    'HOUSE_PREALPI': 'Prealpi',\n",
    "    'HOUSE_TABACCHI': 'Tabacchi',\n",
    "    'HOUSE_TOGNAZZI': 'Tognazzi',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_grab_df['Code'] = first_batch_grab_df['Code'].map(code_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.shape, first_batch_grab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in grab_df.columns:\n",
    "    if column not in first_batch_grab_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in first_batch_grab_df.columns:\n",
    "    if column not in grab_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two dataframes\n",
    "grab_df = pd.concat([grab_df, first_batch_grab_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.sort_values(by='DateTime', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df = pd.read_excel(\n",
    "    os.path.join(clean_data_folder, \"Riunione 24-04-2024\", \"Sensor Data.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df['Code'] = first_batch_sensor_df['Code'].map(code_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_sensor_df.drop(\n",
    "    columns=[\n",
    "        'Flow Rate (m³/s)'\n",
    "    ],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict['Bande Nere']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in first_batch_sensor_df['Code'].unique():\n",
    "    df = first_batch_sensor_df[first_batch_sensor_df['Code'] == code].copy()\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "    df.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    df.drop(columns=['Code'], inplace=True)\n",
    "    \n",
    "    sensor_df = sensor_dict.pop(code)\n",
    "    \n",
    "    sensor_df = pd.concat([sensor_df, df])\n",
    "    sensor_df.sort_index(inplace=True)\n",
    "    sensor_dict[code] = sensor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict['Berna']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.to_excel(os.path.join(clean_data_folder, \"grab.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(clean_data_folder, \"sensors\")):\n",
    "    os.mkdir(os.path.join(clean_data_folder, \"sensors\"))\n",
    "    \n",
    "for code in sensor_dict.keys():\n",
    "    sensor_dict[code].to_excel(os.path.join(clean_data_folder, \"sensors\", f\"{code}.xlsx\"), index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
