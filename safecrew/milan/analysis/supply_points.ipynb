{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supply Points Analysis between Grab and Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"..\", \"utils\")\n",
    "\n",
    "data_folder = os.path.join(\"..\", \"..\", \"data\")\n",
    "clean_data_folder = os.path.join(data_folder, \"Clean Data\")\n",
    "metadata_folder = os.path.join(data_folder, \"Metadata\")\n",
    "plot_folder = os.path.join(data_folder, \"Plots\")\n",
    "\n",
    "sensor_folder = os.path.join(clean_data_folder, \"sensors\")\n",
    "\n",
    "nuwee_folder = os.path.join(plot_folder, \"nuwee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.read_excel(os.path.join(clean_data_folder, \"grab.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict = {}\n",
    "\n",
    "for file in os.listdir(sensor_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        sensor_dict[file.split(\".\")[0]] = pd.read_excel(\n",
    "            os.path.join(sensor_folder, file)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(utils_folder, \"columns_types.json\")) as f:\n",
    "    column_types = json.load(f)\n",
    "\n",
    "metadata_columns = column_types[\"metadata_columns\"]\n",
    "features_columns = column_types[\"features_columns\"]\n",
    "targets_columns = column_types[\"targets_columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import contains\n",
    "\n",
    "\n",
    "label_columns = [col for col in grab_df.columns if contains(col, \"label\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename grab columns\n",
    "feature_mapping = {\n",
    "    \"Cloro residuo libero (al prelievo) (mg/L di Cl2)\": \"Free Chlorine (mg/L)\",\n",
    "    \"Colore (Cu)\": \"Color (CU)\",\n",
    "    \"Concentrazione ioni idrogeno (unità pH)\": \"pH\",\n",
    "    \"Conduttività a 20°C (µS/cm)\": \"Conductivity (uS/cm)\",\n",
    "    \"TOC - carbonio organico totale (mg/L di C)\": \"TOC (mg/L)\",\n",
    "    \"Temperatura (al prelievo) (°C)\": \"Temperature (°C)\",\n",
    "    # \"Torbidità (NTu)\": \"Turbidity (NTU)\",\n",
    "    \"Nitrati (mg/L)\": \"Nitrate (mg/L)\",\n",
    "}\n",
    "\n",
    "targets_mapping = {\n",
    "    \"Bromodiclorometano (µg/L)\": \"Dichlorobromomethane (ug/L)\",\n",
    "    \"Bromoformio (µg/L)\": \"Bromoform (ug/L)\",\n",
    "    \"Cloroformio (µg/L)\": \"Chloroform (ug/L)\",\n",
    "    \"Dibromoclorometano (ug/L)\": \"Dibromochloromethane (ug/L)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename grab_df columns that contain features\n",
    "for column in grab_df.columns:\n",
    "    if column in targets_mapping:\n",
    "        grab_df.rename(columns={column: targets_mapping[column]}, inplace=True)\n",
    "\n",
    "    if len(column.split(\"_\")) > 1:\n",
    "        if column.split(\"_\")[0] in feature_mapping:\n",
    "            new_name = feature_mapping[column.split(\"_\")[0]]\n",
    "            new_name = new_name + \"_\" + column.split(\"_\")[1]\n",
    "            grab_df.rename(columns={column: new_name}, inplace=True)\n",
    "\n",
    "        if column.split(\"_\")[0] in targets_mapping:\n",
    "            new_name = targets_mapping[column.split(\"_\")[0]]\n",
    "            new_name = new_name + \"_\" + column.split(\"_\")[1]\n",
    "            grab_df.rename(columns={column: new_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the label columns\n",
    "for column in grab_df.columns:\n",
    "    if column in label_columns:\n",
    "        variable_name = column.split(\"_\")[0]\n",
    "\n",
    "        if variable_name in feature_mapping:\n",
    "            new_name = feature_mapping[variable_name]\n",
    "            new_name = new_name + \"_\" + column.split(\"_\")[1]\n",
    "            grab_df.rename(columns={column: new_name}, inplace=True)\n",
    "\n",
    "        if variable_name in targets_mapping:\n",
    "            new_name = targets_mapping[variable_name]\n",
    "            new_name = new_name + \"_\" + column.split(\"_\")[1]\n",
    "            grab_df.rename(columns={column: new_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = grab_df[grab_df['DateTime'] > '2024-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [\n",
    "            feature_mapping.values(),\n",
    "            [\n",
    "                \"N° Entries\",\n",
    "                \"N° Valid Samples\",\n",
    "                \"% Missing\",\n",
    "                \"N° < LOQ\",\n",
    "            ],\n",
    "        ]\n",
    "    ),\n",
    "    index=grab_df[\"Code\"].unique(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        df = grab_df[grab_df[\"Code\"] == code][\n",
    "            [\"DateTime\", feature, feature + \"_label\"]\n",
    "        ].copy()\n",
    "\n",
    "        if df.dropna().shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "\n",
    "        start_date = df.dropna()[\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.dropna()[\"DateTime\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        df = df[(df[\"DateTime\"] >= start_date) & (df[\"DateTime\"] <= end_date)]\n",
    "\n",
    "        missing_values = (\n",
    "            df[df[feature + \"_label\"].isna()].shape[0] / df.shape[0] * 100\n",
    "        )\n",
    "\n",
    "        feature_df.loc[code, (feature, \"N° Entries\")] = df.shape[0]\n",
    "\n",
    "        feature_df.loc[code, (feature, \"% Missing\")] = round(missing_values, 2)\n",
    "\n",
    "        feature_df.loc[code, (feature, \"N° < LOQ\")] = df[\n",
    "            df[feature + \"_label\"] == \"Less than\"\n",
    "        ].shape[0]\n",
    "        \n",
    "        \n",
    "        valid_df = df[df[feature + \"_label\"] == \"Normal\"]\n",
    "        loq_df = df[df[feature + \"_label\"] == \"Less than\"]\n",
    "        \n",
    "        feature_df.loc[code, (feature, \"N° Valid Samples\")] = valid_df.shape[0]\n",
    "        feature_df.loc[code, (feature, \"N° < LOQ\")] = loq_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the indexes\n",
    "feature_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the first level of the columns and maintain the order of the second level\n",
    "feature_df = feature_df.sort_index(axis=1, level=0, sort_remaining=False, key=lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = pd.DataFrame(\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [\n",
    "            targets_mapping.values(),\n",
    "            [\n",
    "                \"N° Entries\",\n",
    "                \"N° Valid Samples\",\n",
    "                \"% Missing\",\n",
    "                \"N° < LOQ\",\n",
    "            ],\n",
    "        ]\n",
    "    ),\n",
    "    index=grab_df[\"Code\"].unique(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    for target in targets_mapping.values():\n",
    "        df = grab_df[grab_df[\"Code\"] == code][\n",
    "            [\"DateTime\", target, target + \"_label\"]\n",
    "        ].copy()\n",
    "\n",
    "        if df.dropna().shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "\n",
    "        start_date = df.dropna()[\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.dropna()[\"DateTime\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        df = df[(df[\"DateTime\"] >= start_date) & (df[\"DateTime\"] <= end_date)]\n",
    "\n",
    "        missing_values = (\n",
    "            df[df[target + \"_label\"].isna()].shape[0] / df.shape[0] * 100\n",
    "        )\n",
    "\n",
    "        targets_df.loc[code, (target, \"N° Entries\")] = df.shape[0]\n",
    "\n",
    "        valid_df = df[df[target + \"_label\"] == \"Normal\"]\n",
    "        loq_df = df[df[target + \"_label\"] == \"Less than\"]\n",
    "        \n",
    "        \n",
    "        targets_df.loc[code, (target, \"% Missing\")] = round(missing_values, 2)\n",
    "\n",
    "        targets_df.loc[code, (target, \"N° Valid Samples\")] = valid_df.shape[0]\n",
    "        targets_df.loc[code, (target, \"N° < LOQ\")] = loq_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = targets_df.sort_index(axis=1, level=0, sort_remaining=False, key=lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "feature_df.to_excel(os.path.join(metadata_folder, \"Grab\", \"features.xlsx\"))\n",
    "\n",
    "targets_df.to_excel(os.path.join(metadata_folder, \"Grab\", \"targets.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fix Conductivity name\n",
    "for sensor in sensor_dict:\n",
    "    sensor_dict[sensor].rename(\n",
    "        columns={\"Conductivity (μS/cm)\": \"Conductivity (uS/cm)\"}, inplace=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns = sensor_dict[\"Berna\"].columns.difference([\"DateTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_df = pd.DataFrame(\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [sensor_columns, [\"N° Data\", \"N° Missing\", \"Mean\", \"Std\", \"Start Date\", \"End Date\"]]\n",
    "    ),\n",
    "    index=list(sensor_dict.keys()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor in sensor_dict.keys():\n",
    "    for column in sensor_columns:\n",
    "        if sensor == \"Berna\" and column == \"Turbidity (FTU)\":\n",
    "            df = sensor_dict[sensor].copy()\n",
    "            # remove rows with Turbidity > 2\n",
    "            df = df[df[\"Turbidity (FTU)\"] <= 2]\n",
    "\n",
    "            sensors_df.loc[sensor, (column, \"N° Data\")] = df[column].count()\n",
    "            sensors_df.loc[sensor, (column, \"N° Missing\")] = (\n",
    "                df[column].isna().sum()\n",
    "            )\n",
    "            sensors_df.loc[sensor, (column, \"Mean\")] = df[column].mean()\n",
    "            sensors_df.loc[sensor, (column, \"Std\")] = df[column].std()\n",
    "            continue\n",
    "\n",
    "        sensors_df.loc[sensor, (column, \"N° Data\")] = sensor_dict[sensor][\n",
    "            column\n",
    "        ].count()\n",
    "        sensors_df.loc[sensor, (column, \"N° Missing\")] = (\n",
    "            sensor_dict[sensor][column].isna().sum()\n",
    "        )\n",
    "        sensors_df.loc[sensor, (column, \"Mean\")] = sensor_dict[sensor][\n",
    "            column\n",
    "        ].mean()\n",
    "        sensors_df.loc[sensor, (column, \"Std\")] = sensor_dict[sensor][\n",
    "            column\n",
    "        ].std()\n",
    "        \n",
    "        start_date = sensor_dict[sensor][\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "        end_date = sensor_dict[sensor][\"DateTime\"].max().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        sensors_df.loc[sensor, (column, \"Start Date\")] = start_date\n",
    "        sensors_df.loc[sensor, (column, \"End Date\")] = end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_df.to_excel(os.path.join(metadata_folder, \"Sensor\", \"sensors.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plot Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot pair grid for grab features\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "sns.pairplot(grab_df, vars=feature_mapping.values(), hue=\"Code\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "n_hours = 3\n",
    "\n",
    "for code in sensor_dict:\n",
    "    sensor_df = sensor_dict[code].copy()\n",
    "    \n",
    "    for column in sensor_df.columns:\n",
    "        if column == 'DateTime':\n",
    "            continue\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'{code} - {column}', fontsize=16)\n",
    "        \n",
    "        # Before Moving Average\n",
    "        sns.lineplot(ax=axes[0, 0], x=sensor_df['DateTime'], y=sensor_df[column], color='blue')\n",
    "        axes[0, 0].set_title('Before MA')\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel(column)\n",
    "        axes[0, 0].grid()\n",
    "        \n",
    "        # After Moving Average\n",
    "        df = sensor_df[['DateTime', column]].copy()\n",
    "        df.set_index('DateTime', inplace=True)\n",
    "        \n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "        \n",
    "        df = df.rolling(window=4*n_hours).mean()\n",
    "        sns.lineplot(ax=axes[0, 1], x=sensor_df['DateTime'], y=sensor_df[column], color='blue', alpha=0.3)\n",
    "        sns.lineplot(ax=axes[0, 1], x=df.index, y=df[column], color='green')\n",
    "        axes[0, 1].set_title('After MA')\n",
    "        axes[0, 1].set_xlabel('Time')\n",
    "        axes[0, 1].set_ylabel(column)\n",
    "        axes[0, 1].grid()\n",
    "        \n",
    "        # Histogram Before Moving Average\n",
    "        sns.histplot(ax=axes[1, 0], data=sensor_df, x=column, color='purple', kde=True, stat='density')\n",
    "        axes[1, 0].set_title('Before MA')\n",
    "        axes[1, 0].set_xlabel(column)\n",
    "        axes[1, 0].set_ylabel('Density')\n",
    "        \n",
    "        # Histogram After Moving Average\n",
    "        sns.histplot(ax=axes[1, 1], data=sensor_df, x=column, color='purple', stat='density', alpha=0.3)\n",
    "        sns.histplot(ax=axes[1, 1], data=df, x=column, color='green', kde=True, stat='density')\n",
    "        axes[1, 1].set_title('After MA')\n",
    "        axes[1, 1].set_xlabel(column)\n",
    "        axes[1, 1].set_ylabel('Density')\n",
    "    \n",
    "    \n",
    "        # add a legend of the number of hours\n",
    "        fig.legend(['Before MA', 'After MA'], loc='upper right')\n",
    "        \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        path = os.path.join(plot_folder, 'Clean Data', 'Moving Average', code)\n",
    "        \n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            \n",
    "        column_ = column.replace('/', '_')\n",
    "        \n",
    "        fig.savefig(os.path.join(path, column_ + '.png'), dpi=300)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "# plot the time series of the sensors and the grab data\n",
    "\n",
    "\n",
    "n_hours = 3\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "        \n",
    "        g_df = g_df[[\"DateTime\", feature]].copy()\n",
    "        g_df.dropna(inplace=True)\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "        \n",
    "        start_date = s_df[\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        g_df = g_df[g_df[\"DateTime\"] >= start_date]\n",
    "\n",
    "        # moving average on sensor data\n",
    "\n",
    "        ma_s_df = s_df.copy()\n",
    "\n",
    "        ma_s_df.set_index(\"DateTime\", inplace=True)\n",
    "        ma_s_df = ma_s_df.rolling(window=4 * n_hours).mean()\n",
    "\n",
    "        \n",
    "        std = g_df[feature].std()\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=s_df[\"DateTime\"], y=s_df[feature], mode=\"lines\", name=\"Sensor\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         x=ma_s_df.index,\n",
    "        #         y=ma_s_df[feature],\n",
    "        #         mode=\"lines\",\n",
    "        #         name=\"Sensor MA\",\n",
    "        #         line=dict(color=\"green\"),\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=g_df[\"DateTime\"],\n",
    "                y=g_df[feature],\n",
    "                mode=\"markers\",\n",
    "                name=\"Grab\",\n",
    "                marker=dict(size=12, color=\"red\"), \n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # add the std to each point of the grab data\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature}\",\n",
    "            xaxis_title=\"DateTime\",\n",
    "            yaxis_title=feature,\n",
    "            # put legend inside the plot\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1,\n",
    "            ),\n",
    "            margin=dict(l=0, r=0, t=0, b=0),\n",
    "            hovermode=\"closest\",\n",
    "\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(\n",
    "            os.path.join(plot_folder, \"Comparison\", \"15min\", \"Timeseries\", code)\n",
    "        ):\n",
    "            os.makedirs(\n",
    "                os.path.join(\n",
    "                    plot_folder, \"Comparison\", \"15min\", \"Timeseries\", code\n",
    "                )\n",
    "            )\n",
    "\n",
    "        feature_ = feature.replace(\"/\", \"_\")\n",
    "\n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                plot_folder,\n",
    "                \"Comparison\",\n",
    "                \"15min\",\n",
    "                \"Timeseries\",\n",
    "                code,\n",
    "                f\"{feature_}.png\",\n",
    "            ),\n",
    "            height=600,\n",
    "            width=1200,\n",
    "            scale=3,\n",
    "        )\n",
    "\n",
    "        # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thms_columns = ['Bromodichloromethane (µg/L)', 'Bromoform (µg/L)', 'Chloroform (µg/L)', 'Dibromochloromethane (µg/L)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab_df['TTHMs'] = grab_df[thms_columns].sum(axis=1, min_count=len(thms_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "# plot all the grab samples points\n",
    "\n",
    "n_hours = 3\n",
    "\n",
    "for feature in feature_mapping.values():\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for code in grab_df[\"Code\"].unique():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "        \n",
    "        g_df = g_df[[\"DateTime\", feature]].copy()\n",
    "        g_df.dropna(inplace=True)\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "        \n",
    "        start_date = s_df[\"DateTime\"].min().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        g_df = g_df[g_df[\"DateTime\"] >= start_date]\n",
    "\n",
    "        # moving average on sensor data\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=g_df[\"DateTime\"],\n",
    "                y=g_df[feature],\n",
    "                mode=\"markers+lines\",\n",
    "                name=f\"{code}\",\n",
    "                marker=dict(size=12),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # add the std to each point of the grab data\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"{feature}\",\n",
    "        xaxis_title=\"DateTime\",\n",
    "        yaxis_title=feature,\n",
    "        # put legend inside the plot\n",
    "        legend=dict(\n",
    "            x=1,\n",
    "            y=1,\n",
    "            traceorder=\"normal\",\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, t=30, b=0),\n",
    "        hovermode=\"closest\",\n",
    "\n",
    "    )\n",
    "\n",
    "    feature_ = feature.replace(\"/\", \"_\")\n",
    "\n",
    "    # fig.write_image(\n",
    "    #     f\"{feature_}.png\",\n",
    "    #     scale=3,\n",
    "    # )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplot Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    if month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    if month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    if month in [9, 10, 11]:\n",
    "        return \"Autumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script --false --no-raise-error\n",
    "\n",
    "# plot the box plot of grab data and sensor data\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        # moving average on sensor data\n",
    "        s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "        s_df.set_index(\"DateTime\", inplace=True)\n",
    "        s_df = s_df.rolling(window=4 * n_hours).mean()\n",
    "\n",
    "        sensor_start_date = s_df.index.min().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        g_df[\"DateTime\"] = pd.to_datetime(g_df[\"DateTime\"])\n",
    "        \n",
    "        before_g_df = g_df[g_df[\"DateTime\"] < sensor_start_date]\n",
    "        after_g_df = g_df[g_df[\"DateTime\"] >= sensor_start_date]\n",
    "        \n",
    "        valid_g_df = g_df[g_df[feature + \"_label\"] == \"Normal\"]\n",
    "        loq_g_df = g_df[g_df[feature + \"_label\"] == \"Less than\"]\n",
    "        \n",
    "        valid_before_g_df = valid_g_df[valid_g_df[\"DateTime\"] < sensor_start_date]\n",
    "        valid_after_g_df = valid_g_df[valid_g_df[\"DateTime\"] >= sensor_start_date]\n",
    "        \n",
    "        loq_before_g_df = loq_g_df[loq_g_df[\"DateTime\"] < sensor_start_date]\n",
    "        loq_after_g_df = loq_g_df[loq_g_df[\"DateTime\"] >= sensor_start_date]\n",
    "\n",
    "        # divide before and after into seasons\n",
    "        valid_before_g_df[\"Season\"] = valid_before_g_df[\"DateTime\"].dt.month.apply(\n",
    "            month_to_season\n",
    "        )\n",
    "        valid_after_g_df[\"Season\"] = valid_after_g_df[\"DateTime\"].dt.month.apply(\n",
    "            month_to_season\n",
    "        )\n",
    "\n",
    "        loq_before_g_df[\"Season\"] = loq_before_g_df[\"DateTime\"].dt.month.apply(\n",
    "            month_to_season\n",
    "        )\n",
    "        loq_after_g_df[\"Season\"] = loq_after_g_df[\"DateTime\"].dt.month.apply(\n",
    "            month_to_season\n",
    "        )\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=3,\n",
    "            cols=1,\n",
    "            specs=[[{\"type\": \"xy\"}], [{\"type\": \"table\"}], [{\"type\": \"table\"}]],\n",
    "            subplot_titles=(\n",
    "                \"\",\n",
    "                f\"Grab Samples Before {sensor_start_date}\",\n",
    "                f\"Grab Samples After {sensor_start_date}\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=valid_before_g_df[feature],\n",
    "                name=f\"Valid Old Grab<br>N° Points: {valid_before_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=loq_before_g_df[feature],\n",
    "                name=f\"LOQ Old Grab<br>N° Points: {loq_before_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=before_g_df[feature],\n",
    "                name=f\"Overall Old Grab<br>N° Points: {before_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=valid_after_g_df[feature],\n",
    "                name=f\"Valid New Grab<br>N° Points: {valid_after_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=loq_after_g_df[feature],\n",
    "                name=f\"LOQ New Grab<br>N° Points: {loq_after_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=after_g_df[feature],\n",
    "                name=f\"Overall New Grab<br>N° Points: {after_g_df[feature].count()}\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1\n",
    "        )\n",
    "        \n",
    "\n",
    "        fig.add_trace(go.Box(y=s_df[feature], name=\"Sensor\"), row=1, col=1)\n",
    "\n",
    "        # divide by season for both old and new grab data\n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=[\"Season Valid\", \"N° Points Valid\", \"Mean Valid\", \"Std Valid\", \"Season LOQ\", \"N° Points LOQ\", \"Mean LOQ\", \"Std LOQ\"],\n",
    "                    align=\"center\",\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        valid_before_g_df.groupby(\"Season\").size().index,\n",
    "                        valid_before_g_df.groupby(\"Season\")[feature].count().values,\n",
    "                        valid_before_g_df.groupby(\"Season\")[feature]\n",
    "                        .mean()\n",
    "                        .values.round(2),\n",
    "                        valid_before_g_df.groupby(\"Season\")[feature]\n",
    "                        .std()\n",
    "                        .values.round(2),\n",
    "                        loq_before_g_df.groupby(\"Season\").size().index,\n",
    "                        loq_before_g_df.groupby(\"Season\")[feature].count().values,\n",
    "                        loq_before_g_df.groupby(\"Season\")[feature]\n",
    "                        .mean()\n",
    "                        .values.round(2),\n",
    "                        loq_before_g_df.groupby(\"Season\")[feature]\n",
    "                        .std()\n",
    "                        .values.round(2),\n",
    "                    ],\n",
    "                    align=\"center\",\n",
    "                ),\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=[\"Season Valid\", \"N° Points Valid\", \"Mean Valid\", \"Std Valid\", \"Season LOQ\", \"N° Points LOQ\", \"Mean LOQ\", \"Std LOQ\"],\n",
    "                    align=\"center\",\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=[\n",
    "                        valid_after_g_df.groupby(\"Season\").size().index,\n",
    "                        valid_after_g_df.groupby(\"Season\")[feature].count(),\n",
    "                        valid_after_g_df.groupby(\"Season\")[feature]\n",
    "                        .mean()\n",
    "                        .values.round(2),\n",
    "                        valid_after_g_df.groupby(\"Season\")[feature]\n",
    "                        .std()\n",
    "                        .values.round(2),\n",
    "                        loq_after_g_df.groupby(\"Season\").size().index,\n",
    "                        loq_after_g_df.groupby(\"Season\")[feature].count(),\n",
    "                        loq_after_g_df.groupby(\"Season\")[feature]\n",
    "                        .mean()\n",
    "                        .values.round(2),\n",
    "                        loq_after_g_df.groupby(\"Season\")[feature]\n",
    "                        .std()\n",
    "                        .values.round(2),\n",
    "                    ],\n",
    "                    align=\"center\",\n",
    "                ),\n",
    "            ),\n",
    "            row=3,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{code} - {feature}\",\n",
    "            yaxis_title=feature,\n",
    "        )\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            dict(\n",
    "                x=-0.022,\n",
    "                y=1.07,\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                showarrow=False,\n",
    "                text=f\"Grab Samples divided by date {sensor_start_date}\",\n",
    "                font=dict(size=12, color=\"gray\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(\n",
    "            os.path.join(plot_folder, \"Comparison\", \"Daily\", \"Boxplot\", code)\n",
    "        ):\n",
    "            os.makedirs(\n",
    "                os.path.join(\n",
    "                    plot_folder, \"Comparison\", \"Daily\", \"Boxplot\", code\n",
    "                )\n",
    "            )\n",
    "\n",
    "        feature_ = feature.replace(\"/\", \"_\")\n",
    "\n",
    "        fig.write_image(\n",
    "            os.path.join(\n",
    "                plot_folder, \"Comparison\", \"Daily\", \"Boxplot\", code, f'{feature_}.png',\n",
    "            ),\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            scale=3\n",
    "        )\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bland-Altman Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# With all the supply points together\n",
    "\n",
    "\n",
    "total_g_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "total_s_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "        \n",
    "        # if code == \"Berna\" and feature == \"Free Chlorine (mg/L)\":\n",
    "        #     pass\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        # moving average on sensor data\n",
    "        s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "        s_df.set_index(\"DateTime\", inplace=True)\n",
    "        # 2 hours moving average\n",
    "        s_df = s_df.rolling(window=4 * 2).mean()\n",
    "        \n",
    "        # fix the date of the sensor data to have a frequency of 15 minutes for easier comparison and interpolate to not have nan value\n",
    "        s_df = s_df.resample(\"15min\").mean().interpolate(method=\"time\")\n",
    "\n",
    "        sensor_start_date = s_df.index.dropna().min().strftime(\"%Y-%m-%d\")\n",
    "        sensor_end_date = s_df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        g_df.set_index(\"DateTime\", inplace=True)\n",
    "        \n",
    "        g_df = g_df[(g_df.index >= sensor_start_date) & (g_df.index <= sensor_end_date)]\n",
    "        \n",
    "        g_df = g_df[feature]\n",
    "        g_df.dropna(inplace=True)\n",
    "        \n",
    "        # keep only the sensor values that have the date in the grab data and the hour is between 9 and 11\n",
    "        \n",
    "        dates = pd.Series(s_df.index.date, index=s_df.index).isin(g_df.index.date)\n",
    "        dates = dates[dates.values]\n",
    "        \n",
    "        s_df = s_df.loc[dates.index]\n",
    "        s_df = s_df[(s_df.index.hour == 10) & (s_df.index.minute >= 0) & (s_df.index.minute <= 14)]\n",
    "        \n",
    "        # if there is more than one value for the same date, take the mean\n",
    "        s_df = s_df.groupby(s_df.index.date).mean()\n",
    "        \n",
    "        total_g_df = pd.concat([total_g_df, pd.DataFrame({\"Code\": code, \"DateTime\": g_df.index, \"Feature\": feature, \"Value\": g_df.values})])\n",
    "        total_s_df = pd.concat([total_s_df, pd.DataFrame({\"Code\": code, \"DateTime\": s_df.index, \"Feature\": feature, \"Value\": s_df[feature].values})])\n",
    "\n",
    "\n",
    "for feature in feature_mapping.values():\n",
    "    \n",
    "    g_df = total_g_df[total_g_df[\"Feature\"] == feature]\n",
    "    s_df = total_s_df[total_s_df[\"Feature\"] == feature]\n",
    "    \n",
    "    g_df[\"DateTime\"] = pd.to_datetime(g_df[\"DateTime\"])\n",
    "    s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "    \n",
    "        \n",
    "    df = pd.merge(g_df, s_df, on=[\"Code\", \"DateTime\"], suffixes=(\"_Grab\", \"_Sensor\"))\n",
    "    df[\"Difference\"] = df[\"Value_Grab\"] - df[\"Value_Sensor\"]\n",
    "    df['Mean'] = (df[\"Value_Grab\"] + df[\"Value_Sensor\"]) / 2\n",
    "    \n",
    "\n",
    "    difference_mean = np.mean(df[\"Difference\"].values)\n",
    "    difference_std = np.std(df[\"Difference\"].values)\n",
    "    std_error = difference_std / np.sqrt(g_df.shape[0])\n",
    "\n",
    "    ci_difference_mean = 1.96 * std_error\n",
    "    \n",
    "    \n",
    "    f, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    sns.scatterplot(data=df, x=\"Mean\", y=\"Difference\", hue=\"Code\", ax=ax, s=100)\n",
    "    \n",
    "    ax.axhline(y=difference_mean, color='green', linestyle='--', label='Mean')\n",
    "    \n",
    "    if feature == \"Conductivity (uS/cm)\":\n",
    "        ax.text(x=712, y=difference_mean, s=f'Mean: {difference_mean:.2f}', color='green')\n",
    "    \n",
    "    else:\n",
    "        ax.text(x=df['Mean'].quantile(0.9), y=difference_mean + std_error, s=f'Mean: {difference_mean:.2f}', color='green')\n",
    "    \n",
    "    ax.axhline(y=difference_mean + 1.96 * difference_std, color='red', linestyle='--', label='1.96 * Std')\n",
    "    # add text over the horizontal line\n",
    "    \n",
    "    if feature == \"Conductivity (uS/cm)\":\n",
    "        ax.text(x=712, y=difference_mean + 1.96 * difference_std , s=f'+ 1.96 * Std', color='red')\n",
    "        \n",
    "    else:\n",
    "        ax.text(x=df['Mean'].quantile(0.9), y=difference_mean + 1.96 * difference_std + std_error, s=f'+ 1.96 * Std', color='red')\n",
    "    \n",
    "    ax.axhline(y=difference_mean - 1.96 * difference_std, color='red', linestyle='--', label='-1.96 * Std')\n",
    "    # add text over the horizontal line\n",
    "    \n",
    "    if feature == \"Conductivity (uS/cm)\":\n",
    "        ax.text(x=712, y=difference_mean - 1.96 * difference_std, s=f'-1.96 * Std', color='red')\n",
    "        \n",
    "    else:\n",
    "        ax.text(x=df['Mean'].quantile(0.9), y=difference_mean - 1.96 * difference_std + std_error, s=f'-1.96 * Std', color='red')\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "\n",
    "    plt.annotate(\n",
    "        f'Std error: {std_error:.2f}\\nDifference mean CI: {difference_mean:.2f} ± {ci_difference_mean:.2f}',\n",
    "        xy=(0.6, 0.94),\n",
    "        xycoords='axes fraction',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightblue\"),\n",
    "        color='green',\n",
    "        fontsize=14\n",
    "    )\n",
    "    \n",
    "    plt.title(f'{feature} - Bland-Altman Plot')\n",
    "    plt.tight_layout(pad=2)\n",
    "    \n",
    "    # increment the font size\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(16)\n",
    "        \n",
    "    for item in ax.get_legend().get_texts():\n",
    "        item.set_fontsize(14)\n",
    "\n",
    "    # display Bland-Altman plot\n",
    "    if not os.path.exists(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", \"All\")):\n",
    "        os.makedirs(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", \"All\"))\n",
    "        \n",
    "    feature_ = feature.replace(\"/\", \"_\")\n",
    "\n",
    "    plt.savefig(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", \"All\", f'{feature_}.png'), dpi=300)\n",
    "    # plt.close(f)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# With all the supply points together\n",
    "\n",
    "total_g_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "total_s_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "        \n",
    "        # if code == \"Berna\" and feature == \"Free Chlorine (mg/L)\":\n",
    "        #     pass\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        # moving average on sensor data\n",
    "        s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "        s_df.set_index(\"DateTime\", inplace=True)\n",
    "        # 2 hours moving average\n",
    "        s_df = s_df.rolling(window=4 * 2).mean()\n",
    "        \n",
    "        # fix the date of the sensor data to have a frequency of 15 minutes for easier comparison and interpolate to not have nan value\n",
    "        s_df = s_df.resample(\"15min\").mean().interpolate(method=\"time\")\n",
    "\n",
    "        sensor_start_date = s_df.index.dropna().min().strftime(\"%Y-%m-%d\")\n",
    "        sensor_end_date = s_df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        g_df.set_index(\"DateTime\", inplace=True)\n",
    "        \n",
    "        g_df = g_df[(g_df.index >= sensor_start_date) & (g_df.index <= sensor_end_date)]\n",
    "        \n",
    "        g_df = g_df[feature]\n",
    "        g_df.dropna(inplace=True)\n",
    "        \n",
    "        # keep only the sensor values that have the date in the grab data and the hour is between 9 and 11\n",
    "        \n",
    "        dates = pd.Series(s_df.index.date, index=s_df.index).isin(g_df.index.date)\n",
    "        dates = dates[dates.values]\n",
    "        \n",
    "        s_df = s_df.loc[dates.index]\n",
    "        s_df = s_df[(s_df.index.hour == 10) & (s_df.index.minute >= 0) & (s_df.index.minute <= 14)]\n",
    "        \n",
    "        # if there is more than one value for the same date, take the mean\n",
    "        s_df = s_df.groupby(s_df.index.date).mean()\n",
    "        \n",
    "        total_g_df = pd.concat([total_g_df, pd.DataFrame({\"Code\": code, \"DateTime\": g_df.index, \"Feature\": feature, \"Value\": g_df.values})])\n",
    "        total_s_df = pd.concat([total_s_df, pd.DataFrame({\"Code\": code, \"DateTime\": s_df.index, \"Feature\": feature, \"Value\": s_df[feature].values})])\n",
    "\n",
    "\n",
    "for feature in feature_mapping.values():\n",
    "    \n",
    "    g_df = total_g_df[total_g_df[\"Feature\"] == feature]\n",
    "    s_df = total_s_df[total_s_df[\"Feature\"] == feature]\n",
    "    \n",
    "    g_df[\"DateTime\"] = pd.to_datetime(g_df[\"DateTime\"])\n",
    "    s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "    \n",
    "        \n",
    "    df = pd.merge(g_df, s_df, on=[\"Code\", \"DateTime\"], suffixes=(\"_Grab\", \"_Sensor\"))\n",
    "    df[\"Difference\"] = df[\"Value_Grab\"] - df[\"Value_Sensor\"]\n",
    "    df['Mean'] = (df[\"Value_Grab\"] + df[\"Value_Sensor\"]) / 2\n",
    "    \n",
    "\n",
    "    difference_mean = np.mean(df[\"Difference\"].values)\n",
    "    difference_std = np.std(df[\"Difference\"].values)\n",
    "    std_error = difference_std / np.sqrt(g_df.shape[0])\n",
    "\n",
    "    ci_difference_mean = 1.96 * std_error\n",
    "    \n",
    "    \n",
    "    f, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    sns.scatterplot(data=df, x=\"DateTime\", y=\"Difference\", hue=\"Code\", ax=ax, s=100)\n",
    "    \n",
    "    ax.axhline(y=difference_mean, color='green', linestyle='--', label='Mean')\n",
    "    \n",
    "    ax.text(x=pd.Timestamp('2024-08-15'), y=difference_mean + std_error, s=f'Mean: {difference_mean:.2f}', color='green')\n",
    "    \n",
    "    ax.axhline(y=difference_mean + 1.96 * difference_std, color='red', linestyle='--', label='1.96 * Std')\n",
    "    # add text over the horizontal line\n",
    "    ax.text(x=pd.Timestamp('2024-08-15'), y=difference_mean + 1.96 * difference_std + std_error, s=f'+ 1.96 * Std', color='red')\n",
    "    \n",
    "    ax.axhline(y=difference_mean - 1.96 * difference_std, color='red', linestyle='--', label='-1.96 * Std')\n",
    "    # add text over the horizontal line\n",
    "    ax.text(x=pd.Timestamp('2024-08-15'), y=difference_mean - 1.96 * difference_std + std_error, s=f'-1.96 * Std', color='red')\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "\n",
    "    plt.annotate(\n",
    "        f'Std error: {std_error:.2f}\\nDifference mean CI: {difference_mean:.2f} ± {ci_difference_mean:.2f}',\n",
    "        xy=(0.05, 0.9),\n",
    "        xycoords='axes fraction',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightblue\"),\n",
    "        color='green'\n",
    "    )\n",
    "    \n",
    "    plt.title(f'{feature} - Bland-Altman Plot')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # display Bland-Altman plot\n",
    "    if not os.path.exists(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", \"All_Date\")):\n",
    "        os.makedirs(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", \"All_Date\"))\n",
    "        \n",
    "    feature_ = feature.replace(\"/\", \"_\")\n",
    "\n",
    "    plt.savefig(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", \"All_Date\", f'{feature_}.png'), dpi=300)\n",
    "    plt.close(f)\n",
    "\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# For each supply point\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "        \n",
    "        # if code == \"Berna\" and feature == \"Free Chlorine (mg/L)\":\n",
    "        #     pass\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        # moving average on sensor data\n",
    "        s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "        s_df.set_index(\"DateTime\", inplace=True)\n",
    "        # 2 hours moving average\n",
    "        s_df = s_df.rolling(window=4 * 2).mean()\n",
    "        \n",
    "        # fix the date of the sensor data to have a frequency of 15 minutes for easier comparison and interpolate to not have nan value\n",
    "        s_df = s_df.resample(\"15min\").mean().interpolate(method=\"time\")\n",
    "\n",
    "        sensor_start_date = s_df.index.dropna().min().strftime(\"%Y-%m-%d\")\n",
    "        sensor_end_date = s_df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        g_df.set_index(\"DateTime\", inplace=True)\n",
    "        \n",
    "        g_df = g_df[(g_df.index >= sensor_start_date) & (g_df.index <= sensor_end_date)]\n",
    "        \n",
    "        g_df = g_df[feature]\n",
    "        g_df.dropna(inplace=True)\n",
    "        \n",
    "        # keep only the sensor values that have the date in the grab data and the hour is between 9 and 11\n",
    "        \n",
    "        dates = pd.Series(s_df.index.date, index=s_df.index).isin(g_df.index.date)\n",
    "        dates = dates[dates.values]\n",
    "        \n",
    "        s_df = s_df.loc[dates.index]\n",
    "        s_df = s_df[(s_df.index.hour == 10) & (s_df.index.minute >= 0) & (s_df.index.minute <= 14)]\n",
    "        \n",
    "        # if there is more than one value for the same date, take the mean\n",
    "        s_df = s_df.groupby(s_df.index.date).mean()\n",
    "\n",
    "        \n",
    "        f, ax = plt.figure(figsize=(10, 6)), plt.gca()\n",
    "        \n",
    "        # try:\n",
    "        #     sm.graphics.mean_diff_plot(g_df.values, s_df[feature].values, ax = ax)\n",
    "        # except:\n",
    "        #     pass\n",
    "        sm.graphics.mean_diff_plot(g_df.values, s_df[feature].values, ax = ax)\n",
    "        \n",
    "        \n",
    "        plt.title(f'{code} - {feature}')\n",
    "        \n",
    "        difference_mean = np.mean(g_df.values - s_df[feature].values)\n",
    "        difference_std = np.std(g_df.values - s_df[feature].values)\n",
    "        std_error = difference_std / np.sqrt(g_df.shape[0])\n",
    "        \n",
    "        ci_difference_mean = 1.96 * std_error\n",
    "        \n",
    "        \n",
    "        plt.annotate(\n",
    "            f'Std error: {std_error:.2f}\\nDifference mean CI: {difference_mean:.2f} ± {ci_difference_mean:.2f}',\n",
    "            xy=(0.05, 0.9),\n",
    "            xycoords='axes fraction',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightblue\"),\n",
    "            color='red'\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        #display Bland-Altman plot\n",
    "        if not os.path.exists(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", code)):\n",
    "            os.makedirs(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", code))\n",
    "            \n",
    "        feature_ = feature.replace(\"/\", \"_\")\n",
    "        \n",
    "        plt.savefig(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", code, f'{feature_}.png'), dpi=300)\n",
    "        plt.close(f)\n",
    "        \n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Measurement Time between Supply Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the timestamps are the same for each code\n",
    "\n",
    "common_dates = pd.Series()\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    \n",
    "    code_df = grab_df[grab_df[\"Code\"] == code][\"DateTime\"].copy()\n",
    "        \n",
    "        \n",
    "    if common_dates.empty:\n",
    "        common_dates = code_df\n",
    "    \n",
    "    else:\n",
    "        common_dates = pd.Series(list(set(common_dates).intersection(set(code_df))))\n",
    "        \n",
    "common_dates = common_dates.sort_values()\n",
    "\n",
    "common_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot compare measurements on the same day, because the grab samples are taken at different times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in list(feature_mapping.values()):\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for code in grab_df[\"Code\"].unique():\n",
    "        code_df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=code_df[\"DateTime\"], y=code_df[feature], mode=\"markers\", name=code\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    fig.update_layout(\n",
    "        title=feature,\n",
    "        xaxis_title=\"DateTime\",\n",
    "        yaxis_title=feature\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation with MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_non_normal(row, column):\n",
    "    return np.nan if row[column + \"_label\"] != \"Normal\" else row[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns that contain the word 'label'\n",
    "label_columns = [col for col in grab_df.columns if 'label' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_removed_grab_df = grab_df.copy()\n",
    "\n",
    "for column in grab_df.columns.difference([\"Code\", \"DateTime\"] + label_columns):\n",
    "    label_removed_grab_df[column] = label_removed_grab_df.apply(\n",
    "        lambda row: replace_non_normal(row, column), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that the columns have been replaced correctly\n",
    "for column in label_removed_grab_df.columns.difference([\"Code\", \"DateTime\"] + label_columns):\n",
    "    # assert that all the values that are nan should have a label different from 'Normal'\n",
    "    assert label_removed_grab_df[(label_removed_grab_df[column].isna()) & (label_removed_grab_df[column + \"_label\"] == 'Normal')].shape[0] == 0 \n",
    "\n",
    "    # assert that all the values that are not nan should have a label equal to 'Normal'\n",
    "    assert label_removed_grab_df[(label_removed_grab_df[column].notna()) & (label_removed_grab_df[column + \"_label\"] != 'Normal')].shape[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every feature, save the indexes of the values that are less than the LOQ\n",
    "indexes_df = pd.DataFrame(columns=feature_mapping.values(), index=grab_df[\"Code\"].unique())\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        df = grab_df[grab_df[\"Code\"] == code].copy()\n",
    "        \n",
    "        df = df[df[feature + \"_label\"] == \"Less than\"]\n",
    "        \n",
    "        indexes_df.loc[code, feature] = df.index.to_list()\n",
    "    \n",
    "indexes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the label columns\n",
    "label_removed_grab_df = label_removed_grab_df.drop(columns=label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miceforest as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = label_removed_grab_df.copy()\n",
    "\n",
    "df = df[list(feature_mapping.values()) + ['DateTime'] + ['Code']]\n",
    "\n",
    "# convert datetime column to float\n",
    "df['DateTime'] = pd.to_numeric(df['DateTime'])\n",
    "df['Code'] = df['Code'].astype('category')\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=df,\n",
    "    variable_schema=df.columns.difference(['DateTime', 'Code']).to_list(),\n",
    "    random_state=42,\n",
    "    mean_match_strategy='shap',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['Code'] == code].isnull().all(axis=1).sum())  # Rows with all NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['Code'] == code].isnull().all(axis=0).sum())  # Columns with all NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MICE imputation\n",
    "kernel.mice(4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# comparison of the less than LOQ values before and after imputation\n",
    "\n",
    "\n",
    "for feature in ['Free Chlorine (mg/L)', 'Color (CU)', 'Turbidity (NTU)']:\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for code in grab_df[\"Code\"].unique():\n",
    "    \n",
    "        indexes = indexes_df.loc[code, feature]\n",
    "        \n",
    "        df = label_removed_grab_df.copy()\n",
    "        \n",
    "        df = df[(df[\"Code\"] == code) & (df.index.isin(indexes))]\n",
    "        \n",
    "        df = df[[feature]]\n",
    "        \n",
    "        df['Imputed'] = completed_dataset.loc[df.index, feature]\n",
    "        \n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df.index, y=df['Imputed'], mode='markers', name=code\n",
    "            )\n",
    "        )\n",
    "    if feature == 'Free Chlorine (mg/L)':\n",
    "        loq_value = 0.04\n",
    "    elif feature == 'Color (CU)':\n",
    "        loq_value = 1\n",
    "    else:\n",
    "        loq_value = 0.3\n",
    "        \n",
    "    fig.add_hline(y=loq_value, line_dash='dash', line_color='black', opacity=0.3)\n",
    "        \n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{feature} - LOQ: {loq_value}',\n",
    "        xaxis_title='Index',\n",
    "        yaxis_title=feature\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(os.path.join(plot_folder, 'Imputation', 'LOQ')):\n",
    "            os.makedirs(os.path.join(plot_folder, 'Imputation', 'LOQ'))\n",
    "\n",
    "    feature_ = feature.replace('/', '_')\n",
    "\n",
    "    fig.write_image(os.path.join(plot_folder, 'Imputation', 'LOQ', f'{feature_}.png'), height=600, width=1200, scale=3)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the datetime column back to datetime\n",
    "completed_dataset['DateTime'] = pd.to_datetime(completed_dataset['DateTime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bland-Altman Imputed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# With all the supply points together\n",
    "\n",
    "total_g_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "total_s_df = pd.DataFrame(columns=[\"Code\", \"DateTime\", \"Feature\", \"Value\"])\n",
    "\n",
    "for code in grab_df[\"Code\"].unique():\n",
    "    for feature in feature_mapping.values():\n",
    "        g_df = completed_dataset[completed_dataset[\"Code\"] == code].copy()\n",
    "        \n",
    "        # if code == \"Berna\" and feature == \"Free Chlorine (mg/L)\":\n",
    "        #     pass\n",
    "\n",
    "        s_df = sensor_dict[code].copy()\n",
    "\n",
    "        # moving average on sensor data\n",
    "        s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "        s_df.set_index(\"DateTime\", inplace=True)\n",
    "        # 2 hours moving average\n",
    "        s_df = s_df.rolling(window=4 * 2).mean()\n",
    "        \n",
    "        # fix the date of the sensor data to have a frequency of 15 minutes for easier comparison and interpolate to not have nan value\n",
    "        s_df = s_df.resample(\"15min\").mean().interpolate(method=\"time\")\n",
    "\n",
    "        sensor_start_date = s_df.index.dropna().min().strftime(\"%Y-%m-%d\")\n",
    "        sensor_end_date = s_df.dropna().index.max().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        g_df.set_index(\"DateTime\", inplace=True)\n",
    "        \n",
    "        g_df = g_df[(g_df.index >= sensor_start_date) & (g_df.index <= sensor_end_date)]\n",
    "        \n",
    "        g_df = g_df[feature]\n",
    "        g_df.dropna(inplace=True)\n",
    "        \n",
    "        # keep only the sensor values that have the date in the grab data and the hour is between 9 and 11\n",
    "        \n",
    "        dates = pd.Series(s_df.index.date, index=s_df.index).isin(g_df.index.date)\n",
    "        dates = dates[dates.values]\n",
    "        \n",
    "        s_df = s_df.loc[dates.index]\n",
    "        s_df = s_df[(s_df.index.hour == 10) & (s_df.index.minute >= 0) & (s_df.index.minute <= 14)]\n",
    "        \n",
    "        # if there is more than one value for the same date, take the mean\n",
    "        s_df = s_df.groupby(s_df.index.date).mean()\n",
    "        \n",
    "        total_g_df = pd.concat([total_g_df, pd.DataFrame({\"Code\": code, \"DateTime\": g_df.index, \"Feature\": feature, \"Value\": g_df.values})])\n",
    "        total_s_df = pd.concat([total_s_df, pd.DataFrame({\"Code\": code, \"DateTime\": s_df.index, \"Feature\": feature, \"Value\": s_df[feature].values})])\n",
    "\n",
    "\n",
    "for feature in feature_mapping.values():\n",
    "    \n",
    "    g_df = total_g_df[total_g_df[\"Feature\"] == feature]\n",
    "    s_df = total_s_df[total_s_df[\"Feature\"] == feature]\n",
    "    \n",
    "    g_df[\"DateTime\"] = pd.to_datetime(g_df[\"DateTime\"])\n",
    "    s_df[\"DateTime\"] = pd.to_datetime(s_df[\"DateTime\"])\n",
    "    \n",
    "        \n",
    "    df = pd.merge(g_df, s_df, on=[\"Code\", \"DateTime\"], suffixes=(\"_Grab\", \"_Sensor\"))\n",
    "    df[\"Difference\"] = df[\"Value_Grab\"] - df[\"Value_Sensor\"]\n",
    "    df['Mean'] = (df[\"Value_Grab\"] + df[\"Value_Sensor\"]) / 2\n",
    "    \n",
    "\n",
    "    difference_mean = np.mean(df[\"Difference\"].values)\n",
    "    difference_std = np.std(df[\"Difference\"].values)\n",
    "    std_error = difference_std / np.sqrt(g_df.shape[0])\n",
    "\n",
    "    ci_difference_mean = 1.96 * std_error\n",
    "    \n",
    "    \n",
    "    f, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    sns.scatterplot(data=df, x=\"Mean\", y=\"Difference\", hue=\"Code\", ax=ax, s=100)\n",
    "    \n",
    "    ax.axhline(y=difference_mean, color='green', linestyle='--', label='Mean')\n",
    "    \n",
    "    ax.text(x=df['Mean'].quantile(0.97), y=difference_mean + std_error, s=f'Mean: {difference_mean:.2f}', color='green')\n",
    "    \n",
    "    ax.axhline(y=difference_mean + 1.96 * difference_std, color='red', linestyle='--', label='1.96 * Std')\n",
    "    # add text over the horizontal line\n",
    "    ax.text(x=df['Mean'].quantile(0.97), y=difference_mean + 1.96 * difference_std + std_error, s=f'+ 1.96 * Std', color='red')\n",
    "    \n",
    "    ax.axhline(y=difference_mean - 1.96 * difference_std, color='red', linestyle='--', label='-1.96 * Std')\n",
    "    # add text over the horizontal line\n",
    "    ax.text(x=df['Mean'].quantile(0.97), y=difference_mean - 1.96 * difference_std + std_error, s=f'-1.96 * Std', color='red')\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "\n",
    "    plt.annotate(\n",
    "        f'Std error: {std_error:.2f}\\nDifference mean CI: {difference_mean:.2f} ± {ci_difference_mean:.2f}',\n",
    "        xy=(0.05, 0.9),\n",
    "        xycoords='axes fraction',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"lightblue\"),\n",
    "        color='green'\n",
    "    )\n",
    "    \n",
    "    plt.title(f'{feature} - Bland-Altman Plot')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # display Bland-Altman plot\n",
    "    if not os.path.exists(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", \"Imputed\")):\n",
    "        os.makedirs(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", \"Imputed\"))\n",
    "        \n",
    "    feature_ = feature.replace(\"/\", \"_\")\n",
    "\n",
    "    plt.savefig(os.path.join(plot_folder, \"Bland-Altman\", \"Supply Points\", \"Imputed\", f'{feature_}.png'), dpi=300)\n",
    "    plt.close(f)\n",
    "\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation with LOQ/2\n",
    "\n",
    "Value points with the label 'Less than' are imputed with LOQ/2, while value points with label 'NaN' are imputed with MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_loq(row, column):\n",
    "    return row[column] if row[column + \"_label\"] != \"Less than\" else row[column] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = [col for col in grab_df.columns if 'label' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grab_df.copy()\n",
    "\n",
    "for column in grab_df.columns.difference([\"Code\", \"DateTime\"] + label_columns):\n",
    "    df[column] = df.apply(\n",
    "        lambda row: replace_loq(row, column), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[list(feature_mapping.values()) + ['DateTime'] + ['Code']]\n",
    "\n",
    "# convert datetime column to float\n",
    "df['DateTime'] = pd.to_numeric(df['DateTime'])\n",
    "df['Code'] = df['Code'].astype('category')\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    if df[df['Code'] == code].isnull().all(axis=1).sum() > 0: # Rows with all NaN\n",
    "        print(f'{code} has {df[df['Code'] == code].isnull().all(axis=1).sum()} rows with all NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in grab_df[\"Code\"].unique():\n",
    "    if df[df['Code'] == code].isnull().all(axis=0).sum() > 0: # Columns with all NaN\n",
    "        print(f'{code} has {df[df[\"Code\"] == code].isnull().all(axis=0).sum()} columns with all NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MICE imputation\n",
    "\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=df,\n",
    "    variable_schema=df.columns.difference(['DateTime', 'Code']).to_list(),\n",
    "    random_state=42,\n",
    "    mean_match_strategy='shap',\n",
    ")\n",
    "\n",
    "kernel.mice(4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_codes = grab_df[\"Code\"].unique()\n",
    "\n",
    "code_mapping = {code: i for i, code in enumerate(house_codes)}\n",
    "df = completed_dataset[['Code'] + list(feature_mapping.values())].copy()\n",
    "\n",
    "df['Code'] = df['Code'].map(code_mapping)\n",
    "df['Code'] = df['Code'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert the code mapping\n",
    "# df['Code'] = df['Code'].map({v: k for k, v in code_mapping.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(n_neighbors=10, random_state=42).fit(df[['Code'] + list(feature_mapping.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(mapper, labels=df['Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover_data = pd.DataFrame({\n",
    "    'index': np.arange(df.shape[0]),\n",
    "    'label': df['Code']\n",
    "})\n",
    "\n",
    "hover_data['item'] = hover_data.label.map(\n",
    "    {v: k for k, v in code_mapping.items()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = umap.plot.interactive(mapper, labels=df['Code'], hover_data=hover_data, point_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import save, output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file(os.path.join(plot_folder, 'Imputation', 'UMAP.html'))\n",
    "\n",
    "save(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sembra che Tognazzi sia abbastanza distante dagli altri, anche il pair plot fa notare una leggera differenza rispetto agli altri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COP - KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copkmeans.cop_kmeans import cop_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "\n",
    "must_link = []\n",
    "\n",
    "code_column = df['Code']\n",
    "\n",
    "for code in df['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(df[df['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = df[list(feature_mapping.values())].to_numpy()\n",
    "\n",
    "clusters, centers = cop_kmeans(np_df, n_clusters, ml=must_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = clusters\n",
    "df['Code'] = code_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in df['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = df[df['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    # get the key from the value\n",
    "    codes = [k for k, v in code_mapping.items() if v in codes]\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Cluster'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the connectivity matrix such that each sample with the same code is connected\n",
    "connectivity = np.zeros((df.shape[0], df.shape[0]))\n",
    "\n",
    "for code in df['Code'].unique():\n",
    "    # indexes = df[df['Code'] == code].index.to_numpy()\n",
    "    # # Set connectivity for all pairs of samples with the same 'Code' to 1\n",
    "    # connectivity[indexes[:, None], indexes] = 1\n",
    "    \n",
    "    index_pairs = list(combinations(df[df['Code'] == code].index, 2))\n",
    "    for i, j in index_pairs:\n",
    "        connectivity[i, j] = 1\n",
    "        connectivity[j, i] = 1\n",
    "\n",
    "connectivity = sparse.csr_matrix(connectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward = AgglomerativeClustering(n_clusters=9, linkage='ward', connectivity=connectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward.fit(df[['Code'] + list(feature_mapping.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = ward.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in df['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = df[df['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    # get the key from the value\n",
    "    codes = [k for k, v in code_mapping.items() if v in codes]\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "df1 = completed_dataset[completed_dataset['Code'] == 'Berna']['Temperature (°C)'].copy()\n",
    "df2 = completed_dataset[completed_dataset['Code'] == 'Tabacchi']['Temperature (°C)'].copy()\n",
    "\n",
    "print(\"Wasserstein no standardization:\", wasserstein_distance(df1, df2))\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df1 = pd.Series(scaler.fit_transform(df1.values.reshape(-1, 1)).flatten())\n",
    "\n",
    "df2 = pd.Series(scaler.fit_transform(df2.values.reshape(-1, 1)).flatten())\n",
    "\n",
    "print(\"Wasserstein with standardization:\", wasserstein_distance(df1, df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house1, house2 in combinations(house_codes, 2):\n",
    "    df1 = completed_dataset[completed_dataset['Code'] == house1][feature_mapping.values()].copy()\n",
    "    df2 = completed_dataset[completed_dataset['Code'] == house2][feature_mapping.values()].copy()\n",
    "    \n",
    "    print(f\"House1: {house1} - House2: {house2}\")\n",
    "    print(np.mean(cosine_similarity(df1, df2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standadization affects the wasserstein distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On THMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thms_columns = ['Bromodichloromethane (µg/L)', 'Bromoform (µg/L)', 'Chloroform (µg/L)', 'Dibromochloromethane (µg/L)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[['Code'] + thms_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_codes = grab_df[\"Code\"].unique()\n",
    "\n",
    "code_mapping = {code: i for i, code in enumerate(house_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grab_df[['Code'] + thms_columns].copy()\n",
    "\n",
    "df['Code'] = df['Code'].map(code_mapping)\n",
    "df['Code'] = df['Code'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df.copy()\n",
    "dataframe = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['TTHM'] = dataframe[thms_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe['TTHM'].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = []\n",
    "\n",
    "for n_cluster in range(2, 10):\n",
    "    clusters, centers = cop_kmeans(np_df, n_cluster, ml=must_link)\n",
    "    \n",
    "    dataframe['Cluster'] = clusters\n",
    "    \n",
    "    # compute the variance of each cluster\n",
    "    variance = 0\n",
    "    for cluster in dataframe['Cluster'].unique():\n",
    "        cluster_df = dataframe[dataframe['Cluster'] == cluster].copy()\n",
    "        variance += np.var(cluster_df['TTHM'])\n",
    "    \n",
    "    # compute average variance for n_cluster\n",
    "    variance /= n_cluster    \n",
    "    \n",
    "    variances.append(variance)\n",
    "    \n",
    "    dataframe.drop(columns=['Cluster'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the elbow curve\n",
    "plt.plot(range(2, 10), variances)\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average Variance')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimal number of clusters is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "\n",
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe['TTHM'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "clusters, centers = cop_kmeans(np_df, n_clusters, ml=must_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in dataframe['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = dataframe[dataframe['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    # get the key from the value\n",
    "    codes = [k for k, v in code_mapping.items() if v in codes]\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the clusters\n",
    "fig = go.Figure()\n",
    "\n",
    "# for each code, use a different marker\n",
    "# for each cluster, use a different color\n",
    "\n",
    "\n",
    "symbols = [\"circle\", \"square\", \"diamond\", \"cross\", \"triangle-up\", \"triangle-down\", \n",
    "           \"pentagon\", \"hexagon\", \"star\"]\n",
    "\n",
    "colors = [\"red\", \"blue\", \"green\", \"purple\",]\n",
    "\n",
    "for i, cluster in enumerate(dataframe['Cluster'].unique()):\n",
    "    for j, code in enumerate(dataframe['Code'].unique()):\n",
    "        subset = dataframe[(dataframe[\"Cluster\"] == cluster) & (dataframe[\"Code\"] == code)]\n",
    "        \n",
    "        code_name = [k for k, v in code_mapping.items() if v == code][0]\n",
    "        \n",
    "        if not subset.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=subset['Code'].map({v: k for k, v in code_mapping.items()}),  # X-axis as the index\n",
    "                y=subset[\"TTHM\"],  # Y-axis as the variable\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    symbol=symbols[j],  # Unique symbol for each code\n",
    "                    size=10,\n",
    "                    line=dict(width=1),\n",
    "                    color=colors[i],  # Unique color for each cluster\n",
    "                ),\n",
    "                name=f\"{code_name}, Cluster: {cluster}\"\n",
    "            ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Clusters\",\n",
    "    xaxis_title=\"Code\",\n",
    "    yaxis_title=\"TTHM\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thms_columns = ['Bromodichloromethane (µg/L)', 'Bromoform (µg/L)', 'Chloroform (µg/L)', 'Dibromochloromethane (µg/L)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df[['Code'] + thms_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_codes = grab_df[\"Code\"].unique()\n",
    "\n",
    "code_mapping = {code: i for i, code in enumerate(house_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grab_df[['Code'] + thms_columns].copy()\n",
    "\n",
    "df['Code'] = df['Code'].map(code_mapping)\n",
    "df['Code'] = df['Code'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df.copy()\n",
    "dataframe = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['TTHM'] = dataframe[thms_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe[thms_columns + ['TTHM']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = []\n",
    "\n",
    "for n_cluster in range(2, 10):\n",
    "    clusters, centers = cop_kmeans(np_df, n_cluster, ml=must_link)\n",
    "    \n",
    "    dataframe['Cluster'] = clusters\n",
    "    \n",
    "    # compute the variance of each cluster\n",
    "    variance = 0\n",
    "    for cluster in dataframe['Cluster'].unique():\n",
    "        cluster_df = dataframe[dataframe['Cluster'] == cluster].copy()\n",
    "        variance += np.var(cluster_df[thms_columns + ['TTHM']]).mean()\n",
    "    \n",
    "    # compute average variance for n_cluster\n",
    "    variance /= n_cluster    \n",
    "    \n",
    "    variances.append(variance)\n",
    "    \n",
    "    dataframe.drop(columns=['Cluster'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the elbow curve\n",
    "plt.plot(range(2, 10), variances)\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average Variance')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimal number of clusters is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "\n",
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe[thms_columns + ['TTHM']].to_numpy()\n",
    "\n",
    "clusters, centers = cop_kmeans(np_df, n_clusters, ml=must_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in dataframe['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = dataframe[dataframe['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    # get the key from the value\n",
    "    codes = [k for k, v in code_mapping.items() if v in codes]\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On inputs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset['DateTime'] = pd.to_datetime(completed_dataset['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = list(feature_mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_codes = completed_dataset[\"Code\"].unique()\n",
    "\n",
    "code_mapping = {code: i for i, code in enumerate(house_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = completed_dataset.copy()\n",
    "\n",
    "df['Code'] = df['Code'].map(code_mapping)\n",
    "df['Code'] = df['Code'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df.copy()\n",
    "dataframe = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "dataframe[input_columns] = pd.DataFrame(scaler.fit_transform(dataframe[input_columns]), columns=input_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe[input_columns].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "variances = []\n",
    "sil_scores = []\n",
    "db_scores = []\n",
    "ch_scores = []\n",
    "\n",
    "\n",
    "total_points = dataframe.shape[0]\n",
    "\n",
    "for n_cluster in range(2, 10):\n",
    "    clusters, centers = cop_kmeans(np_df, n_cluster, ml=must_link)\n",
    "    \n",
    "    sil_score = silhouette_score(np_df, clusters)\n",
    "    db_score = davies_bouldin_score(np_df, clusters)\n",
    "    ch_score = calinski_harabasz_score(np_df, clusters)\n",
    "    \n",
    "    dataframe['Cluster'] = clusters\n",
    "    \n",
    "    # compute the variance of each cluster\n",
    "    variance = 0\n",
    "    for cluster in dataframe['Cluster'].unique():\n",
    "        cluster_df = dataframe[dataframe['Cluster'] == cluster].copy()\n",
    "        variance += np.var(cluster_df[input_columns], axis=0).mean() * cluster_df.shape[0] / total_points\n",
    "    \n",
    "    # compute average variance for n_cluster\n",
    "    variance /= n_cluster    \n",
    "    \n",
    "    variances.append(variance)\n",
    "    sil_scores.append(sil_score)\n",
    "    db_scores.append(db_score)\n",
    "    ch_scores.append(ch_score)\n",
    "    \n",
    "    dataframe.drop(columns=['Cluster'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the scores\n",
    "variances = np.array(variances)\n",
    "sil_scores = np.array(sil_scores)\n",
    "db_scores = np.array(db_scores)\n",
    "ch_scores = np.array(ch_scores)\n",
    "\n",
    "variances = scaler.fit_transform(variances.reshape(-1, 1)).flatten()\n",
    "sil_scores = scaler.fit_transform(sil_scores.reshape(-1, 1)).flatten()\n",
    "db_scores = scaler.fit_transform(db_scores.reshape(-1, 1)).flatten()\n",
    "ch_scores = scaler.fit_transform(ch_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "plt.plot(range(2, 10), variances, label='Weighted Average Variance')\n",
    "plt.plot(range(2, 10), sil_scores, label='Silhouette Score')\n",
    "plt.plot(range(2, 10), db_scores, label='Davies Bouldin Score')\n",
    "plt.plot(range(2, 10), ch_scores, label='Calinski Harabasz Score')\n",
    "\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('(Normalized) Scores')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a good number of clusters is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "\n",
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe[input_columns].to_numpy()\n",
    "\n",
    "clusters, centers = cop_kmeans(np_df, n_clusters, ml=must_link, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in dataframe['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = dataframe[dataframe['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    # get the key from the value\n",
    "    codes = [k for k, v in code_mapping.items() if v in codes]\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On joint with only TTHMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = completed_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thms_columns =list(targets_mapping.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tthms_df = grab_df[thms_columns].copy()\n",
    "tthms_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the thms columns\n",
    "df['TTHMs'] = tthms_df.sum(axis=1, min_count=len(thms_columns)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_columns = list(feature_mapping.values()) + ['TTHMs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_codes = df[\"Code\"].unique()\n",
    "\n",
    "code_mapping = {code: i for i, code in enumerate(house_codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Code'] = df['Code'].map(code_mapping)\n",
    "df['Code'] = df['Code'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = df.copy()\n",
    "dataframe = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "dataframe[joint_columns] = pd.DataFrame(scaler.fit_transform(dataframe[joint_columns]), columns=joint_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe[joint_columns].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(seed=1)\n",
    "\n",
    "variances = []\n",
    "sil_scores = []\n",
    "db_scores = []\n",
    "ch_scores = []\n",
    "\n",
    "total_points = dataframe.shape[0]\n",
    "\n",
    "variance = np.var(dataframe[joint_columns], axis=0).mean()\n",
    "variances.append(variance)\n",
    "\n",
    "for n_cluster in range(2, 10):\n",
    "    clusters, centers = cop_kmeans(np_df, n_cluster, ml=must_link)\n",
    "    \n",
    "    sil_score = silhouette_score(np_df, clusters)\n",
    "    db_score = davies_bouldin_score(np_df, clusters)\n",
    "    ch_score = calinski_harabasz_score(np_df, clusters)\n",
    "    \n",
    "    dataframe['Cluster'] = clusters\n",
    "    \n",
    "    # compute the variance of each cluster\n",
    "    variance = 0\n",
    "    for cluster in dataframe['Cluster'].unique():\n",
    "        cluster_df = dataframe[dataframe['Cluster'] == cluster].copy()\n",
    "        variance += np.var(cluster_df[joint_columns], axis=0).mean() * cluster_df.shape[0] / total_points\n",
    "    \n",
    "    # compute average variance for n_cluster\n",
    "    variance /= n_cluster    \n",
    "    \n",
    "    variances.append(variance)\n",
    "    sil_scores.append(sil_score)\n",
    "    db_scores.append(db_score)\n",
    "    ch_scores.append(ch_score)\n",
    "    \n",
    "    dataframe.drop(columns=['Cluster'], inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the scores\n",
    "variances = np.array(variances)\n",
    "sil_scores = np.array(sil_scores)\n",
    "db_scores = np.array(db_scores)\n",
    "ch_scores = np.array(ch_scores)\n",
    "\n",
    "variances = scaler.fit_transform(variances.reshape(-1, 1)).flatten()\n",
    "sil_scores = scaler.fit_transform(sil_scores.reshape(-1, 1)).flatten()\n",
    "db_scores = scaler.fit_transform(db_scores.reshape(-1, 1)).flatten()\n",
    "ch_scores = scaler.fit_transform(ch_scores.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "plt.plot(range(1, 10), variances, label='Weighted Average Variance')\n",
    "plt.plot(range(2, 10), sil_scores, label='Silhouette Score')\n",
    "plt.plot(range(2, 10), db_scores, label='Davies Bouldin Score')\n",
    "plt.plot(range(2, 10), ch_scores, label='Calinski Harabasz Score')\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('(Normalized) Scores')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a good number of clusters is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "n_clusters = 3\n",
    "\n",
    "must_link = []\n",
    "\n",
    "\n",
    "for code in dataframe['Code'].unique():\n",
    "    # get all the pair combinations of the entries with same code to add to the must_link\n",
    "    index_pairs = list(combinations(dataframe[dataframe['Code'] == code].index, 2)) \n",
    "    must_link.extend(index_pairs)\n",
    "\n",
    "np_df = dataframe[joint_columns].to_numpy()\n",
    "\n",
    "clusters, centers = cop_kmeans(np_df, n_clusters, ml=must_link, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in dataframe['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = dataframe[dataframe['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    # get the key from the value\n",
    "    codes = [k for k, v in code_mapping.items() if v in codes]\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap the code to the original code\n",
    "dataframe['Code'] = dataframe['Code'].map({v: k for k, v in code_mapping.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Code'] = dataframe['Code']\n",
    "df['Cluster'] = dataframe['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(os.path.join(clean_data_folder, 'modelling_grab.xlsx'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
