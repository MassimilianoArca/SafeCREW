{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"..\", \"utils\")\n",
    "\n",
    "data_folder = os.path.join(\"..\", \"..\", \"data\")\n",
    "clean_data_folder = os.path.join(data_folder, \"Clean Data\")\n",
    "metadata_folder = os.path.join(data_folder, \"Metadata\")\n",
    "plot_folder = os.path.join(data_folder, \"Plots\", \"Feltre\")\n",
    "\n",
    "sensor_folder = os.path.join(clean_data_folder, \"sensors\")\n",
    "\n",
    "feltre_sqlites_folder =  'feltre_sqlites'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feltre_df = pd.read_excel(os.path.join(clean_data_folder, 'feltre.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variables = {\n",
    "    'ICC [1/mL]': 'ICC (1/mL)',\n",
    "    'HNAC [1/mL]': 'HNAC (1/mL)', \n",
    "    'LNAC [1/mL]': 'LNAC (1/mL)',\n",
    "    'HNAP [%]': 'HNAP (%)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = {\n",
    "    'Pressione [atm]': 'Pressione (atm)',\n",
    "    'TOCeq [mg/l]': 'TOCeq (mg/l)',\n",
    "    'DOCeq [mg/l]': 'DOCeq (mg/l)',\n",
    "    'Turbidity [FTU]': 'Turbidity (FTU)', \n",
    "    'Conductivity [uS/cm]': 'Conductivity (uS/cm)',\n",
    "    'Temperature [째C]': 'Temperature (째C)',\n",
    "    'pH': 'pH',\n",
    "    'Free Chlorine [mg/l]': 'Free Chlorine (mg/l)',\n",
    "    'nitrati': 'nitrati',\n",
    "    'UV254': 'UV254',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feltre_df.rename(\n",
    "    columns=input_variables,\n",
    "    inplace=True\n",
    ")\n",
    "feltre_df.rename(\n",
    "    columns=target_variables,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "for target_variable in target_variables.values():\n",
    "    datasets[target_variable] = feltre_df[['DateTime', target_variable] + list(input_variables.values())].copy()\n",
    "    datasets[target_variable].set_index('DateTime', inplace=True)\n",
    "    datasets[target_variable].sort_index(inplace=True)\n",
    "    datasets[target_variable].dropna(inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# We are going to extend the features of the input variables for each target variable\n",
    "# -\n",
    "# We are going to add:\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "lags_in_hours = 3\n",
    "shifts_in_indexes = int(0.25 * 4 * lags_in_hours)\n",
    "rolling_window_in_hours = 6\n",
    "rolling_window = int(0.25 * 4 * rolling_window_in_hours)\n",
    "polyn_degree = 2\n",
    "\n",
    "lstm_datasets = {}\n",
    "\n",
    "for target_variable, df in datasets.items():\n",
    "    X, y = df[list(input_variables.values())].copy(), df[target_variable].copy()\n",
    "    \n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "    \n",
    "    # we are going to use the log1p of the target variable for the modelling to avoid instability\n",
    "    y = np.log1p(y)\n",
    "    \n",
    "    # need to change the name of target variable to avoid the / character\n",
    "    \n",
    "    target_variable = target_variable.replace(\"/\", \"_\")\n",
    "    \n",
    "    # do not use the extended features for the LSTM model\n",
    "    lstm_datasets[target_variable] = X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    print(f\"Target variable: {target_variable}\")\n",
    "    # print number of nan values in X\n",
    "    print(f\"Number of nan values in X: {X.isna().sum().sum()}\")\n",
    "    # print number of nan values in y\n",
    "    print(f\"Number of nan values in y: {y.isna().sum().sum()}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the LSTM model takes as input a tensor of shape (num_samples, time_steps, n_features)\n",
    "# we need to convert the pandas dataframe into a numpy array of shape (num_samples, time_steps, n_features)\n",
    "# each sample is a sequence of window_size time steps, containing the features and the target variable\n",
    "def create_sequences(X_df, y_df, window_size):\n",
    "    \"\"\"\n",
    "    Converts Pandas DataFrames into overlapping sequences for LSTM input.\n",
    "    \n",
    "    Returns:\n",
    "        X_seq: NumPy array of shape (num_samples - window_size, window_size, n_features)\n",
    "        y_seq: NumPy array of shape (num_samples - window_size, 1) with the last target value of each window\n",
    "        y_timestamps: List of timestamps corresponding to the predictions.\n",
    "    \"\"\"\n",
    "    timesteps = X_df.index\n",
    "    \n",
    "    X_values = X_df.to_numpy()\n",
    "    y_values = y_df.to_numpy()\n",
    "    \n",
    "    X_seq, y_seq, y_timestamps = [], [], []\n",
    "    \n",
    "    # Create sequences for X and corresponding y for only the last value of each window\n",
    "    for i in range(len(X_values) - window_size):\n",
    "        X_seq.append(X_values[i : i + window_size])  # Input sequence\n",
    "        y_seq.append(y_values[i + window_size - 1])  # Only the last value in the target window\n",
    "        y_timestamps.append(timesteps[i + window_size - 1])  # Timestamp for the last timestep\n",
    "        \n",
    "    return np.array(X_seq), np.array(y_seq), np.array(y_timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = lstm_datasets['HNAC (1_mL)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "X_train_seq, y_train_seq, timestamp_train = create_sequences(X_train, y_train, window_size)\n",
    "X_test_seq, y_test_seq, timestamp_test = create_sequences(X_test, y_test, window_size)\n",
    "\n",
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Bidirectional(LSTM(units=64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), seed=42)))\n",
    "bi_lstm_model.add(Dropout(0.3))\n",
    "bi_lstm_model.add(Bidirectional(LSTM(units=32)))\n",
    "bi_lstm_model.add(Dropout(0.3))\n",
    "bi_lstm_model.add(Dense(1))\n",
    "bi_lstm_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss=MeanSquaredError(),\n",
    "    metrics=[RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "bi_gru_model = Sequential()\n",
    "bi_gru_model.add(Bidirectional(GRU(units=64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), seed=42)))\n",
    "bi_gru_model.add(Dropout(0.3))\n",
    "bi_gru_model.add(Bidirectional(GRU(units=32)))\n",
    "bi_gru_model.add(Dropout(0.3))\n",
    "bi_gru_model.add(Dense(1))\n",
    "bi_gru_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    loss=MeanSquaredError(),\n",
    "    metrics=[RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "models = [bi_lstm_model, bi_gru_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq.shape, y_train_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Ensure that the validation data is provided in the correct format\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    _ = model.fit(X_train_seq, y_train_seq, epochs=50, callbacks=[early_stopping], validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "\n",
    "# Warm-up the model\n",
    "    warm_up_pred = model.predict(X_train_seq[-window_size - 1:])\n",
    "    warm_up_pred = np.squeeze(warm_up_pred)\n",
    "\n",
    "    y_pred = model.predict(X_test_seq)\n",
    "    y_pred = np.squeeze(y_pred)\n",
    "\n",
    "    # concatenate the warm-up predictions with the test predictions\n",
    "    y_pred = np.concatenate([warm_up_pred, y_pred])\n",
    "    \n",
    "    \n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timestamp_train,\n",
    "        y=np.expm1(y_train), \n",
    "        mode='lines',\n",
    "        name='True',\n",
    "        line=dict(color='blue'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timestamp_test,\n",
    "        y=np.expm1(y_test),\n",
    "        mode='lines',\n",
    "        name='True',\n",
    "        line=dict(color='blue'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timestamp_test,\n",
    "        y=np.expm1(y_pred),\n",
    "        mode='lines',\n",
    "        name=f'{model.name}',\n",
    "    ))\n",
    "\n",
    "target_variable_name = f\"{target_variable.replace('_', '/')}\"\n",
    "\n",
    "# fig.update_yaxes(type=\"log\")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=target_variable_name,\n",
    "    margin=dict(l=0, r=10, t=30, b=0),\n",
    "    font=dict(\n",
    "        size=14,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# put the legend at the top\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    yanchor=\"bottom\",\n",
    "    y=1.02,\n",
    "    xanchor=\"right\",\n",
    "    x=1\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter\n",
    "(\n",
    "    x=timestamp_train,\n",
    "    y=np.expm1(y_train), \n",
    "    mode='lines',\n",
    "    name='True',\n",
    "    line=dict(color='blue'),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter\n",
    "(\n",
    "    x=timestamp_test,\n",
    "    y=np.expm1(y_test),\n",
    "    mode='lines',\n",
    "    name='True',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter\n",
    "(\n",
    "    x=timestamp_test,\n",
    "    y=np.expm1(y_pred),\n",
    "    mode='lines',\n",
    "    name='Predicted',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "target_variable_name = f\"{target_variable.replace('_', '/')}\"\n",
    "\n",
    "# fig.update_yaxes(type=\"log\")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': f\"{target_variable_name}\",\n",
    "        'y':0.98,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'\n",
    "    },\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=target_variable_name,\n",
    "    margin=dict(l=0, r=10, t=30, b=0),\n",
    "    font=dict(\n",
    "        size=14,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# put the legend at the top\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    yanchor=\"bottom\",\n",
    "    y=1.02,\n",
    "    xanchor=\"right\",\n",
    "    x=1\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_seq.shape, y_train_seq.shape)\n",
    "print(X_test_seq.shape, y_test_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first window_size samples of y_test\n",
    "y_test = y_test[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super(WeightsLogger, self).__init__()\n",
    "        self.file_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        with self.file_writer.as_default():\n",
    "            for layer in self.model.layers:\n",
    "                if len(layer.weights) > 0:  # Check if the layer has weights\n",
    "                    for i, weight in enumerate(layer.weights):\n",
    "                        tf.summary.histogram(f\"{layer.name}/weight_{i}\", weight, step=epoch)\n",
    "        self.file_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientsLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super(GradientsLogger, self).__init__()\n",
    "        self.file_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        X_train_seq, y_train_seq = self.model._training_data  # Assuming training data is accessible\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(X_train_seq, training=True)\n",
    "            loss = self.model.compiled_loss(y_train_seq, predictions)  # Compute loss\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)  # Compute gradients\n",
    "\n",
    "        with self.file_writer.as_default():\n",
    "            for i, grad in enumerate(gradients):\n",
    "                if grad is not None:  # Some layers might not have gradients\n",
    "                    tf.summary.histogram(f\"gradients/var_{i}\", grad, step=epoch)\n",
    "        \n",
    "        self.file_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Input(shape=(window_size, X.shape[-1])))\n",
    "bi_lstm_model.add(LSTM(units=n_units_1, return_sequences=True, seed=42))\n",
    "bi_lstm_model.add(Dropout(dropout_1))\n",
    "bi_lstm_model.add(LSTM(units=n_units_2, return_sequences=True, seed=42))\n",
    "bi_lstm_model.add(Dropout(dropout_2))\n",
    "bi_lstm_model.add(LSTM(units=n_units_3, seed=42))\n",
    "bi_lstm_model.add(Dropout(dropout_3))\n",
    "bi_lstm_model.add(Dense(n_neurons))\n",
    "bi_lstm_model.add(Dense(1))\n",
    "bi_lstm_model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss=MeanSquaredError(),\n",
    "    metrics=[RootMeanSquaredError()],\n",
    ")\n",
    "\n",
    "bi_lstm_model.summary()\n",
    "\n",
    "\n",
    "log_dir = \"lstm_logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='root_mean_squared_error', patience=40, restore_best_weights=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "weights_logger = WeightsLogger(log_dir)\n",
    "gradient_logger = GradientsLogger(log_dir)\n",
    "\n",
    "bi_lstm_model._training_data = (X_train_seq, y_train_seq)\n",
    "\n",
    "history = bi_lstm_model.fit(\n",
    "    X_train_seq,\n",
    "    y_train_seq,\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        early_stopping,\n",
    "        tensorboard_callback,\n",
    "        weights_logger,\n",
    "        gradient_logger\n",
    "    ], \n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bi_lstm_model.predict(X_test_seq)\n",
    "y_pred = np.squeeze(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir lstm_logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X_df, y_df, window_size):\n",
    "    \"\"\"\n",
    "    Converts Pandas DataFrames into overlapping sequences for LSTM input.\n",
    "    \n",
    "    Returns:\n",
    "        X_seq: NumPy array of shape (num_samples - window_size, window_size, n_features)\n",
    "        y_seq: NumPy array of shape (num_samples - window_size, 1) with the last target value of each window\n",
    "        y_timestamps: List of timestamps corresponding to the predictions.\n",
    "    \"\"\"\n",
    "    timesteps = X_df.index\n",
    "    \n",
    "    X_values = X_df.to_numpy()\n",
    "    y_values = y_df.to_numpy()\n",
    "    \n",
    "    X_seq, y_seq, y_timestamps = [], [], []\n",
    "    \n",
    "    # Create sequences for X and corresponding y for only the last value of each window\n",
    "    for i in range(len(X_values) - window_size):\n",
    "        X_seq.append(X_values[i : i + window_size])  # Input sequence\n",
    "        y_seq.append(y_values[i + window_size - 1])  # Only the last value in the target window\n",
    "        y_timestamps.append(timesteps[i + window_size - 1])  # Timestamp for the last timestep\n",
    "        \n",
    "    return np.array(X_seq), np.array(y_seq), np.array(y_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part_df = pd.read_excel(os.path.join(clean_data_folder, 'Feltre', 'second_part.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variables = {\n",
    "    'ICC [1/mL]': 'ICC (1/mL)',\n",
    "    'HNAC [1/mL]': 'HNAC (1/mL)', \n",
    "    'LNAC [1/mL]': 'LNAC (1/mL)',\n",
    "    'HNAP [%]': 'HNAP (%)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = {\n",
    "    'Pressione [atm]': 'Pressione (atm)',\n",
    "    'TOCeq [mg/l]': 'TOCeq (mg/l)',\n",
    "    'DOCeq [mg/l]': 'DOCeq (mg/l)',\n",
    "    'Turbidity [FTU]': 'Turbidity (FTU)', \n",
    "    'Conductivity [uS/cm]': 'Conductivity (uS/cm)',\n",
    "    'Temperature [째C]': 'Temperature (째C)',\n",
    "    'pH': 'pH',\n",
    "    'Free Chlorine [mg/l]': 'Free Chlorine (mg/l)',\n",
    "    'Nitrate [mg/l]': 'Nitrate (mg/l)',\n",
    "    'UV254 [1/m]': 'UV254 (1/m)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part_df.rename(\n",
    "    columns=input_variables,\n",
    "    inplace=True\n",
    ")\n",
    "second_part_df.rename(\n",
    "    columns=target_variables,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "for target_variable in target_variables.values():\n",
    "    datasets[target_variable] = second_part_df[['DateTime', target_variable] + list(input_variables.values())].copy()\n",
    "    datasets[target_variable].set_index('DateTime', inplace=True)\n",
    "    datasets[target_variable].sort_index(inplace=True)\n",
    "    datasets[target_variable].dropna(inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# We are going to extend the features of the input variables for each target variable\n",
    "# -\n",
    "# We are going to add:\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "lags_in_hours = 3\n",
    "shifts_in_indexes = int(0.25 * 4 * lags_in_hours)\n",
    "rolling_window_in_hours = 6\n",
    "rolling_window = int(0.25 * 4 * rolling_window_in_hours)\n",
    "polyn_degree = 2\n",
    "\n",
    "ds = datasets.copy()\n",
    "lstm_datasets = {}\n",
    "\n",
    "for target_variable, df in datasets.items():\n",
    "    ds[target_variable] = df[list(input_variables.values())].copy(), df[target_variable].copy()\n",
    "    \n",
    "    X = ds[target_variable][0]\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "    \n",
    "    # uncomment based on the dataset you want to use\n",
    "    # X_extended = extend_features(X, lags_in_hours, rolling_window, polyn_degree)\n",
    "    X_extended = X\n",
    "    \n",
    "    y = ds[target_variable][1]\n",
    "    \n",
    "    # we are going to use the log1p of the target variable for the modelling to avoid instability\n",
    "    y = np.log1p(y)\n",
    "    \n",
    "    # need to change the name of target variable to avoid the / character\n",
    "    ds.pop(target_variable)\n",
    "    \n",
    "    target_variable = target_variable.replace(\"/\", \"_\")\n",
    "    \n",
    "    ds[target_variable] = X_extended, y\n",
    "    \n",
    "    # do not use the extended features for the LSTM model\n",
    "    lstm_datasets[target_variable] = X, y\n",
    "    \n",
    "datasets = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in datasets.items():\n",
    "    print(f\"Target variable: {target_variable}\")\n",
    "    # print number of nan values in X\n",
    "    print(f\"Number of nan values in X: {X.isna().sum().sum()}\")\n",
    "    # print number of nan values in y\n",
    "    print(f\"Number of nan values in y: {y.isna().sum().sum()}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'LSTM' : {},\n",
    "    'XGBoost': {},\n",
    "    'LGBM': {},\n",
    "    # 'QRNN': {},\n",
    "    # 'GRU': {},\n",
    "    # 'BI_LSTM': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    # modify based on the target variable to plot\n",
    "    if target_variable != 'ICC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    # ==== LSTM ====\n",
    "    \n",
    "    predictions['LSTM'][target_variable] = {}\n",
    "    \n",
    "    window_size = 12\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, timesteps_train = create_sequences(X_train, y_train, window_size)\n",
    "    X_test_seq, y_test_seq, timesteps_test = create_sequences(X_test, y_test, window_size)\n",
    "    \n",
    "\n",
    "    n_units_1 = 16\n",
    "    n_neurons = 16\n",
    "    learning_rate = 0.0001\n",
    "    dropout_1 = 0.1\n",
    "    batch_size = 12\n",
    "    # n_units_2 = lstm_studies[target_variable].best_trial.params[\"n_units_2\"]\n",
    "    # dropout_2 = lstm_studies[target_variable].best_trial.params[\"dropout_2\"]\n",
    "    \n",
    "    # fit the model 50 times to get a better estimate of the predictions and the uncertainty\n",
    "    n_iterations = 1\n",
    "    \n",
    "    y_pred_list = []\n",
    "    \n",
    "    n_units_1 = 16\n",
    "    n_neurons = 16\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(window_size, X_train_seq.shape[-1])))\n",
    "        model.add(LSTM(units=n_units_1, return_sequences=False, seed=seed))\n",
    "        # model.add(Dropout(dropout_1, seed=seed))\n",
    "        # model.add(LSTM(units=n_units_2, return_sequences=False, seed=seed))\n",
    "        # model.add(Dropout(dropout_2, seed=seed))\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate, epsilon=0.01),\n",
    "            loss=MeanSquaredError(),\n",
    "            metrics=[RootMeanSquaredError()],\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(X_train_seq, y_train_seq, epochs=500, callbacks=[early_stopping], verbose=0, batch_size=batch_size, validation_split=0.3)\n",
    "        \n",
    "        # Warm-up the model\n",
    "        warm_up_pred = model.predict(X_train_seq[-window_size - 1:])\n",
    "        warm_up_pred = np.squeeze(warm_up_pred)\n",
    "        \n",
    "        y_pred = model.predict(X_test_seq)\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "\n",
    "        # concatenate the warm-up predictions with the test predictions\n",
    "        y_pred = np.concatenate([warm_up_pred, y_pred])\n",
    "        \n",
    "        y_pred_list.append(y_pred)\n",
    "    \n",
    "    # get a timesteps_test as a one-dimensional array with no duplicates\n",
    "    timesteps_test = np.unique(timesteps_test)\n",
    "    timesteps_train = np.unique(timesteps_train)\n",
    "\n",
    "    predictions['LSTM'][target_variable][\"timesteps_test\"] = timesteps_test\n",
    "    predictions['LSTM'][target_variable][\"timesteps_train\"] = timesteps_train\n",
    "    predictions['LSTM'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['LSTM'][target_variable][\"y_train\"] = y_train\n",
    "    \n",
    "    mean_pred = np.mean(y_pred_list, axis=0)\n",
    "    std_pred = np.std(y_pred_list, axis=0)\n",
    "    \n",
    "    predictions['LSTM'][target_variable][\"mean_pred\"] = mean_pred\n",
    "    predictions['LSTM'][target_variable][\"std_pred\"] = std_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Add training and validation loss traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=history.history['loss'][5:],\n",
    "        name='Training Loss',\n",
    "        line=dict(color='blue')\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=history.history['val_loss'][5:],\n",
    "        name='Validation Loss',\n",
    "        line=dict(color='red')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Model Loss During Training',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Add training and validation loss traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=history.history['root_mean_squared_error'],\n",
    "        name='Training Loss',\n",
    "        line=dict(color='blue')\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=history.history['val_root_mean_squared_error'],\n",
    "        name='Validation Loss',\n",
    "        line=dict(color='red')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='RMSE Training and Validation',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='RMSE',\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM PLOTS\n",
    "\n",
    "for target_variable in lstm_datasets.keys():\n",
    "    \n",
    "    # modify based on the target variable to plot\n",
    "    if target_variable != 'ICC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    timesteps_test = predictions['LSTM'][target_variable][\"timesteps_test\"]\n",
    "    timesteps_train = predictions['LSTM'][target_variable][\"timesteps_train\"]\n",
    "    y_train = predictions['LSTM'][target_variable][\"y_train\"]\n",
    "    y_test = predictions['LSTM'][target_variable][\"y_test\"]\n",
    "    \n",
    "    \n",
    "    y_pred_lstm = predictions['LSTM'][target_variable][\"mean_pred\"]\n",
    "    std_pred_lstm = predictions['LSTM'][target_variable][\"std_pred\"]    \n",
    "    \n",
    "    # y_pred_gru = predictions['GRU'][target_variable][\"mean_pred\"]\n",
    "    # std_pred_gru = predictions['GRU'][target_variable][\"std_pred\"]\n",
    "    \n",
    "    # y_pred_bi_lstm = predictions['BI_LSTM'][target_variable][\"mean_pred\"]\n",
    "    # std_pred_bi_lstm = predictions['BI_LSTM'][target_variable][\"std_pred\"]\n",
    "    \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_train,\n",
    "        y=np.expm1(y_train), \n",
    "        mode='lines',\n",
    "        name='True',\n",
    "        line=dict(color='blue'),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_test),\n",
    "        mode='lines',\n",
    "        name='True',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm),\n",
    "        mode='lines',\n",
    "        name='LSTM',\n",
    "        line=dict(color='red')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        name='Upper Bound',\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm + 1.96 * std_pred_lstm),\n",
    "        mode='lines',\n",
    "        line=dict(width=0),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        name='Lower Bound',\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm - 1.96 * std_pred_lstm),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        fillcolor='rgba(255, 102, 102, 0.3)',  # light red color\n",
    "        fill='tonexty',\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "    target_variable_name = f\"{target_variable.replace('_', '/')}\"\n",
    "    \n",
    "    \n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f\"{target_variable_name}\",\n",
    "            'y':0.98,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=target_variable_name,\n",
    "        margin=dict(l=0, r=10, t=30, b=0),\n",
    "        font=dict(\n",
    "            size=14,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # put the legend at the top\n",
    "    fig.update_layout(legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    ))\n",
    "    \n",
    "    fig.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
