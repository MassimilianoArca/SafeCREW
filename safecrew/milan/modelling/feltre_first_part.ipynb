{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"..\", \"utils\")\n",
    "\n",
    "data_folder = os.path.join(\"..\", \"..\", \"data\")\n",
    "clean_data_folder = os.path.join(data_folder, \"Clean Data\")\n",
    "metadata_folder = os.path.join(data_folder, \"Metadata\")\n",
    "plot_folder = os.path.join(data_folder, \"Plots\", \"Feltre\")\n",
    "\n",
    "sensor_folder = os.path.join(clean_data_folder, \"sensors\")\n",
    "\n",
    "feltre_sqlites_folder = 'feltre_sqlites_first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part_df = pd.read_excel(os.path.join(clean_data_folder, 'Feltre', 'first_part.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def extend_features(df: pd.DataFrame, lags: int, rolling_window: int, poly_degree: int):\n",
    "    \n",
    "    initial_features = df.columns\n",
    "    # add polynomial features\n",
    "    poly = PolynomialFeatures(degree=poly_degree)\n",
    "    df_poly = poly.fit_transform(df)\n",
    "    df = pd.DataFrame(df_poly, columns=poly.get_feature_names_out(df.columns))\n",
    "    \n",
    "    # add lagged, rolling and expanding features for each variable in df\n",
    "    for col in initial_features.difference([\"Year\", \"Month\"]):\n",
    "        for lag in range(1, lags + 1):\n",
    "            df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "            \n",
    "        df[f\"{col}_rolling{rolling_window}\"] = df[col].rolling(rolling_window).mean()\n",
    "        \n",
    "    # fill NaN values with bfill\n",
    "    df.bfill(inplace=True)\n",
    "    \n",
    "    df.drop(columns=['1'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variables = {\n",
    "    'ICC [1/mL]': 'ICC (1/mL)',\n",
    "    'HNAC [1/mL]': 'HNAC (1/mL)', \n",
    "    'LNAC [1/mL]': 'LNAC (1/mL)',\n",
    "    'HNAP [%]': 'HNAP (%)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = {\n",
    "    'Pressione [atm]': 'Pressione (atm)',\n",
    "    'TOCeq [mg/l]': 'TOCeq (mg/l)',\n",
    "    'DOCeq [mg/l]': 'DOCeq (mg/l)',\n",
    "    'Turbidity [FTU]': 'Turbidity (FTU)', \n",
    "    'Conductivity [uS/cm]': 'Conductivity (uS/cm)',\n",
    "    'Temperature [°C]': 'Temperature (°C)',\n",
    "    'pH': 'pH',\n",
    "    'Free Chlorine [mg/l]': 'Free Chlorine (mg/l)',\n",
    "    'Nitrate [mg/l]': 'Nitrate (mg/l)',\n",
    "    'UV254 [1/m]': 'UV254 (1/m)',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part_df.rename(\n",
    "    columns=input_variables,\n",
    "    inplace=True\n",
    ")\n",
    "first_part_df.rename(\n",
    "    columns=target_variables,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "for target_variable in target_variables.values():\n",
    "    datasets[target_variable] = first_part_df[['DateTime', target_variable] + list(input_variables.values())].copy()\n",
    "    datasets[target_variable].set_index('DateTime', inplace=True)\n",
    "    datasets[target_variable].sort_index(inplace=True)\n",
    "    datasets[target_variable].dropna(inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# We are going to extend the features of the input variables for each target variable\n",
    "# -\n",
    "# We are going to add:\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "lags_in_hours = 3\n",
    "shifts_in_indexes = int(0.25 * 4 * lags_in_hours)\n",
    "rolling_window_in_hours = 6\n",
    "rolling_window = int(0.25 * 4 * rolling_window_in_hours)\n",
    "polyn_degree = 2\n",
    "\n",
    "ds = datasets.copy()\n",
    "lstm_datasets = {}\n",
    "\n",
    "for target_variable, df in datasets.items():\n",
    "    ds[target_variable] = df[list(input_variables.values())].copy(), df[target_variable].copy()\n",
    "    \n",
    "    X = ds[target_variable][0]\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "    \n",
    "    # uncomment based on the dataset you want to use\n",
    "    # X_extended = extend_features(X, lags_in_hours, rolling_window, polyn_degree)\n",
    "    X_extended = X\n",
    "    \n",
    "    y = ds[target_variable][1]\n",
    "    \n",
    "    # we are going to use the log1p of the target variable for the modelling to avoid instability\n",
    "    y = np.log1p(y)\n",
    "    \n",
    "    # need to change the name of target variable to avoid the / character\n",
    "    ds.pop(target_variable)\n",
    "    \n",
    "    target_variable = target_variable.replace(\"/\", \"_\")\n",
    "    \n",
    "    ds[target_variable] = X_extended, y\n",
    "    \n",
    "    # do not use the extended features for the LSTM model\n",
    "    lstm_datasets[target_variable] = X, y\n",
    "    \n",
    "datasets = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in datasets.items():\n",
    "    print(f\"Target variable: {target_variable}\")\n",
    "    # print number of nan values in X\n",
    "    print(f\"Number of nan values in X: {X.isna().sum().sum()}\")\n",
    "    # print number of nan values in y\n",
    "    print(f\"Number of nan values in y: {y.isna().sum().sum()}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "We are going to train different models:\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- QRNN\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in datasets.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    datasets[target_variable] = X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X_train, X_test, y_train, y_test) in datasets.items():\n",
    "    print(f\"Target variable: {target_variable}\")\n",
    "    # print number of nan values in X\n",
    "    print(f\"Number of nan values in X: {X_train.isna().sum().sum()}\")\n",
    "    # print number of nan values in y\n",
    "    print(f\"Number of nan values in y: {y_train.isna().sum().sum()}\")\n",
    "    print(\"-\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=n_folds)\n",
    "\n",
    "for target_variable, (X_train, X_test, y_train, y_test) in datasets.items():\n",
    "    X_cv = X_train.copy()\n",
    "    y_cv = y_train.copy()\n",
    "\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        X_train_cv, X_test_cv = X_cv.iloc[train_index], X_cv.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y_cv.iloc[train_index], y_cv.iloc[test_index]\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        \n",
    "        y_pred_cv = model.predict(X_test_cv)\n",
    "        cv_rmse[i] = np.sqrt(mean_squared_error(y_test_cv, y_pred_cv)) \n",
    "\n",
    "\n",
    "    print(f\"Target variable: {target_variable}\")\n",
    "    print(f\"Mean CV RMSE: {np.mean(cv_rmse)}\")\n",
    "    \n",
    "    lr_results[target_variable] = {\n",
    "        \"mean_cv_rmse\": np.mean(cv_rmse),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_xgb_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = XGBRegressor(random_state=seed, **params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    if np.isnan(y_val).any() or np.isnan(y_val_pred).any():\n",
    "        print(f\"y_val: {y_val}\")\n",
    "        print(f\"y_val_pred: {y_val_pred}\")\n",
    "    return np.sqrt(mean_squared_error(y_val.values, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    eta = trial.suggest_float(\"eta\", 1e-5, 1, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1, log=True)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1, log=True)\n",
    "    learning_rate = trial.suggest_float(\n",
    "        \"learning_rate\", 1e-5, 1e-1, log=True\n",
    "    )\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 10, 500)\n",
    "    updater = trial.suggest_categorical(\n",
    "        \"updater\", [\"shotgun\", \"coord_descent\"]\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gblinear\",\n",
    "        \"eta\": eta,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"updater\": updater,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "\n",
    "    cv = TimeSeriesSplit(n_splits=n_folds)\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_xgb_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            params,\n",
    "        )\n",
    "\n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "\n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_studies = {}\n",
    "\n",
    "for target_variable, (X_train, _, y_train, _) in datasets.items():\n",
    "\n",
    "    if os.path.exists(f\"{feltre_sqlites_folder}/XGBoost - {target_variable}.sqlite3\"):\n",
    "            \n",
    "        study = optuna.load_study(\n",
    "        study_name=\"Hyperparameter Tuning - XGBoost - \" + target_variable,\n",
    "        storage=f\"sqlite:///{feltre_sqlites_folder}/XGBoost - {target_variable}.sqlite3\",\n",
    "        )\n",
    "             \n",
    "    else:\n",
    "            \n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=f\"sqlite:///{feltre_sqlites_folder}/XGBoost - {target_variable}.sqlite3\",\n",
    "            study_name=\"Hyperparameter Tuning - XGBoost - \" + target_variable,\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "            \n",
    "    xgb_studies[target_variable] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_lgbm_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=seed,\n",
    "        linear_tree=True,\n",
    "    )\n",
    "\n",
    "    if params is not None:\n",
    "        model.set_params(**params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val.values, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_estimators\": trial.suggest_int(\n",
    "            \"n_estimators\", 1, 20, step=1\n",
    "        ),\n",
    "        \"learning_rate\": trial.suggest_float(\n",
    "            \"learning_rate\", 1e-5, 1e-1, log=True\n",
    "        ),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 16, step=1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 20, step=1),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\n",
    "            \"min_data_in_leaf\", 2, 50, step=1\n",
    "        ),\n",
    "        \"lambda_l1\": trial.suggest_float(\n",
    "            \"lambda_l1\", 1e-3, 10, log=True\n",
    "        ),\n",
    "        \"lambda_l2\": trial.suggest_float(\n",
    "            \"lambda_l2\", 1e-3, 10, log=True\n",
    "        ),\n",
    "        \"min_split_gain\": trial.suggest_float(\n",
    "            \"min_split_gain\", 0, 15, step=0.5\n",
    "        ),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1),\n",
    "        \"bagging_fraction\": trial.suggest_float(\n",
    "            \"bagging_fraction\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"feature_fraction\": trial.suggest_float(\n",
    "            \"feature_fraction\", 1e-3, 1, log=True\n",
    "        ),\n",
    "        \"min_child_samples\": trial.suggest_int(\n",
    "            \"min_child_samples\", 20, 1000, log=True\n",
    "        ),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 10, 500, step=10),\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_lgbm_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_studies = {}\n",
    "\n",
    "for target_variable, (X_train, _, y_train, _) in datasets.items():\n",
    "    \n",
    "        if os.path.exists(f\"{feltre_sqlites_folder}/LGBM - {target_variable}.sqlite3\"):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=\"Hyperparameter Tuning - LGBM - \" + target_variable,\n",
    "            storage=f\"sqlite:///{feltre_sqlites_folder}/LGBM - {target_variable}.sqlite3\",\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=f\"sqlite:///{feltre_sqlites_folder}/LGBM - {target_variable}.sqlite3\",\n",
    "                study_name=\"Hyperparameter Tuning - LGBM - \" + target_variable,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        lgbm_studies[target_variable] = study  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantnn.qrnn import QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "def fit_and_validate_qrnn_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index].to_numpy(), X.iloc[val_index].to_numpy()\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    n_layers = params[\"n_layers\"]\n",
    "    n_units = params[\"n_units\"]\n",
    "    activation = params[\"activation\"]\n",
    "\n",
    "    model = QRNN(\n",
    "        n_inputs=X_tr.shape[1],\n",
    "        quantiles=quantiles,\n",
    "        model=(n_layers, n_units, activation),\n",
    "    )\n",
    "    \n",
    "    n_epochs = 50\n",
    "    optimizer = torch.optim.AdamW(model.model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "    \n",
    "    model.train(\n",
    "        training_data=(X_tr, np.array(y_tr)),\n",
    "        validation_data=(X_val, np.array(y_val)),\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        n_epochs=n_epochs,\n",
    "        device=\"cpu\",\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        logger=None,\n",
    "        \n",
    "        \n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model.predict(X_val).numpy()\n",
    "    \n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val.values, y_val_pred.mean(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\n",
    "    \"elu\",\n",
    "    \"hardshrink\",\n",
    "    \"hardtanh\",\n",
    "    \"prelu\",\n",
    "    \"relu\",\n",
    "    \"selu\",\n",
    "    \"celu\",\n",
    "    \"sigmoid\",\n",
    "    \"softplus\",\n",
    "    \"softmin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"n_layers\": trial.suggest_int(\"n_layers\", 1, 3),\n",
    "        \"n_units\": trial.suggest_int(\"n_units\", 32, 512, log=True),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", activations),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "    }\n",
    "\n",
    "    cv = TimeSeriesSplit(n_splits=n_folds)\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_qrnn_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrnn_studies = {}\n",
    "\n",
    "for target_variable, (X_train, _, y_train, _) in datasets.items():\n",
    "    if os.path.exists(f\"{feltre_sqlites_folder}/QRNN - {target_variable}.sqlite3\"):\n",
    "            \n",
    "        study = optuna.load_study(\n",
    "        study_name=\"Hyperparameter Tuning - QRNN - \" + target_variable,\n",
    "        storage=f\"sqlite:///{feltre_sqlites_folder}/QRNN - {target_variable}.sqlite3\",\n",
    "        )\n",
    "            \n",
    "    else:\n",
    "            \n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            storage=f\"sqlite:///{feltre_sqlites_folder}/QRNN - {target_variable}.sqlite3\",\n",
    "            study_name=\"Hyperparameter Tuning - QRNN - \" + target_variable,\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        \n",
    "        study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "        \n",
    "    qrnn_studies[target_variable] = study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the LSTM model takes as input a tensor of shape (num_samples, time_steps, n_features)\n",
    "# we need to convert the pandas dataframe into a numpy array of shape (num_samples, time_steps, n_features)\n",
    "# each sample is a sequence of window_size time steps, containing the features and the target variable\n",
    "def create_sequences(X_df, y_df, window_size):\n",
    "    \"\"\"\n",
    "    Converts Pandas DataFrames into overlapping sequences for LSTM input.\n",
    "    \n",
    "    Returns:\n",
    "        X_seq: NumPy array of shape (num_samples - window_size, window_size, n_features)\n",
    "        y_seq: NumPy array of shape (num_samples - window_size, 1) with the last target value of each window\n",
    "        y_timestamps: List of timestamps corresponding to the predictions.\n",
    "    \"\"\"\n",
    "    timesteps = X_df.index\n",
    "    \n",
    "    X_values = X_df.to_numpy()\n",
    "    y_values = y_df.to_numpy()\n",
    "    \n",
    "    X_seq, y_seq, y_timestamps = [], [], []\n",
    "    \n",
    "    # Create sequences for X and corresponding y for only the last value of each window\n",
    "    for i in range(len(X_values) - window_size):\n",
    "        X_seq.append(X_values[i : i + window_size])  # Input sequence\n",
    "        y_seq.append(y_values[i + window_size - 1])  # Only the last value in the target window\n",
    "        y_timestamps.append(timesteps[i + window_size - 1])  # Timestamp for the last timestep\n",
    "        \n",
    "    return np.array(X_seq), np.array(y_seq), np.array(y_timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_lstm_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X[train_index], X[val_index]\n",
    "    y_tr, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(params[\"window_size\"], X_tr.shape[-1])))\n",
    "    model.add(LSTM(units=params[\"n_units_1\"], return_sequences=False, seed=seed))\n",
    "    model.add(Dropout(params[\"dropout_1\"], seed=seed))\n",
    "    model.add(Dense(params[\"n_neurons\"]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params[\"learning_rate\"]),\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError()],\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "    \n",
    "    _ = model.fit(X_tr, y_tr, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0, batch_size=params[\"batch_size\"])\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.squeeze(y_val_pred)\n",
    "    \n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_units_1\": trial.suggest_categorical(\"n_units_1\", [20, 40, 60]),\n",
    "        # \"n_units_2\": trial.suggest_categorical(\"n_units_2\", [20, 40, 60]),\n",
    "        \"n_neurons\": trial.suggest_categorical(\"n_neurons\", [20, 40, 60]),        \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True),\n",
    "        \"window_size\": trial.suggest_int(\"window_size\", 1, 24, step=1),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "        \"dropout_1\": trial.suggest_float(\"dropout_1\", 0.1, 0.5),\n",
    "        # \"dropout_2\": trial.suggest_float(\"dropout_2\", 0.1, 0.5),\n",
    "    }\n",
    "    \n",
    "    window_size = config[\"window_size\"]\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(X_cv, y_cv, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, _ = create_sequences(X_train, y_train, window_size)\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_train_seq, y_train_seq)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_lstm_model(\n",
    "            X_train_seq,\n",
    "            y_train_seq,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_studies = {}\n",
    "\n",
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable == 'HNAC (1_mL)' or target_variable == 'ICC (1_mL)':\n",
    "    \n",
    "        if os.path.exists(f\"{feltre_sqlites_folder}/LSTM - {target_variable}.sqlite3\"):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=\"Hyperparameter Tuning - LSTM - \" + target_variable,\n",
    "            storage=f\"sqlite:///{feltre_sqlites_folder}/LSTM - {target_variable}.sqlite3\",\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=f\"sqlite:///{feltre_sqlites_folder}/LSTM - {target_variable}.sqlite3\",\n",
    "                study_name=\"Hyperparameter Tuning - LSTM - \" + target_variable,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            study.optimize(lambda trial: objective(trial, X.copy(), y.copy()), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        lstm_studies[target_variable] = study  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_gru_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X[train_index], X[val_index]\n",
    "    y_tr, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(params[\"window_size\"], X_tr.shape[-1])))\n",
    "    model.add(GRU(units=params[\"n_units_1\"], return_sequences=False, seed=seed))\n",
    "    model.add(Dropout(params[\"dropout_1\"], seed=seed))\n",
    "    model.add(Dense(params[\"n_neurons\"], seed=seed))\n",
    "    model.add(Dense(1, seed=seed))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params[\"learning_rate\"]),\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError()],\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "    \n",
    "    _ = model.fit(X_tr, y_tr, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0, batch_size=params[\"batch_size\"])\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.squeeze(y_val_pred)\n",
    "    \n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_units_1\": trial.suggest_categorical(\"n_units_1\", [20, 40, 60]),\n",
    "        # \"n_units_2\": trial.suggest_categorical(\"n_units_2\", [20, 40, 60]),\n",
    "        \"n_neurons\": trial.suggest_categorical(\"n_neurons\", [20, 40, 60]),        \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True),\n",
    "        \"window_size\": trial.suggest_int(\"window_size\", 1, 24, step=1),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "        \"dropout_1\": trial.suggest_float(\"dropout_1\", 0.1, 0.5),\n",
    "        # \"dropout_2\": trial.suggest_float(\"dropout_2\", 0.1, 0.5),\n",
    "    }\n",
    "    \n",
    "    window_size = config[\"window_size\"]\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(X_cv, y_cv, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, _ = create_sequences(X_train, y_train, window_size)\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_train_seq, y_train_seq)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_gru_model(\n",
    "            X_train_seq,\n",
    "            y_train_seq,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_studies = {}\n",
    "\n",
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable == 'HNAC (1_mL)':\n",
    "    \n",
    "        if os.path.exists(f\"{feltre_sqlites_folder}/GRU - {target_variable}.sqlite3\"):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=\"Hyperparameter Tuning - GRU - \" + target_variable,\n",
    "            storage=f\"sqlite:///{feltre_sqlites_folder}/GRU - {target_variable}.sqlite3\",\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=f\"sqlite:///{feltre_sqlites_folder}/GRU - {target_variable}.sqlite3\",\n",
    "                study_name=\"Hyperparameter Tuning - GRU - \" + target_variable,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            study.optimize(lambda trial: objective(trial, X.copy(), y.copy()), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        gru_studies[target_variable] = study  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_bi_lstm_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X[train_index], X[val_index]\n",
    "    y_tr, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(params[\"window_size\"], X_tr.shape[-1])))\n",
    "    model.add(Bidirectional(LSTM(units=params[\"n_units_1\"], return_sequences=False, seed=seed)))\n",
    "    model.add(Dropout(params[\"dropout_1\"], seed=seed))\n",
    "    # model.add(LSTM(units=params[\"n_units_2\"], seed=seed))\n",
    "    # model.add(Dropout(params[\"dropout_2\"], seed=seed))\n",
    "    model.add(Dense(params[\"n_neurons\"], seed=seed))\n",
    "    model.add(Dense(1), seed=seed)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params[\"learning_rate\"]),\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError()],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "    \n",
    "    _ = model.fit(X_tr, y_tr, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0, batch_size=params[\"batch_size\"])\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.squeeze(y_val_pred)\n",
    "    \n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_units_1\": trial.suggest_categorical(\"n_units_1\", [20, 40, 60]),\n",
    "        # \"n_units_2\": trial.suggest_categorical(\"n_units_2\", [20, 40, 60]),\n",
    "        \"n_neurons\": trial.suggest_categorical(\"n_neurons\", [20, 40, 60]),       \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True),\n",
    "        \"window_size\": trial.suggest_int(\"window_size\", 1, 24, step=1),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "        \"dropout_1\": trial.suggest_float(\"dropout_1\", 0.1, 0.5),\n",
    "        # \"dropout_2\": trial.suggest_float(\"dropout_2\", 0.1, 0.5),\n",
    "    }\n",
    "    \n",
    "    window_size = config[\"window_size\"]\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(X_cv, y_cv, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, _ = create_sequences(X_train, y_train, window_size)\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_train_seq, y_train_seq)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_bi_lstm_model(\n",
    "            X_train_seq,\n",
    "            y_train_seq,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_studies = {}\n",
    "\n",
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable == 'HNAC (1_mL)':\n",
    "    \n",
    "        if os.path.exists(f\"{feltre_sqlites_folder}/BI_LSTM - {target_variable}.sqlite3\"):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=\"Hyperparameter Tuning - BI_LSTM - \" + target_variable,\n",
    "            storage=f\"sqlite:///{feltre_sqlites_folder}/BI_LSTM - {target_variable}.sqlite3\",\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=f\"sqlite:///{feltre_sqlites_folder}/BI_LSTM - {target_variable}.sqlite3\",\n",
    "                study_name=\"Hyperparameter Tuning - BI_LSTM - \" + target_variable,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            study.optimize(lambda trial: objective(trial, X.copy(), y.copy()), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        bi_lstm_studies[target_variable] = study  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare studies results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the best study for each model and for each target variable\n",
    "best_studies = {}\n",
    "\n",
    "for target_variable in datasets.keys():\n",
    "    best_studies[target_variable] = {\n",
    "        \"LinearRegression\": lr_results[target_variable],\n",
    "        \"XGBoost\": xgb_studies[target_variable].best_trial,\n",
    "        \"LGBM\": lgbm_studies[target_variable].best_trial,\n",
    "        \"QRNN\": qrnn_studies[target_variable].best_trial,\n",
    "        \"LSTM\": lstm_studies[target_variable].best_trial if target_variable == 'HNAC (1_mL)' or target_variable == 'ICC (1_mL)' else None,\n",
    "        \"GRU\": gru_studies[target_variable].best_trial if target_variable == 'HNAC (1_mL)' else None,\n",
    "        \"BI_LSTM\": bi_lstm_studies[target_variable].best_trial if target_variable == 'HNAC (1_mL)' else None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(columns=list(best_studies.keys()))\n",
    "\n",
    "for target_variable, best_trials in best_studies.items():\n",
    "    for model, best_trial in best_trials.items():\n",
    "        if best_trial is not None:\n",
    "            if model == \"LinearRegression\":\n",
    "                comparison_df.loc[model, target_variable] = np.round(best_trial[\"mean_cv_rmse\"], 2)\n",
    "            else:\n",
    "                comparison_df.loc[model, target_variable] = np.round(best_trial.value, 2)\n",
    "        else:\n",
    "            comparison_df.loc[model, target_variable] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation error for each target transformed with log1p\n",
    "# LSTM outperforming the other models in all target variables\n",
    "# comparison_df.to_excel('comparison_df.xlsx')\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'LSTM' : {},\n",
    "    'XGBoost': {},\n",
    "    'LGBM': {},\n",
    "    'QRNN': {},\n",
    "    'GRU': {},\n",
    "    'BI_LSTM': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)' or target_variable == 'ICC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    # ==== LSTM ====\n",
    "    \n",
    "    predictions['LSTM'][target_variable] = {}\n",
    "    \n",
    "    window_size = lstm_studies[target_variable].best_trial.params[\"window_size\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, timesteps_train = create_sequences(X_train, y_train, window_size)\n",
    "    X_test_seq, y_test_seq, timesteps_test = create_sequences(X_test, y_test, window_size)\n",
    "    \n",
    "\n",
    "    n_units_1 = lstm_studies[target_variable].best_trial.params[\"n_units_1\"]\n",
    "    n_neurons = lstm_studies[target_variable].best_trial.params[\"n_neurons\"]\n",
    "    dropout_1 = lstm_studies[target_variable].best_trial.params[\"dropout_1\"]\n",
    "    learning_rate = lstm_studies[target_variable].best_trial.params[\"learning_rate\"]\n",
    "    batch_size = lstm_studies[target_variable].best_trial.params[\"batch_size\"] \n",
    "    \n",
    "    # fit the model 50 times to get a better estimate of the predictions and the uncertainty\n",
    "    n_iterations = 50\n",
    "    \n",
    "    y_pred_list = []\n",
    "\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(window_size, X_train_seq.shape[-1])))\n",
    "        model.add(LSTM(units=n_units_1, return_sequences=False, seed=42))\n",
    "        model.add(Dropout(dropout_1))\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=MeanSquaredError(),\n",
    "            metrics=[RootMeanSquaredError()],\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        _ = model.fit(X_train_seq, y_train_seq, epochs=50, callbacks=[early_stopping], verbose=0, batch_size=batch_size)\n",
    "        \n",
    "        # Warm-up the model\n",
    "        warm_up_pred = model.predict(X_train_seq[-window_size - 1:])\n",
    "        warm_up_pred = np.squeeze(warm_up_pred)\n",
    "        \n",
    "        y_pred = model.predict(X_test_seq)\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "\n",
    "        # concatenate the warm-up predictions with the test predictions\n",
    "        y_pred = np.concatenate([warm_up_pred, y_pred])\n",
    "        \n",
    "        y_pred_list.append(y_pred)\n",
    "    \n",
    "    # get a timesteps_test as a one-dimensional array with no duplicates\n",
    "    timesteps_test = np.unique(timesteps_test)\n",
    "    timesteps_train = np.unique(timesteps_train)\n",
    "\n",
    "    predictions['LSTM'][target_variable][\"timesteps_test\"] = timesteps_test\n",
    "    predictions['LSTM'][target_variable][\"timesteps_train\"] = timesteps_train\n",
    "    predictions['LSTM'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['LSTM'][target_variable][\"y_train\"] = y_train\n",
    "    \n",
    "    mean_pred = np.mean(y_pred_list, axis=0)\n",
    "    std_pred = np.std(y_pred_list, axis=0)\n",
    "    \n",
    "    predictions['LSTM'][target_variable][\"mean_pred\"] = mean_pred\n",
    "    predictions['LSTM'][target_variable][\"std_pred\"] = std_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    # ==== GRU ====\n",
    "    \n",
    "    predictions['GRU'][target_variable] = {}\n",
    "    \n",
    "    window_size = lstm_studies[target_variable].best_trial.params[\"window_size\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, timesteps_train = create_sequences(X_train, y_train, window_size)\n",
    "    X_test_seq, y_test_seq, timesteps_test = create_sequences(X_test, y_test, window_size)\n",
    "    \n",
    "\n",
    "    n_units_1 = gru_studies[target_variable].best_trial.params[\"n_units_1\"]\n",
    "    n_neurons = gru_studies[target_variable].best_trial.params[\"n_neurons\"]\n",
    "    dropout_1 = gru_studies[target_variable].best_trial.params[\"dropout_1\"]\n",
    "    learning_rate = gru_studies[target_variable].best_trial.params[\"learning_rate\"]\n",
    "    batch_size = gru_studies[target_variable].best_trial.params[\"batch_size\"] \n",
    "    \n",
    "    # fit the model 50 times to get a better estimate of the predictions and the uncertainty\n",
    "    n_iterations = 50\n",
    "    \n",
    "    y_pred_list = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(window_size, X_train_seq.shape[-1])))\n",
    "        model.add(GRU(units=n_units_1, return_sequences=False, seed=42))\n",
    "        model.add(Dropout(dropout_1))\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=MeanSquaredError(),\n",
    "            metrics=[RootMeanSquaredError()],\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        _ = model.fit(X_train_seq, y_train_seq, epochs=50, callbacks=[early_stopping], verbose=0, batch_size=batch_size)\n",
    "        \n",
    "        # Warm-up the model\n",
    "        warm_up_pred = model.predict(X_train_seq[-window_size - 1:])\n",
    "        warm_up_pred = np.squeeze(warm_up_pred)\n",
    "        \n",
    "        y_pred = model.predict(X_test_seq)\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "        \n",
    "        # concatenate the warm-up predictions with the test predictions\n",
    "        y_pred = np.concatenate([warm_up_pred, y_pred])\n",
    "        \n",
    "        y_pred_list.append(y_pred)\n",
    "    \n",
    "    # get a timesteps_test as a one-dimensional array with no duplicates\n",
    "    timesteps_test = np.unique(timesteps_test)\n",
    "    timesteps_train = np.unique(timesteps_train)\n",
    "\n",
    "    predictions['GRU'][target_variable][\"timesteps_test\"] = timesteps_test\n",
    "    predictions['GRU'][target_variable][\"timesteps_train\"] = timesteps_train\n",
    "    predictions['GRU'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['GRU'][target_variable][\"y_train\"] = y_train\n",
    "    \n",
    "    mean_pred = np.mean(y_pred_list, axis=0)\n",
    "    std_pred = np.std(y_pred_list, axis=0)\n",
    "    \n",
    "    predictions['GRU'][target_variable][\"mean_pred\"] = mean_pred\n",
    "    predictions['GRU'][target_variable][\"std_pred\"] = std_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    # ==== BIDIRECTIONAL LSTM ====\n",
    "    \n",
    "    predictions['BI_LSTM'][target_variable] = {}\n",
    "    \n",
    "    window_size = bi_lstm_studies[target_variable].best_trial.params[\"window_size\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, timesteps_train = create_sequences(X_train, y_train, window_size)\n",
    "    X_test_seq, y_test_seq, timesteps_test = create_sequences(X_test, y_test, window_size)\n",
    "    \n",
    "\n",
    "    n_units_1 = bi_lstm_studies[target_variable].best_trial.params[\"n_units_1\"]\n",
    "    n_neurons = bi_lstm_studies[target_variable].best_trial.params[\"n_neurons\"]\n",
    "    dropout_1 = bi_lstm_studies[target_variable].best_trial.params[\"dropout_1\"]\n",
    "    learning_rate = bi_lstm_studies[target_variable].best_trial.params[\"learning_rate\"]\n",
    "    batch_size = bi_lstm_studies[target_variable].best_trial.params[\"batch_size\"] \n",
    "    \n",
    "    # fit the model 50 times to get a better estimate of the predictions and the uncertainty\n",
    "    n_iterations = 50\n",
    "    \n",
    "    y_pred_list = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(window_size, X_train_seq.shape[-1])))\n",
    "        model.add(Bidirectional(LSTM(units=n_units_1, return_sequences=False, seed=42)))\n",
    "        model.add(Dropout(dropout_1))\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=MeanSquaredError(),\n",
    "            metrics=[RootMeanSquaredError()],\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        _ = model.fit(X_train_seq, y_train_seq, epochs=50, callbacks=[early_stopping], verbose=0, batch_size=batch_size)\n",
    "        \n",
    "        # Warm-up the model\n",
    "        warm_up_pred = model.predict(X_train_seq[-window_size - 1:])\n",
    "        warm_up_pred = np.squeeze(warm_up_pred)\n",
    "        \n",
    "        y_pred = model.predict(X_test_seq)\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "        \n",
    "        # concatenate the warm-up predictions with the test predictions\n",
    "        y_pred = np.concatenate([warm_up_pred, y_pred])\n",
    "        \n",
    "        y_pred_list.append(y_pred)\n",
    "    \n",
    "    # get a timesteps_test as a one-dimensional array with no duplicates\n",
    "    timesteps_test = np.unique(timesteps_test)\n",
    "    timesteps_train = np.unique(timesteps_train)\n",
    "\n",
    "    predictions['BI_LSTM'][target_variable][\"timesteps_test\"] = timesteps_test\n",
    "    predictions['BI_LSTM'][target_variable][\"timesteps_train\"] = timesteps_train\n",
    "    predictions['BI_LSTM'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['BI_LSTM'][target_variable][\"y_train\"] = y_train\n",
    "    \n",
    "    mean_pred = np.mean(y_pred_list, axis=0)\n",
    "    std_pred = np.std(y_pred_list, axis=0)\n",
    "    \n",
    "    predictions['BI_LSTM'][target_variable][\"mean_pred\"] = mean_pred\n",
    "    predictions['BI_LSTM'][target_variable][\"std_pred\"] = std_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM PLOTS\n",
    "\n",
    "for target_variable in lstm_datasets.keys():\n",
    "    \n",
    "    # modify based on the target variable to plot\n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    timesteps_test = predictions['LSTM'][target_variable][\"timesteps_test\"]\n",
    "    timesteps_train = predictions['LSTM'][target_variable][\"timesteps_train\"]\n",
    "    y_train = predictions['LSTM'][target_variable][\"y_train\"]\n",
    "    y_test = predictions['LSTM'][target_variable][\"y_test\"]\n",
    "    \n",
    "    \n",
    "    y_pred_lstm = predictions['LSTM'][target_variable][\"mean_pred\"]\n",
    "    std_pred_lstm = predictions['LSTM'][target_variable][\"std_pred\"]    \n",
    "    \n",
    "    # y_pred_gru = predictions['GRU'][target_variable][\"mean_pred\"]\n",
    "    # std_pred_gru = predictions['GRU'][target_variable][\"std_pred\"]\n",
    "    \n",
    "    # y_pred_bi_lstm = predictions['BI_LSTM'][target_variable][\"mean_pred\"]\n",
    "    # std_pred_bi_lstm = predictions['BI_LSTM'][target_variable][\"std_pred\"]\n",
    "    \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    # fig.add_trace(go.Scatter\n",
    "    # (\n",
    "    #     x=timesteps_train,\n",
    "    #     y=np.expm1(y_train), \n",
    "    #     mode='lines',\n",
    "    #     name='True',\n",
    "    #     line=dict(color='blue'),\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_test),\n",
    "        mode='lines',\n",
    "        name='True',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm),\n",
    "        mode='lines',\n",
    "        name='LSTM',\n",
    "        line=dict(color='red')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        name='Upper Bound',\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm + 1.96 * std_pred_lstm),\n",
    "        mode='lines',\n",
    "        line=dict(width=0),\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        name='Lower Bound',\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm - 1.96 * std_pred_lstm),\n",
    "        line=dict(width=0),\n",
    "        mode='lines',\n",
    "        fillcolor='rgba(255, 102, 102, 0.3)',  # light red color\n",
    "        fill='tonexty',\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter\n",
    "    # (\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_gru),\n",
    "    #     mode='lines',\n",
    "    #     name='GRU',\n",
    "    #     line=dict(color='green')\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Upper Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_gru + 1.96 * std_pred_gru),\n",
    "    #     mode='lines',\n",
    "    #     line=dict(width=0),\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Lower Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_gru - 1.96 * std_pred_gru),\n",
    "    #     line=dict(width=0),\n",
    "    #     mode='lines',\n",
    "    #     fillcolor='rgba(102, 255, 102, 0.3)',  # light green color\n",
    "    #     fill='tonexty',\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter\n",
    "    # (\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_bi_lstm),\n",
    "    #     mode='lines',\n",
    "    #     name='BI LSTM',\n",
    "    #     line=dict(color='orange')\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Upper Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_bi_lstm + 1.96 * std_pred_bi_lstm),\n",
    "    #     mode='lines',\n",
    "    #     line=dict(width=0),\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Lower Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_bi_lstm - 1.96 * std_pred_bi_lstm),\n",
    "    #     line=dict(width=0),\n",
    "    #     mode='lines',\n",
    "    #     fillcolor='rgba(255, 204, 102, 0.3)',  # light orange color\n",
    "    #     fill='tonexty',\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "\n",
    "    target_variable_name = f\"{target_variable.replace('_', '/')}\"\n",
    "    \n",
    "    # fig.update_yaxes(type=\"log\")\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f\"{target_variable_name}\",\n",
    "            'y':0.98,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=target_variable_name,\n",
    "        margin=dict(l=0, r=10, t=30, b=0),\n",
    "        font=dict(\n",
    "            size=14,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # put the legend at the top\n",
    "    fig.update_layout(legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    ))\n",
    "    \n",
    "    # fig.write_image(os.path.join(plot_folder, f\"LSTM - {target_variable}.png\"), scale=3)\n",
    "    \n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHER MODELS\n",
    "\n",
    "for target_variable, _ in lstm_datasets.items():\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = datasets[target_variable]\n",
    "    \n",
    "    # ==== XGBoost ====\n",
    "    \n",
    "    predictions['XGBoost'][target_variable] = {}\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gblinear\",\n",
    "        \"eta\": xgb_studies[target_variable].best_trial.params[\"eta\"],\n",
    "        \"reg_lambda\": xgb_studies[target_variable].best_trial.params[\"reg_lambda\"],\n",
    "        \"reg_alpha\": xgb_studies[target_variable].best_trial.params[\"reg_alpha\"],\n",
    "        \"learning_rate\": xgb_studies[target_variable].best_trial.params[\"learning_rate\"],\n",
    "        \"updater\": xgb_studies[target_variable].best_trial.params[\"updater\"],\n",
    "        \"n_estimators\": xgb_studies[target_variable].best_trial.params[\"n_estimators\"],\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(random_state=seed, **params)\n",
    "    \n",
    "    _ = model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions['XGBoost'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['XGBoost'][target_variable][\"y_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, _ in lstm_datasets.items():\n",
    "# ==== LGBM ====\n",
    "        \n",
    "    predictions['LGBM'][target_variable] = {}\n",
    "    \n",
    "    config = {\n",
    "        \"n_estimators\": lgbm_studies[target_variable].best_trial.params[\"n_estimators\"],\n",
    "        \"learning_rate\": lgbm_studies[target_variable].best_trial.params[\"learning_rate\"],\n",
    "        \"max_depth\": lgbm_studies[target_variable].best_trial.params[\"max_depth\"],\n",
    "        \"num_leaves\": lgbm_studies[target_variable].best_trial.params[\"num_leaves\"],\n",
    "        \"min_data_in_leaf\": lgbm_studies[target_variable].best_trial.params[\"min_data_in_leaf\"],\n",
    "        \"lambda_l1\": lgbm_studies[target_variable].best_trial.params[\"lambda_l1\"],\n",
    "        \"lambda_l2\": lgbm_studies[target_variable].best_trial.params[\"lambda_l2\"],\n",
    "        \"min_split_gain\": lgbm_studies[target_variable].best_trial.params[\"min_split_gain\"],\n",
    "        \"subsample\": lgbm_studies[target_variable].best_trial.params[\"subsample\"],\n",
    "        \"bagging_fraction\": lgbm_studies[target_variable].best_trial.params[\"bagging_fraction\"],\n",
    "        \"feature_fraction\": lgbm_studies[target_variable].best_trial.params[\"feature_fraction\"],\n",
    "        \"min_child_samples\": lgbm_studies[target_variable].best_trial.params[\"min_child_samples\"],\n",
    "        \"max_bin\": lgbm_studies[target_variable].best_trial.params[\"max_bin\"],\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=seed,\n",
    "        linear_tree=True,\n",
    "    )\n",
    "    \n",
    "    model.set_params(**config)\n",
    "    \n",
    "    _ = model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    predictions['LGBM'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['LGBM'][target_variable][\"y_pred\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, _ in lstm_datasets.items():\n",
    "# ==== QRNN ====\n",
    "    \n",
    "    predictions['QRNN'][target_variable] = {}\n",
    "    \n",
    "    config = {\n",
    "        \"n_layers\": qrnn_studies[target_variable].best_trial.params[\"n_layers\"],\n",
    "        \"n_units\": qrnn_studies[target_variable].best_trial.params[\"n_units\"],\n",
    "        \"activation\": qrnn_studies[target_variable].best_trial.params[\"activation\"],\n",
    "        \"batch_size\": qrnn_studies[target_variable].best_trial.params[\"batch_size\"],\n",
    "    }\n",
    "    \n",
    "    model = QRNN(\n",
    "        n_inputs=X_train.shape[1],\n",
    "        quantiles=[0.05, 0.5, 0.95],\n",
    "        model=(config[\"n_layers\"], config[\"n_units\"], config[\"activation\"]),\n",
    "    )\n",
    "    \n",
    "    n_epochs = 50\n",
    "    optimizer = torch.optim.AdamW(model.model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "    \n",
    "    model.train(\n",
    "        training_data=(X_train.to_numpy(), np.array(y_train)),\n",
    "        validation_data=(X_test.to_numpy(), np.array(y_test)),\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        n_epochs=n_epochs,\n",
    "        device=\"cpu\",\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        logger=None,\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = model.predict(X_test.to_numpy()).numpy()\n",
    "        \n",
    "        \n",
    "    predictions['QRNN'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['QRNN'][target_variable][\"y_pred_median\"] = y_pred[:, 1]\n",
    "    predictions['QRNN'][target_variable][\"y_pred_lower\"] = y_pred[:, 0]\n",
    "    predictions['QRNN'][target_variable][\"y_pred_upper\"] = y_pred[:, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL MODELS PLOTS\n",
    "\n",
    "# plot the predictions for each target variable\n",
    "for target_variable in datasets.keys():\n",
    "    \n",
    "    fig = go.Figure()\n",
    "        \n",
    "    fig.add_trace(go.Scatter(x=predictions['LGBM'][target_variable][\"y_test\"].index, y=predictions['LGBM'][target_variable][\"y_test\"], mode='lines', name='True'))\n",
    "        \n",
    "        \n",
    "    for model in predictions.keys():\n",
    "        \n",
    "        if model == 'QRNN':\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred_median\"], mode='lines', name='QRNN Predicted'))\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred_lower\"], mode='lines', name='Lower Bound'))\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred_upper\"], mode='lines', name='Upper Bound'))\n",
    "        \n",
    "        if model == 'LGBM':\n",
    "            fig.add_trace(go.Scatter\n",
    "                        (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred\"], mode='lines', name='LGBM Predicted'))\n",
    "            \n",
    "        if model == 'XGBoost':\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"y_test\"].index, y=predictions[model][target_variable][\"y_pred\"], mode='lines', name='XGBoost Predicted'))\n",
    "            \n",
    "        if model == 'LSTM':\n",
    "            fig.add_trace(go.Scatter\n",
    "                            (x=predictions[model][target_variable][\"timesteps_test\"], y=predictions[model][target_variable][\"y_pred\"], mode='lines', name='LSTM Predicted'))\n",
    "            \n",
    "    fig.update_layout(title=f'{target_variable}', xaxis_title='Date', yaxis_title=target_variable)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic SHAP\n",
    "\n",
    "In order for Shap package to work, these requirements need to be satisfied:\n",
    "* TensorFlow 1.14\n",
    "* Python 3.7\n",
    "* Protobuf 3.20\n",
    "* h5py 2.10\n",
    "\n",
    "TODO implement docker container with all the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    # ==== LSTM ====\n",
    "    \n",
    "    predictions['LSTM'][target_variable] = {}\n",
    "    \n",
    "    window_size = lstm_studies[target_variable].best_trial.params[\"window_size\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, timesteps_train = create_sequences(X_train, y_train, window_size)\n",
    "    X_test_seq, y_test_seq, timesteps_test = create_sequences(X_test, y_test, window_size)\n",
    "    \n",
    "\n",
    "    n_units_1 = lstm_studies[target_variable].best_trial.params[\"n_units_1\"]\n",
    "    n_neurons = lstm_studies[target_variable].best_trial.params[\"n_neurons\"]\n",
    "    dropout_1 = lstm_studies[target_variable].best_trial.params[\"dropout_1\"]\n",
    "    learning_rate = lstm_studies[target_variable].best_trial.params[\"learning_rate\"]\n",
    "    batch_size = lstm_studies[target_variable].best_trial.params[\"batch_size\"]\n",
    "    \n",
    "    feature_names = X_train.columns.tolist() \n",
    "    \n",
    "    # fit the model 50 times to get a better estimate of the predictions and the uncertainty\n",
    "    n_iterations = 1\n",
    "    \n",
    "    y_pred_list = []\n",
    "    \n",
    "    shap_values_list = []\n",
    "    \n",
    "    np.save(\"docker/X_train_seq.npy\", X_train_seq)\n",
    "    np.save(\"docker/y_train_seq.npy\", y_train_seq)\n",
    "    np.save(\"docker/X_test_seq.npy\", X_test_seq)\n",
    "    np.save(\"docker/feature_names.npy\", feature_names)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # call docker container with the model\n",
    "        cmd = f\"docker run --rm --platform linux/amd64 -v '{os.getcwd()}'/docker:/app -w /app shap_values_image:latest python shap_analysis.py\"\n",
    "        \n",
    "        # !{cmd}\n",
    "        \n",
    "        # load the shap values\n",
    "        shap_values = np.load(\"docker/shap_values.npy\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = np.load(\"docker/shap_values.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap_values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_per_timestep = pd.DataFrame(index=timesteps_test, columns=feature_names, dtype=float)\n",
    "\n",
    "for i, timestamp in enumerate(timesteps_test):\n",
    "    \n",
    "    # shap values for that timestamp\n",
    "    shap_values_i = shap_values[i]\n",
    "    # shap values for each feature\n",
    "    shap_values_i = shap_values_i.squeeze()\n",
    "    mean_shap = np.mean(np.abs(shap_values_i), axis=0)\n",
    "    shap_per_timestep.loc[timestamp] = mean_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_test),\n",
    "        mode='lines',\n",
    "        name='HNAC (1/mL)',\n",
    "        line=dict(color='blue')\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "for feature in feature_names:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=shap_per_timestep.index,\n",
    "            y=shap_per_timestep[feature],\n",
    "            mode='lines',\n",
    "            name=feature\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for feature in feature_names:\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=shap_per_timestep.index,\n",
    "            y=shap_per_timestep[feature],\n",
    "            mode='lines',\n",
    "            name=feature\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeshap.utils import calc_avg_event, calc_avg_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(X, y, window_size):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with sequence ids and time steps based on the window size. \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(X) - window_size):\n",
    "        # Create a sequence of window_size rows\n",
    "        seq = X.iloc[i : i + window_size].copy()\n",
    "        seq[\"seq_id\"] = i\n",
    "        seq[\"time_step\"] = range(window_size)\n",
    "        # Add the target variable for the last row of the sequence\n",
    "        seq[\"target\"] = y.iloc[i + window_size - 1]\n",
    "        # Append the sequence to the DataFrame\n",
    "        if df.empty:\n",
    "            df = seq\n",
    "        else:\n",
    "            df = pd.concat([df, seq], ignore_index=True)\n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_normalized = transform_data(X_train, y_train, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_event = calc_avg_event(data=d_train_normalized, numerical_feats=feature_names, categorical_feats=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sequence = calc_avg_sequence(data=d_train_normalized, numerical_feats=feature_names, categorical_feats=[], model_features=feature_names, entity_col='seq_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model such that it can be used with timeshap\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(None, X_train_seq.shape[-1]))\n",
    "lstm1 = tf.keras.layers.LSTM(units=n_units_1, return_sequences=False)(inputs)\n",
    "dropout1 = tf.keras.layers.Dropout(dropout_1)(lstm1)\n",
    "dense1 = tf.keras.layers.Dense(n_neurons)(dropout1)\n",
    "outputs = tf.keras.layers.Dense(1)(dense1)\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError()],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_seq, y_train_seq, epochs=50, callbacks=[early_stopping], verbose=0, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeshap.utils import get_avg_score_with_avg_event\n",
    "avg_score_over_len = get_avg_score_with_avg_event(model=f, med=avg_event, top=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_over_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test_normalized = transform_data(X_test, y_test, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeshap.explainer import global_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema = list(d_test_normalized.columns)\n",
    "pruning_dict = {'tol': [0.05, 0.075], 'path': 'outputs/prun_all_tf.csv'}\n",
    "event_dict = {'path': 'outputs/event_all_tf.csv', 'rs': 42, 'nsamples': 7000}\n",
    "feature_dict = {'path': 'outputs/feature_all_tf.csv', 'rs': 42, 'nsamples': 7000, 'feature_names': feature_names}\n",
    "prun_stats, global_plot = global_report(f=f, data=d_test_normalized, pruning_dict=pruning_dict, event_dict=event_dict, feature_dict=feature_dict, baseline=avg_event, model_features=feature_names, schema=schema, time_col='time_step', entity_col='seq_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prun_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeshap.explainer import prune_all, pruning_statistics, event_explain_all, feat_explain_all\n",
    "from timeshap.plot import plot_global_event, plot_global_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial On Second Part\n",
    "\n",
    "We try to use the model trained on the first part to predict the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part_df = pd.read_excel(os.path.join(clean_data_folder, 'Feltre', 'second_part.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part_df.rename(\n",
    "    columns=input_variables,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "second_part_df.rename(\n",
    "    columns=target_variables,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'HNAC (1/mL)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_second, y_second = second_part_df[input_variables.values()], second_part_df[target_variable]\n",
    "y_second = np.log1p(y_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_second_seq, y_second_seq, timesteps_second = create_sequences(X_second, y_second, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    # ==== LSTM ====\n",
    "    \n",
    "    predictions['LSTM'][target_variable] = {}\n",
    "    \n",
    "    window_size = lstm_studies[target_variable].best_trial.params[\"window_size\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=seed)\n",
    "    \n",
    "    X_train_seq, y_train_seq, timesteps_train = create_sequences(X_train, y_train, window_size)\n",
    "    \n",
    "\n",
    "    n_units_1 = lstm_studies[target_variable].best_trial.params[\"n_units_1\"]\n",
    "    n_neurons = lstm_studies[target_variable].best_trial.params[\"n_neurons\"]\n",
    "    dropout_1 = lstm_studies[target_variable].best_trial.params[\"dropout_1\"]\n",
    "    learning_rate = lstm_studies[target_variable].best_trial.params[\"learning_rate\"]\n",
    "    batch_size = lstm_studies[target_variable].best_trial.params[\"batch_size\"] \n",
    "    \n",
    "    # fit the model 50 times to get a better estimate of the predictions and the uncertainty\n",
    "    n_iterations = 50\n",
    "    \n",
    "    y_pred_list = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(window_size, X_train_seq.shape[-1])))\n",
    "        model.add(LSTM(units=n_units_1, return_sequences=False, seed=42))\n",
    "        model.add(Dropout(dropout_1))\n",
    "        model.add(Dense(n_neurons))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=MeanSquaredError(),\n",
    "            metrics=[RootMeanSquaredError()],\n",
    "        )\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        _ = model.fit(X_train_seq, y_train_seq, epochs=50, callbacks=[early_stopping], verbose=0, batch_size=batch_size)\n",
    "        \n",
    "        # Warm-up the model\n",
    "        warm_up_pred = model.predict(X_train_seq[-window_size - 1:])\n",
    "        warm_up_pred = np.squeeze(warm_up_pred)\n",
    "        \n",
    "        y_pred = model.predict(X_second_seq)\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "\n",
    "        # concatenate the warm-up predictions with the test predictions\n",
    "        y_pred = np.concatenate([warm_up_pred, y_pred])\n",
    "        \n",
    "        y_pred_list.append(y_pred)\n",
    "    \n",
    "    # get a timesteps_test as a one-dimensional array with no duplicates\n",
    "    timesteps_test = np.unique(timesteps_test)\n",
    "    timesteps_train = np.unique(timesteps_train)\n",
    "\n",
    "    predictions['LSTM'][target_variable][\"timesteps_test\"] = timesteps_second\n",
    "    predictions['LSTM'][target_variable][\"timesteps_train\"] = timesteps_train\n",
    "    predictions['LSTM'][target_variable][\"y_test\"] = y_test\n",
    "    predictions['LSTM'][target_variable][\"y_train\"] = y_train\n",
    "    \n",
    "    mean_pred = np.mean(y_pred_list, axis=0)\n",
    "    std_pred = np.std(y_pred_list, axis=0)\n",
    "    \n",
    "    predictions['LSTM'][target_variable][\"mean_pred\"] = mean_pred\n",
    "    predictions['LSTM'][target_variable][\"std_pred\"] = std_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM PLOTS\n",
    "\n",
    "for target_variable in lstm_datasets.keys():\n",
    "    \n",
    "    if target_variable != 'HNAC (1_mL)':\n",
    "        continue\n",
    "    \n",
    "    timesteps_test = predictions['LSTM'][target_variable][\"timesteps_test\"]\n",
    "    timesteps_train = predictions['LSTM'][target_variable][\"timesteps_train\"]\n",
    "    y_train = predictions['LSTM'][target_variable][\"y_train\"]\n",
    "    y_test = predictions['LSTM'][target_variable][\"y_test\"]\n",
    "    \n",
    "    \n",
    "    y_pred_lstm = predictions['LSTM'][target_variable][\"mean_pred\"]\n",
    "    std_pred_lstm = predictions['LSTM'][target_variable][\"std_pred\"]    \n",
    "    \n",
    "    \n",
    "    fig = go.Figure()\n",
    "    # fig.add_trace(go.Scatter\n",
    "    # (\n",
    "    #     x=timesteps_train,\n",
    "    #     y=np.expm1(y_train), \n",
    "    #     mode='lines',\n",
    "    #     name='True',\n",
    "    #     line=dict(color='blue'),\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_second),\n",
    "        mode='lines',\n",
    "        name='True',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter\n",
    "    (\n",
    "        x=timesteps_test,\n",
    "        y=np.expm1(y_pred_lstm),\n",
    "        mode='lines',\n",
    "        name='LSTM',\n",
    "        line=dict(color='red')\n",
    "    ))\n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Upper Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_lstm + 1.96 * std_pred_lstm),\n",
    "    #     mode='lines',\n",
    "    #     line=dict(width=0),\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # fig.add_trace(go.Scatter(\n",
    "    #     name='Lower Bound',\n",
    "    #     x=timesteps_test,\n",
    "    #     y=np.expm1(y_pred_lstm - 1.96 * std_pred_lstm),\n",
    "    #     line=dict(width=0),\n",
    "    #     mode='lines',\n",
    "    #     fillcolor='rgba(255, 102, 102, 0.3)',  # light red color\n",
    "    #     fill='tonexty',\n",
    "    #     showlegend=False\n",
    "    # ))\n",
    "    \n",
    "    target_variable_name = f\"{target_variable.replace('_', '/')}\"\n",
    "    \n",
    "    # fig.update_yaxes(type=\"log\")\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f\"{target_variable_name}\",\n",
    "            'y':0.98,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'\n",
    "        },\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=target_variable_name,\n",
    "        margin=dict(l=0, r=10, t=30, b=0),\n",
    "        font=dict(\n",
    "            size=14,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # put the legend at the top\n",
    "    fig.update_layout(legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    ))\n",
    "    \n",
    "    # fig.write_image(f\"LSTM - {target_variable}.png\", scale=3)\n",
    "    \n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model cannot be used for the second part, there are two different distributions. A new model should be trained on the second part of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
