{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e26417",
   "metadata": {},
   "source": [
    "Modelling of supply points for soft-sensor for NUWEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6337c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78102c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"..\", \"utils\")\n",
    "\n",
    "data_folder = os.path.join(\"..\", \"..\", \"data\")\n",
    "clean_data_folder = os.path.join(data_folder, \"Clean Data\")\n",
    "metadata_folder = os.path.join(data_folder, \"Metadata\")\n",
    "plot_folder = os.path.join(data_folder, \"Plots\")\n",
    "\n",
    "sensor_folder = os.path.join(clean_data_folder, \"sensors\")\n",
    "\n",
    "nuwee_figures_folder = os.path.join(plot_folder, \"nuwee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee64441",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.read_excel(os.path.join(clean_data_folder, \"modelling_grab.xlsx\"))\n",
    "\n",
    "nuwee_site1_df = pd.read_excel(os.path.join(clean_data_folder, 'nuwee', 'Site1_tabular.xlsx'))\n",
    "nuwee_site2_df = pd.read_excel(os.path.join(clean_data_folder, 'nuwee', 'Site2_tabular.xlsx'))\n",
    "nuwee_site3_df = pd.read_excel(os.path.join(clean_data_folder, 'nuwee', 'Site3_tabular.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da37e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6124af",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['DateTime'] = pd.to_datetime(grab_df['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121719a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df['DateTime'] = pd.to_datetime(nuwee_site1_df['DateTime'])\n",
    "nuwee_site2_df['DateTime'] = pd.to_datetime(nuwee_site2_df['DateTime'])\n",
    "nuwee_site3_df['DateTime'] = pd.to_datetime(nuwee_site3_df['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = {}\n",
    "for cluster in grab_df['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = grab_df[grab_df['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    codes_dict[cluster] = codes\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = nuwee_site1_df.columns.difference(['DateTime', 'Sampling Point', 'TTHMs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = grab_df[['DateTime', 'Cluster', 'Code', 'TTHMs'] + common_columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53742f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = grab_df[grab_df['Cluster'] == 0].copy()\n",
    "cluster_1_df = grab_df[grab_df['Cluster'] == 1].copy()\n",
    "cluster_2_df = grab_df[grab_df['Cluster'] == 2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ed74f",
   "metadata": {},
   "source": [
    "# NUWEE Data preprocessing\n",
    "\n",
    "We are going to fill deal with missing values and imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52684440",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we nee to clean the nuwee data by removing the rows with all missing values, then impute the rest\n",
    "nuwee_site1_df.dropna(how='all', subset=common_columns, inplace=True)\n",
    "nuwee_site2_df.dropna(how='all', subset=common_columns, inplace=True)\n",
    "nuwee_site3_df.dropna(how='all', subset=common_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, we need to remove the rows with all missing values in the TTHMs column\n",
    "nuwee_site1_df.dropna(how='all', subset=['TTHMs'], inplace=True)\n",
    "nuwee_site2_df.dropna(how='all', subset=['TTHMs'], inplace=True)\n",
    "nuwee_site3_df.dropna(how='all', subset=['TTHMs'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df.reset_index(drop=True, inplace=True)\n",
    "nuwee_site2_df.reset_index(drop=True, inplace=True)\n",
    "nuwee_site3_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235b38b",
   "metadata": {},
   "source": [
    "## Plots for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_axis_sizes = {\n",
    "    'Free Chlorine (mg/L)': [-0.02, 0.18],\n",
    "    'pH': [6.4, 8.6],\n",
    "    'Temperature (°C)': [-2, 25],\n",
    "    'Conductivity (uS/cm)': [120, 720],\n",
    "    'Nitrate (mg/L)': [0, 38],\n",
    "    'TTHMs': [-1, 50],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in common_columns.to_list() + ['TTHMs']:\n",
    "    for index, site in enumerate([nuwee_site1_df, nuwee_site2_df, nuwee_site3_df]): \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        df = site[['DateTime', 'Sampling Point', column]].copy()\n",
    "        df.sort_values(by='DateTime', inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        for sp in sorted(df['Sampling Point'].unique()):\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df[df['Sampling Point'] == sp]['DateTime'],\n",
    "                    y=df[df['Sampling Point'] == sp][column],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'{sp}',\n",
    "                    marker=dict(\n",
    "                        size=12\n",
    "                    ),\n",
    "                    opacity=0.3\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # compute the mean of the column for each date\n",
    "        df['mean'] = df.groupby('DateTime')[column].transform('mean')\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['DateTime'],\n",
    "                y=df['mean'],\n",
    "                mode='lines+markers',\n",
    "                name='Mean',\n",
    "                marker=dict(\n",
    "                    size=12,\n",
    "                    color='black'\n",
    "                ),\n",
    "                line=dict(\n",
    "                    color='black'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.update_yaxes(\n",
    "            range=column_axis_sizes[column]\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title=column if column != 'TTHMs' else 'TTHMs (µg/L)',\n",
    "            legend_title='Sampling Point',\n",
    "            margin=dict(\n",
    "                l=30,\n",
    "                r=10,\n",
    "                b=20,\n",
    "                t=0,\n",
    "            ),\n",
    "            font=dict(\n",
    "                size=20,\n",
    "            ),\n",
    "            width=1000,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        column_name = column.replace('/', '_')\n",
    "        \n",
    "        fig.write_image(os.path.join(nuwee_figures_folder, f\"nuwee_site_{index + 1}_{column_name}.png\"), scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in  common_columns.to_list() + ['TTHMs']:\n",
    "    \n",
    "    df = grab_df[['DateTime', 'Code', column]].copy()\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for code in df['Code'].unique(): \n",
    "        \n",
    "        df_code = df[df['Code'] == code].copy()\n",
    "        df_code.sort_values(by='DateTime', inplace=True)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_code['DateTime'],\n",
    "                y=df_code[column],\n",
    "                mode='lines+markers',\n",
    "                marker=dict(\n",
    "                    size=12\n",
    "                ),\n",
    "                name=code\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    fig.update_layout(\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title=column if column != 'TTHMs' else 'TTHMs (µg/L)',\n",
    "        legend_title='Supply Point',\n",
    "        margin=dict(\n",
    "            l=30,\n",
    "            r=5,\n",
    "            b=20,\n",
    "            t=50,\n",
    "        ),\n",
    "        font=dict(\n",
    "            size=20,\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        plot_bgcolor='rgba(144, 238, 144, 0.2)',\n",
    "        # paper_bgcolor='',\n",
    "        \n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "            range=column_axis_sizes[column]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    fig.write_image(\n",
    "        os.path.join( nuwee_figures_folder, f\"milan_{column.replace('/', '_')}.png\")\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all sites together\n",
    "for column in common_columns.to_list() + ['TTHMs']:\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    \n",
    "    milan_cluster_0_df = grab_df[grab_df['Cluster'] == 0][['DateTime', 'Code', column]].copy()\n",
    "    milan_cluster_0_df.sort_values(by='DateTime', inplace=True)\n",
    "    milan_cluster_0_df.dropna(inplace=True)\n",
    "    \n",
    "    milan_cluster_0_df['mean'] = milan_cluster_0_df.groupby('DateTime')[column].transform('mean')\n",
    "    \n",
    "    milan_cluster_1_df = grab_df[grab_df['Cluster'] == 1][['DateTime', 'Code', column]].copy()\n",
    "    milan_cluster_1_df.sort_values(by='DateTime', inplace=True)\n",
    "    milan_cluster_1_df.dropna(inplace=True)\n",
    "    \n",
    "    milan_cluster_1_df['mean'] = milan_cluster_1_df.groupby('DateTime')[column].transform('mean')\n",
    "    \n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=milan_cluster_1_df['DateTime'],\n",
    "            y=milan_cluster_1_df['mean'],\n",
    "            mode='lines+markers',\n",
    "            name=f'Mean Milan Cluster 1',\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=milan_cluster_0_df['DateTime'],\n",
    "            y=milan_cluster_0_df['mean'],\n",
    "            mode='lines+markers',\n",
    "            name=f'Mean Milan Cluster 2',\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    for index, site in enumerate([nuwee_site1_df, nuwee_site2_df, nuwee_site3_df]): \n",
    "        \n",
    "        df = site[['DateTime', 'Sampling Point', column]].copy()\n",
    "        df.sort_values(by='DateTime', inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        # compute the mean of the column for each date\n",
    "        df['mean'] = df.groupby('DateTime')[column].transform('mean')\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['DateTime'],\n",
    "                y=df['mean'],\n",
    "                mode='lines+markers',\n",
    "                name=f'Mean NUWEE Site {index + 1}',\n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.update_yaxes(\n",
    "            range=column_axis_sizes[column]\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title=column if column != 'TTHMs' else 'TTHMs (µg/L)',\n",
    "            legend_title='Sampling Point',\n",
    "            margin=dict(\n",
    "                l=30,\n",
    "                r=10,\n",
    "                b=20,\n",
    "                t=0,\n",
    "            ),\n",
    "            font=dict(\n",
    "                size=20,\n",
    "            ),\n",
    "            width=1000,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        column_name = column.replace('/', '_')\n",
    "        \n",
    "    fig.write_image(os.path.join(nuwee_figures_folder, f\"{column_name}.png\"), scale=3)\n",
    "        \n",
    "    # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8840244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can impute the missing values in the common columns\n",
    "import miceforest as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f080b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a kernel for each site\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=nuwee_site1_df[common_columns],\n",
    "    variable_schema=common_columns.tolist(),\n",
    "    random_state=42,\n",
    "    mean_match_strategy='shap',\n",
    ")\n",
    "\n",
    "kernel.mice(5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df[common_columns] = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a kernel for each site\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=nuwee_site2_df[common_columns],\n",
    "    variable_schema=common_columns.tolist(),\n",
    "    random_state=42,\n",
    "    mean_match_strategy='shap',\n",
    ")\n",
    "\n",
    "kernel.mice(5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af10c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df[common_columns] = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879179e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a kernel for each site\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=nuwee_site3_df[common_columns],\n",
    "    variable_schema=common_columns.tolist(),\n",
    "    random_state=42,\n",
    "    mean_match_strategy='shap',\n",
    ")\n",
    "\n",
    "kernel.mice(5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df[common_columns] = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check\n",
    "nuwee_site1_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f55c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a170e0",
   "metadata": {},
   "source": [
    "# Clustering based on Mahalanobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and covariance for each cluster\n",
    "cluster_0_mean = cluster_0_df[common_columns].mean()\n",
    "cluster_0_cov = cluster_0_df[common_columns].cov()\n",
    "cluster_1_mean = cluster_1_df[common_columns].mean()\n",
    "cluster_1_cov = cluster_1_df[common_columns].cov()\n",
    "cluster_2_mean = cluster_2_df[common_columns].mean()\n",
    "cluster_2_cov = cluster_2_df[common_columns].cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_stats = {\n",
    "    0: {\n",
    "        'mean': cluster_0_mean,\n",
    "        'cov': cluster_0_cov\n",
    "    },\n",
    "    1: {\n",
    "        'mean': cluster_1_mean,\n",
    "        'cov': cluster_1_cov\n",
    "    },\n",
    "    2: {\n",
    "        'mean': cluster_2_mean,\n",
    "        'cov': cluster_2_cov\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "def assign_cluster(row, clusters_stats):\n",
    "    distances = {}\n",
    "    for cluster, stats in clusters_stats.items():\n",
    "        mean = stats['mean']\n",
    "        cov = stats['cov']\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "        distance = mahalanobis(row[common_columns], mean, inv_cov)\n",
    "        distances[cluster] = distance\n",
    "    return min(distances, key=distances.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b170b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuwee site 1\n",
    "\n",
    "nuwee_site1_df['Cluster'] = -1\n",
    "for i, row in nuwee_site1_df.iterrows():\n",
    "    cluster = assign_cluster(row, clusters_stats)\n",
    "    nuwee_site1_df.at[i, 'Cluster'] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuwee site 2\n",
    "nuwee_site2_df['Cluster'] = -1\n",
    "\n",
    "for i, row in nuwee_site2_df.iterrows():\n",
    "    cluster = assign_cluster(row, clusters_stats)\n",
    "    nuwee_site2_df.at[i, 'Cluster'] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764033cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuwee site 3\n",
    "nuwee_site3_df['Cluster'] = -1\n",
    "\n",
    "for i, row in nuwee_site3_df.iterrows():\n",
    "    cluster = assign_cluster(row, clusters_stats)\n",
    "    nuwee_site3_df.at[i, 'Cluster'] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206693d",
   "metadata": {},
   "source": [
    "# PCA Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1606189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "cluster_0_pca = pca.fit_transform(cluster_0_df[common_columns])\n",
    "ev_cluster_0 = pca.explained_variance_ratio_\n",
    "\n",
    "nuwee_site1_pca = pca.transform(nuwee_site1_df[common_columns])\n",
    "ev_nuwee_site1 = pca.explained_variance_ratio_\n",
    "nuwee_site2_pca = pca.transform(nuwee_site2_df[common_columns])\n",
    "ev_nuwee_site2 = pca.explained_variance_ratio_\n",
    "nuwee_site3_pca = pca.transform(nuwee_site3_df[common_columns])\n",
    "ev_nuwee_site3 = pca.explained_variance_ratio_\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cluster_0_pca[:, 0],\n",
    "        y=cluster_0_pca[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='blue',\n",
    "            size=12\n",
    "        ),\n",
    "        name=f'Milan Custer 2'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site1_pca[:, 0],\n",
    "        y=nuwee_site1_pca[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='green',\n",
    "            size=12\n",
    "        ),\n",
    "        name=f'Nuwee Site 1'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site2_pca[:, 0],\n",
    "        y=nuwee_site2_pca[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='orange',\n",
    "            size=12\n",
    "        ),\n",
    "        name=f'Nuwee Site 2'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site3_pca[:, 0],\n",
    "        y=nuwee_site3_pca[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='purple',\n",
    "            size=12\n",
    "        ),\n",
    "        name=f'Nuwee Site 3'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='PCA Visualization of Clusters Across Sites',\n",
    "    xaxis_title='Principal Component 1',\n",
    "    yaxis_title='Principal Component 2',\n",
    "    legend_title='Sites',\n",
    "    legend=dict(\n",
    "        x=0.01,\n",
    "        y=0.25,\n",
    "        xanchor='left',\n",
    "        yanchor='top',\n",
    "        bgcolor='rgba(255, 255, 255, 0.7)',\n",
    "    ),\n",
    "    font=dict(\n",
    "        family=\"Arial, sans-serif\",\n",
    "        size=20,\n",
    "        color=\"black\"\n",
    "    ),\n",
    "    margin=dict(\n",
    "        l=30,\n",
    "        r=10,\n",
    "        b=20,\n",
    "        t=40,\n",
    "        pad=4\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.write_image(os.path.join(nuwee_figures_folder, \"pca.png\"), scale=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case without the sampling point 1 from nuwee site 3\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "cluster_0_pca = pca.fit_transform(cluster_0_df[common_columns])\n",
    "ev_cluster_0 = pca.explained_variance_ratio_\n",
    "\n",
    "nuwee_site1_pca = pca.transform(nuwee_site1_df[common_columns])\n",
    "ev_nuwee_site1 = pca.explained_variance_ratio_\n",
    "nuwee_site2_pca = pca.transform(nuwee_site2_df[common_columns])\n",
    "ev_nuwee_site2 = pca.explained_variance_ratio_\n",
    "\n",
    "df = nuwee_site3_df[nuwee_site3_df['Sampling Point'] != 1].copy()\n",
    "\n",
    "\n",
    "nuwee_site3_pca = pca.transform(df[common_columns])\n",
    "ev_nuwee_site3 = pca.explained_variance_ratio_\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=cluster_0_pca[:, 0],\n",
    "        y=cluster_0_pca[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='blue',\n",
    "            size=12\n",
    "        ),\n",
    "        name=f'Milan Custer 2'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site1_pca[:, 0],\n",
    "        y=nuwee_site1_pca[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='green',\n",
    "            size=12\n",
    "        ),\n",
    "        name=f'Nuwee Site 1'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site2_pca[:, 0],\n",
    "        y=nuwee_site2_pca[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='orange',\n",
    "            size=12\n",
    "        ),\n",
    "        name=f'Nuwee Site 2'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site3_pca[:, 0],\n",
    "        y=nuwee_site3_pca[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color='purple',\n",
    "            size=12\n",
    "        ),\n",
    "        name=f'Nuwee Site 3'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='PCA Visualization of Clusters Across Sites',\n",
    "    xaxis_title='Principal Component 1',\n",
    "    yaxis_title='Principal Component 2',\n",
    "    legend_title='Sites',\n",
    "    legend=dict(\n",
    "        x=0.6,\n",
    "        y=1.0,\n",
    "        xanchor='left',\n",
    "        yanchor='top',\n",
    "        bgcolor='rgba(255, 255, 255, 0.7)',\n",
    "    ),\n",
    "    font=dict(\n",
    "        family=\"Arial, sans-serif\",\n",
    "        size=20,\n",
    "        color=\"black\"\n",
    "    ),\n",
    "    margin=dict(\n",
    "        l=30,\n",
    "        r=10,\n",
    "        b=20,\n",
    "        t=40,\n",
    "        pad=4\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.write_image(os.path.join(nuwee_figures_folder, \"pca.png\"), scale=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8418aed0",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464acf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the month as a feature to give time context\n",
    "cluster_0_df['Month'] = cluster_0_df[\"DateTime\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d63cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df['Month'] = nuwee_site1_df['DateTime'].dt.month\n",
    "nuwee_site2_df['Month'] = nuwee_site2_df['DateTime'].dt.month\n",
    "nuwee_site3_df['Month'] = nuwee_site3_df['DateTime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030175c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns.append(pd.Index(['Month']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# All the data from cluster 0 will be used for training\n",
    "\n",
    "cluster_0_df.set_index('DateTime', inplace=True)\n",
    "X, y = cluster_0_df[common_columns], cluster_0_df['TTHMs']\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8344395",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d206d5",
   "metadata": {},
   "source": [
    "## PLS\n",
    "\n",
    "The Partial Least Squares regression (PLS) is a method which reduces the variables, used to predict, to a smaller set of predictors. These predictors are then used to perform a regression.\n",
    "\n",
    "It projects the predictors (independent variables) and the response variable (dependent variable) into a new space that maximizes the covariance between them. The procedure identifies components (latent variables) that explain the most variance in the predictors while also being predictive of the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfdc344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_pls_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    n_components = params[\"n_components\"]\n",
    "    tol = params[\"tol\"]\n",
    "\n",
    "    model = PLSRegression(\n",
    "        n_components=n_components,\n",
    "        tol=tol,\n",
    "        scale=False,\n",
    "        max_iter=1000,\n",
    "    )\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fab09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"n_components\": trial.suggest_int(\"n_components\", 2, X_cv.shape[1]),\n",
    "        \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-1),\n",
    "        \n",
    "    }\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_pls_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b33105",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/PLS.sqlite3\"):\n",
    "    \n",
    "    study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - PLS\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/PLS.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"Hyperparameter Tuning - PLS\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/PLS.sqlite3\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "\n",
    "pls_study = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f07a2",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689dac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVR from sklearn\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the parameters of the model\n",
    "svr = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c40579",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = [\n",
    "    \"linear\",\n",
    "    \"rbf\",\n",
    "    \"sigmoid\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b71154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_svr_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    kernel = params[\"kernel\"]\n",
    "    C = params[\"C\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    gamma = params[\"gamma\"]\n",
    "\n",
    "    model = SVR(\n",
    "        kernel=kernel,\n",
    "        C=C,\n",
    "        epsilon=epsilon,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ebb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", kernel),\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-6, 1, log=True),\n",
    "        \"epsilon\": trial.suggest_float(\"epsilon\", 1e-6, 1, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-6, 1, log=True),\n",
    "        \n",
    "    }\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_svr_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a165ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/SVR.sqlite3\"):\n",
    "    \n",
    "    study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - SVR\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/SVR.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"Hyperparameter Tuning - SVR\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/SVR.sqlite3\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "\n",
    "svr_study = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f9c06",
   "metadata": {},
   "source": [
    "## QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantnn.qrnn import QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aefa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.linspace(0.01, 0.99, 99)\n",
    "\n",
    "def fit_and_validate_qrnn_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index].to_numpy(), X.iloc[val_index].to_numpy()\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    n_layers = params[\"n_layers\"]\n",
    "    n_units = params[\"n_units\"]\n",
    "    activation = params[\"activation\"]\n",
    "\n",
    "    model = QRNN(\n",
    "        n_inputs=X_tr.shape[1],\n",
    "        quantiles=quantiles,\n",
    "        model=(n_layers, n_units, activation),\n",
    "    )\n",
    "    \n",
    "    n_epochs = 50\n",
    "    optimizer = torch.optim.AdamW(model.model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "    \n",
    "    model.train(\n",
    "        training_data=(np.array(X_tr), np.array(y_tr)),\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        n_epochs=n_epochs,\n",
    "        device=\"cpu\",\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        logger=None,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model.predict(X_val)\n",
    "    \n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred.mean(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\n",
    "    \"elu\",\n",
    "    \"hardshrink\",\n",
    "    \"hardtanh\",\n",
    "    \"prelu\",\n",
    "    \"relu\",\n",
    "    \"selu\",\n",
    "    \"celu\",\n",
    "    \"sigmoid\",\n",
    "    \"softplus\",\n",
    "    \"softmin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780865ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"n_layers\": trial.suggest_int(\"n_layers\", 1, 3),\n",
    "        \"n_units\": trial.suggest_int(\"n_units\", 32, 512, log=True),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", activations),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16]),\n",
    "    }\n",
    "\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_qrnn_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/QRNN.sqlite3\"):\n",
    "    \n",
    "    study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - QRNN\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/QRNN.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"Hyperparameter Tuning - QRNN\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/QRNN.sqlite3\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "\n",
    "qrnn_study = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063e731",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_xgb_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = XGBRegressor(random_state=42, **params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719adcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    eta = trial.suggest_float(\"eta\", 1e-5, 1, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1, log=True)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1, log=True)\n",
    "    learning_rate = trial.suggest_float(\n",
    "        \"learning_rate\", 1e-5, 1, log=True\n",
    "    )\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 1, 500)\n",
    "    updater = trial.suggest_categorical(\n",
    "        \"updater\", [\"shotgun\", \"coord_descent\"]\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gblinear\",\n",
    "        \"eta\": eta,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"updater\": updater,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_xgb_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            params,\n",
    "        )\n",
    "\n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "\n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74caed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/XGB.sqlite3\"):\n",
    "    \n",
    "    study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - XGB\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/XGB.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"Hyperparameter Tuning - XGB\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/XGB.sqlite3\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=100, show_progress_bar=True)\n",
    "\n",
    "xgb_study = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751af02",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the studies\n",
    "\n",
    "best_studies= {\n",
    "    \"PLS\": pls_study.best_trial,\n",
    "    \"SVR\": svr_study.best_trial,\n",
    "    \"QRNN\": qrnn_study.best_trial,\n",
    "    \"XGB\": xgb_study.best_trial,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(\n",
    "    columns=['RMSE'],\n",
    "    index=list(best_studies.keys()),\n",
    ")\n",
    "\n",
    "for model, study in best_studies.items():\n",
    "    comparison_df.loc[model, :] = np.round(study.value, 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd40062",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e93d9e",
   "metadata": {},
   "source": [
    "# Model Prediction with all common features\n",
    "\n",
    "Since all the points were associated to cluster 0, we are going to use the model that performed best on all the features for cluster 0 and we are going to use it on these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/XGB.sqlite3\"):\n",
    "    \n",
    "    xgb_study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - XGB\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/XGB.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"SQLite file not found. Please check the path.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smape(actual, predicted) -> float:\n",
    "\n",
    "    # Convert actual and predicted to numpy\n",
    "    # array data type if not already\n",
    "    if not all([isinstance(actual, np.ndarray), \n",
    "                isinstance(predicted, np.ndarray)]):\n",
    "        actual, predicted = np.array(actual),\n",
    "        np.array(predicted)\n",
    "\n",
    "    return round(\n",
    "        np.mean(\n",
    "            np.abs(predicted - actual) / \n",
    "            ((np.abs(predicted) + np.abs(actual))/2)\n",
    "        )*100, 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cabe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the sites df\n",
    "nuwee_site1_df[common_columns] = scaler.transform(nuwee_site1_df[common_columns])\n",
    "nuwee_site2_df[common_columns] = scaler.transform(nuwee_site2_df[common_columns])\n",
    "nuwee_site3_df[common_columns] = scaler.transform(nuwee_site3_df[common_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e385283",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50\n",
    "\n",
    "milan_preds = []\n",
    "site1_preds = []\n",
    "site2_preds = []\n",
    "site3_preds = []\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    xgb_best_trial = xgb_study.best_trial\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        random_state=42,\n",
    "        objective=\"reg:squarederror\",\n",
    "        booster=\"gblinear\",\n",
    "        eta=xgb_best_trial.params['eta'],\n",
    "        reg_lambda=xgb_best_trial.params['reg_lambda'],\n",
    "        reg_alpha=xgb_best_trial.params['reg_alpha'],\n",
    "        learning_rate=xgb_best_trial.params['learning_rate'],\n",
    "        updater=xgb_best_trial.params['updater'],\n",
    "        n_estimators=xgb_best_trial.params['n_estimators'],\n",
    "        eval_metric=\"rmse\"\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    milan_preds.append(model.predict(X_test))\n",
    "    site1_preds.append(model.predict(nuwee_site1_df[common_columns]))\n",
    "    site2_preds.append(model.predict(nuwee_site2_df[common_columns]))\n",
    "    site3_preds.append(model.predict(nuwee_site3_df[common_columns]))\n",
    "\n",
    "eval_preds = {\n",
    "    \"y_test_milan\": y_test,\n",
    "    \"y_test1\": nuwee_site1_df['TTHMs'],\n",
    "    \"y_test2\": nuwee_site2_df['TTHMs'],\n",
    "    \"y_test3\": nuwee_site3_df['TTHMs'],\n",
    "    \"y_test_mean_milan\": np.mean(milan_preds, axis=0),\n",
    "    \"y_test_mean1\": np.mean(site1_preds, axis=0),\n",
    "    \"y_test_mean2\": np.mean(site2_preds, axis=0),\n",
    "    \"y_test_mean3\": np.mean(site3_preds, axis=0),\n",
    "    \"y_test_lower_milan\": np.quantile(milan_preds, 0.025, axis=0),\n",
    "    \"y_test_lower1\": np.quantile(site1_preds, 0.025, axis=0),\n",
    "    \"y_test_lower2\": np.quantile(site2_preds, 0.025, axis=0),\n",
    "    \"y_test_lower3\": np.quantile(site3_preds, 0.025, axis=0),\n",
    "    \"y_test_upper_milan\": np.quantile(milan_preds, 0.975, axis=0),\n",
    "    \"y_test_upper1\": np.quantile(site1_preds, 0.975, axis=0),\n",
    "    \"y_test_upper2\": np.quantile(site2_preds, 0.975, axis=0),\n",
    "    \"y_test_upper3\": np.quantile(site3_preds, 0.975, axis=0),\n",
    "    \"rmse_milan\": np.sqrt(mean_squared_error(y_test.values, np.mean(milan_preds, axis=0))),\n",
    "    \"rmse1\": np.sqrt(mean_squared_error(nuwee_site1_df['TTHMs'].values, np.mean(site1_preds, axis=0))),\n",
    "    \"rmse2\": np.sqrt(mean_squared_error(nuwee_site2_df['TTHMs'].values, np.mean(site2_preds, axis=0))),\n",
    "    \"rmse3\": np.sqrt(mean_squared_error(nuwee_site3_df['TTHMs'].values, np.mean(site3_preds, axis=0))),\n",
    "    \"mape_milan\": calculate_smape(y_test.values, np.mean(milan_preds, axis=0)),\n",
    "    \"mape1\": calculate_smape(nuwee_site1_df['TTHMs'].values, np.mean(site1_preds, axis=0)),\n",
    "    \"mape2\": calculate_smape(nuwee_site2_df['TTHMs'].values, np.mean(site2_preds, axis=0)),\n",
    "    \"mape3\": calculate_smape(nuwee_site3_df['TTHMs'].values, np.mean(site3_preds, axis=0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a182a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Milan RMSE: \" + str(eval_preds['rmse_milan']))\n",
    "print(\"Site 1 RMSE: \" + str(eval_preds[\"rmse1\"]))\n",
    "print(\"Site 2 RMSE: \" + str(eval_preds[\"rmse2\"]))\n",
    "print(\"Site 3 RMSE: \" + str(eval_preds[\"rmse3\"]))\n",
    "print()\n",
    "print(\"Milan MAPE: \" + str(eval_preds[\"mape_milan\"]))\n",
    "print(\"Site 1 MAPE: \" + str(eval_preds[\"mape1\"]))\n",
    "print(\"Site 2 MAPE: \" + str(eval_preds[\"mape2\"]))\n",
    "print(\"Site 3 MAPE: \" + str(eval_preds[\"mape3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the font size\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 1\n",
    "y_test1 = eval_preds[\"y_test1\"]\n",
    "y_test_mean1 = eval_preds[\"y_test_mean1\"]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Get unique sampling points\n",
    "unique_sampling_points = nuwee_site1_df['Sampling Point'].unique()\n",
    "\n",
    "# Plot each sampling point with a different color\n",
    "for sampling_point in unique_sampling_points:\n",
    "    mask = nuwee_site1_df['Sampling Point'] == sampling_point\n",
    "    plt.scatter(\n",
    "        y_test1[mask], \n",
    "        y_test_mean1[mask], \n",
    "        label=sampling_point\n",
    "    )\n",
    "\n",
    "plt.plot([0, 9], [0, 9], \"--\", color=\"black\")\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.legend(title=\"Sampling Point\", fontsize=12)\n",
    "\n",
    "plt.title(\"Site 1 - True vs Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(nuwee_figures_folder, \"site1_true_vs_predicted.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8133a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 2\n",
    "y_test2 = eval_preds[\"y_test2\"]\n",
    "y_test_mean2 = eval_preds[\"y_test_mean2\"]\n",
    "\n",
    "# Get unique sampling points\n",
    "unique_sampling_points = nuwee_site2_df['Sampling Point'].unique()\n",
    "\n",
    "# Plot each sampling point with a different color\n",
    "plt.figure(figsize=(10, 5))\n",
    "for sampling_point in unique_sampling_points:\n",
    "    mask = nuwee_site2_df['Sampling Point'] == sampling_point\n",
    "    plt.scatter(\n",
    "        y_test2[mask], \n",
    "        y_test_mean2[mask], \n",
    "        label=sampling_point\n",
    "    )\n",
    "\n",
    "plt.plot([0, 14], [0, 14], \"--\", color=\"black\")\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.legend(title=\"Sampling Point\", fontsize=12)\n",
    "plt.title(\"Site 2 - True vs Predicted (by Sampling Point)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(nuwee_figures_folder, \"site2_true_vs_predicted.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 3\n",
    "y_test3 = eval_preds[\"y_test3\"]\n",
    "y_test_mean3 = eval_preds[\"y_test_mean3\"]\n",
    "\n",
    "# Get unique sampling points\n",
    "unique_sampling_points = nuwee_site3_df['Sampling Point'].unique()\n",
    "\n",
    "# Plot each sampling point with a different color\n",
    "plt.figure(figsize=(10, 5))\n",
    "for sampling_point in unique_sampling_points:\n",
    "    mask = nuwee_site3_df['Sampling Point'] == sampling_point\n",
    "    plt.scatter(\n",
    "        y_test3[mask], \n",
    "        y_test_mean3[mask], \n",
    "        label=sampling_point\n",
    "    )\n",
    "\n",
    "plt.plot([0, 14], [0, 14], \"--\", color=\"black\")\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.legend(title=\"Sampling Point\", fontsize=12)\n",
    "plt.title(\"Site 3 - True vs Predicted (by Sampling Point)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(nuwee_figures_folder, \"site3_true_vs_predicted.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 1\n",
    "y_test = eval_preds[\"y_test1\"]\n",
    "y_pred_mean = eval_preds[\"y_test_mean1\"]\n",
    "y_pred_lower = eval_preds[\"y_test_lower1\"]\n",
    "y_pred_upper = eval_preds[\"y_test_upper1\"]\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site1_df['DateTime'],\n",
    "        y=nuwee_site1_df['TTHMs'],\n",
    "        mode=\"markers\",\n",
    "        name=\"True TTHMs\",\n",
    "        line=dict(color=\"black\"),\n",
    "        marker=dict(size=10),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site1_df['DateTime'],\n",
    "        y=y_pred_mean,\n",
    "        mode=\"markers\",\n",
    "        name=\"Predicted TTHMs (95% PI)\",\n",
    "        line=dict(color=\"green\"),\n",
    "        marker=dict(size=10),\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=y_pred_upper,\n",
    "            arrayminus=y_pred_lower,\n",
    "            thickness=2,\n",
    "            width=5,\n",
    "            color=\"green\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# get the legend inside the plot\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=30, b=10),\n",
    "    title=\"Site 1\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"TTHMs (µg/L)\")\n",
    "\n",
    "# fig.update_yaxes(range=[0, 25])\n",
    "\n",
    "# update the overall font\n",
    "fig.update_layout(font=dict(family=\"Arial\", size=18))\n",
    "\n",
    "fig.write_image(os.path.join(nuwee_figures_folder, \"site1_pred_with_CI.png\"), scale=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 2\n",
    "y_test = eval_preds[\"y_test2\"]\n",
    "y_pred_mean = eval_preds[\"y_test_mean2\"]\n",
    "y_pred_lower = eval_preds[\"y_test_lower2\"]\n",
    "y_pred_upper = eval_preds[\"y_test_upper2\"]\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site2_df['DateTime'],\n",
    "        y=nuwee_site2_df['TTHMs'],\n",
    "        mode=\"markers\",\n",
    "        name=\"True TTHMs\",\n",
    "        line=dict(color=\"black\"),\n",
    "        marker=dict(size=10),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site2_df['DateTime'],\n",
    "        y=y_pred_mean,\n",
    "        mode=\"markers\",\n",
    "        name=\"Predicted TTHMs (95% PI)\",\n",
    "        line=dict(color=\"green\"),\n",
    "        marker=dict(size=10),\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=y_pred_upper,\n",
    "            arrayminus=y_pred_lower,\n",
    "            thickness=2,\n",
    "            width=5,\n",
    "            color=\"green\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# get the legend inside the plot\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=30, b=10),\n",
    "    title=\"Site 2\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"TTHMs (µg/L)\")\n",
    "\n",
    "# fig.update_yaxes(range=[0, 25])\n",
    "\n",
    "# update the overall font\n",
    "fig.update_layout(font=dict(family=\"Arial\", size=18))\n",
    "\n",
    "fig.write_image(os.path.join(nuwee_figures_folder, \"site2_pred_with_CI.png\"), scale=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08474aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 3\n",
    "y_test3 = eval_preds[\"y_test3\"]\n",
    "y_test_mean3 = eval_preds[\"y_test_mean3\"]\n",
    "y_test_lower3 = eval_preds[\"y_test_lower3\"]\n",
    "y_test_upper3 = eval_preds[\"y_test_upper3\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site3_df['DateTime'],\n",
    "        y=nuwee_site3_df['TTHMs'],\n",
    "        mode=\"markers\",\n",
    "        name=\"True TTHMs\",\n",
    "        line=dict(color=\"black\"),\n",
    "        marker=dict(size=10),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site3_df['DateTime'],\n",
    "        y=y_test_mean3,\n",
    "        mode=\"markers\",\n",
    "        name=\"Predicted TTHMs (95% PI)\",\n",
    "        line=dict(color=\"green\"),\n",
    "        marker=dict(size=10),\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=y_test_upper3,\n",
    "            arrayminus=y_test_lower3,\n",
    "            thickness=2,\n",
    "            width=5,\n",
    "            color=\"green\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=30, b=10),\n",
    "    title=\"Site 3\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"TTHMs (µg/L)\")\n",
    "\n",
    "# fig.update_yaxes(range=[0, 25])\n",
    "\n",
    "# update the overall font\n",
    "fig.update_layout(font=dict(family=\"Arial\", size=18))\n",
    "\n",
    "fig.write_image(os.path.join(nuwee_figures_folder, \"site3_pred_with_CI.png\"), scale=3)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6968a",
   "metadata": {},
   "source": [
    "# Try Cluster 0 based on Feature Selection\n",
    "\n",
    "In the feature_selection/supply_points.ipynb notebook, we assessed that there can be improvements if we perform feature selection. For cluster 0, the best results were achieved with XGBoost with features: ['Conductivity (uS/cm)', 'TOC (mg/L)', 'Temperature (°C)'].\n",
    "Let's try to use that model with those 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89017ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_xgb_folder = os.path.join(\"..\", \"feature_selection\", \"supply_points_sqlites\", \"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload grab_df and call it df\n",
    "df = pd.read_excel(os.path.join(clean_data_folder, \"modelling_grab.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a467f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552486e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.difference([\"DateTime\", \"Code\", \"TTHMs\", \"Cluster\"], sort=False).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "cluster_name = \"cluster_0\"\n",
    "\n",
    "feature_combinations = []\n",
    "for i in range(1, len(features) + 1):\n",
    "    feature_combinations.extend(combinations(features, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_combinations = [list(comb) for comb in feature_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5853e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the configuration that performed the best\n",
    "xgb_studies = {}\n",
    "\n",
    "    \n",
    "xgb_studies = {}\n",
    "for feature_combination in tqdm_notebook(feature_combinations):\n",
    "    \n",
    "    study_name = f\"Hyperparameter Tuning - XGB_{cluster_name}\" + str(feature_combination)\n",
    "    storage_path = f\"sqlite:///{feature_selection_xgb_folder}/{cluster_name}\" + str(feature_combination).replace('/', '_') + \".sqlite3\"\n",
    "\n",
    "    if os.path.exists(f\"{feature_selection_xgb_folder}/{cluster_name}\" + str(feature_combination).replace('/', '_') + \".sqlite3\"):\n",
    "        \n",
    "        \n",
    "        \n",
    "        study = optuna.load_study(\n",
    "            study_name=study_name,\n",
    "            storage=storage_path,\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        raise FileExistsError(f\"Study with name {study_name} at path {storage_path} does not exists.\")\n",
    "    \n",
    "    xgb_studies[str(feature_combination)] = study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171009e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_study_info = sorted(xgb_studies.items(), key=lambda x: x[1].best_value)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_study_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_study_features = ['Conductivity (uS/cm)', 'TOC (mg/L)', 'Temperature (°C)']\n",
    "best_study = best_study_info[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09242c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['DateTime', 'Cluster', 'Code', 'TTHMs'] + best_study_features]\n",
    "cl_0_df = df[df['Cluster'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_0_df.set_index('DateTime', inplace=True)\n",
    "X_fs, y_fs = cl_0_df[best_study_features], cl_0_df['TTHMs']\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_fs = pd.DataFrame(scaler.fit_transform(X_fs), columns=X_fs.columns, index=X_fs.index)\n",
    "\n",
    "X_train_fs, X_test_fs, y_train_fs, y_test_fs = train_test_split(X_fs, y_fs, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc65502",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50\n",
    "\n",
    "medians = []\n",
    "lower = []\n",
    "upper = []\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        random_state=42,\n",
    "        objective=\"reg:squarederror\",\n",
    "        booster=\"gblinear\",\n",
    "        eta=best_study.best_trial.params[\"eta\"],\n",
    "        reg_lambda=best_study.best_trial.params[\"reg_lambda\"],\n",
    "        reg_alpha=best_study.best_trial.params[\"reg_alpha\"],\n",
    "        learning_rate=best_study.best_trial.params[\"learning_rate\"],\n",
    "        updater=best_study.best_trial.params[\"updater\"],\n",
    "        n_estimators=best_study.best_trial.params[\"n_estimators\"],\n",
    "    )\n",
    "    \n",
    "    # train model\n",
    "    _ = model.fit(X_train_fs, y_train_fs)\n",
    "    \n",
    "    # obtain predictions\n",
    "    y_test_median = model.predict(X_test_fs)\n",
    "    y_test_lower = y_test_median - 1.96 * np.std(y_test_median)\n",
    "    y_test_upper = y_test_median + 1.96 * np.std(y_test_median)\n",
    "\n",
    "    \n",
    "    medians.append(y_test_median)\n",
    "    lower.append(y_test_lower)\n",
    "    upper.append(y_test_upper)\n",
    "\n",
    "eval = {\n",
    "    \"y_test\": y_test_fs,\n",
    "    \"y_test_median\": np.mean(medians, axis=0),\n",
    "    \"y_test_lower\": np.mean(lower, axis=0),\n",
    "    \"y_test_upper\": np.mean(upper, axis=0),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test_fs.values, eval[\"y_test_median\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05274507",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test_fs, eval[\"y_test_median\"], \"o\")\n",
    "plt.plot([0, 14], [0, 14], \"--\")\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "\n",
    "plt.title(f\"{cluster_name} - True vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac49a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_fs = eval[\"y_test\"]\n",
    "y_pred_xgb_median = eval[\"y_test_median\"]\n",
    "y_pred_xgb_lower = eval[\"y_test_lower\"]\n",
    "y_pred_xgb_upper = eval[\"y_test_upper\"]\n",
    "\n",
    "y_test_fs.index = pd.to_datetime(y_test_fs.index)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_test_fs.index,\n",
    "        y=y_test_fs,\n",
    "        mode=\"markers\",\n",
    "        name=\"True TTHMs\",\n",
    "        line=dict(color=\"black\"),\n",
    "        marker=dict(size=10),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=y_test_fs.index,\n",
    "        y=y_pred_xgb_median,\n",
    "        mode=\"markers\",\n",
    "        name=\"Predicted TTHMs (95% PI)\",\n",
    "        line=dict(color=\"green\"),\n",
    "        marker=dict(size=10),\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=y_pred_xgb_upper,\n",
    "            arrayminus=y_pred_xgb_lower,\n",
    "            thickness=2,\n",
    "            width=5,\n",
    "            color=\"green\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# get the legend inside the plot\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=30, b=10),\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"TTHMs (µg/L)\")\n",
    "\n",
    "fig.update_yaxes(range=[0, 25])\n",
    "\n",
    "# update the overall font\n",
    "fig.update_layout(font=dict(family=\"Arial\", size=18))\n",
    "\n",
    "# fig.write_image(\n",
    "#     f\"{cluster_name}.png\",\n",
    "#     scale=3,\n",
    "# )\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
