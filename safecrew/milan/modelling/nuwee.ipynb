{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e26417",
   "metadata": {},
   "source": [
    "Modelling of supply points for soft-sensor for NUWEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6337c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78102c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_folder = os.path.join(\"..\", \"..\", \"utils\")\n",
    "\n",
    "data_folder = os.path.join(\"..\", \"..\", \"data\")\n",
    "clean_data_folder = os.path.join(data_folder, \"Clean Data\")\n",
    "metadata_folder = os.path.join(data_folder, \"Metadata\")\n",
    "plot_folder = os.path.join(data_folder, \"Plots\")\n",
    "\n",
    "sensor_folder = os.path.join(clean_data_folder, \"sensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee64441",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = pd.read_excel(os.path.join(clean_data_folder, \"modelling_grab.xlsx\"))\n",
    "\n",
    "nuwee_site1_df = pd.read_excel(os.path.join(clean_data_folder, 'nuwee', 'Site1_tabular.xlsx'))\n",
    "nuwee_site2_df = pd.read_excel(os.path.join(clean_data_folder, 'nuwee', 'Site2_tabular.xlsx'))\n",
    "nuwee_site3_df = pd.read_excel(os.path.join(clean_data_folder, 'nuwee', 'Site3_tabular.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da37e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6124af",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df['DateTime'] = pd.to_datetime(grab_df['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121719a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df['DateTime'] = pd.to_datetime(nuwee_site1_df['DateTime'])\n",
    "nuwee_site2_df['DateTime'] = pd.to_datetime(nuwee_site2_df['DateTime'])\n",
    "nuwee_site3_df['DateTime'] = pd.to_datetime(nuwee_site3_df['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc0a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = {}\n",
    "for cluster in grab_df['Cluster'].unique():\n",
    "    print(f'Cluster {cluster}')\n",
    "    codes = grab_df[grab_df['Cluster'] == cluster]['Code'].unique().tolist()\n",
    "    codes_dict[cluster] = codes\n",
    "    print(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f3c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = nuwee_site1_df.columns.difference(['DateTime', 'Sampling Point', 'TTHMs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df = grab_df[['DateTime', 'Cluster', 'Code', 'TTHMs'] + common_columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53742f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0_df = grab_df[grab_df['Cluster'] == 0].copy()\n",
    "cluster_1_df = grab_df[grab_df['Cluster'] == 1].copy()\n",
    "cluster_2_df = grab_df[grab_df['Cluster'] == 2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ed74f",
   "metadata": {},
   "source": [
    "# NUWEE Data preprocessing\n",
    "\n",
    "We are going to fill deal with missing values and imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52684440",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we nee to clean the nuwee data by removing the rows with all missing values, then impute the rest\n",
    "nuwee_site1_df.dropna(how='all', subset=common_columns, inplace=True)\n",
    "nuwee_site2_df.dropna(how='all', subset=common_columns, inplace=True)\n",
    "nuwee_site3_df.dropna(how='all', subset=common_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, we need to remove the rows with all missing values in the TTHMs column\n",
    "nuwee_site1_df.dropna(how='all', subset=['TTHMs'], inplace=True)\n",
    "nuwee_site2_df.dropna(how='all', subset=['TTHMs'], inplace=True)\n",
    "nuwee_site3_df.dropna(how='all', subset=['TTHMs'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df.reset_index(drop=True, inplace=True)\n",
    "nuwee_site2_df.reset_index(drop=True, inplace=True)\n",
    "nuwee_site3_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8840244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can impute the missing values in the common columns\n",
    "import miceforest as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f080b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a kernel for each site\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=nuwee_site1_df[common_columns],\n",
    "    variable_schema=common_columns.tolist(),\n",
    "    random_state=42,\n",
    "    mean_match_strategy='shap',\n",
    ")\n",
    "\n",
    "kernel.mice(5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df[common_columns] = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a kernel for each site\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=nuwee_site2_df[common_columns],\n",
    "    variable_schema=common_columns.tolist(),\n",
    "    random_state=42,\n",
    "    mean_match_strategy='shap',\n",
    ")\n",
    "\n",
    "kernel.mice(5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af10c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df[common_columns] = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879179e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a kernel for each site\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=nuwee_site3_df[common_columns],\n",
    "    variable_schema=common_columns.tolist(),\n",
    "    random_state=42,\n",
    "    mean_match_strategy='shap',\n",
    ")\n",
    "\n",
    "kernel.mice(5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df[common_columns] = kernel.complete_data(dataset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check\n",
    "nuwee_site1_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f55c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a170e0",
   "metadata": {},
   "source": [
    "# Clustering based on Mahalanobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and covariance for each cluster\n",
    "cluster_0_mean = cluster_0_df[common_columns].mean()\n",
    "cluster_0_cov = cluster_0_df[common_columns].cov()\n",
    "cluster_1_mean = cluster_1_df[common_columns].mean()\n",
    "cluster_1_cov = cluster_1_df[common_columns].cov()\n",
    "cluster_2_mean = cluster_2_df[common_columns].mean()\n",
    "cluster_2_cov = cluster_2_df[common_columns].cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0a63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_stats = {\n",
    "    0: {\n",
    "        'mean': cluster_0_mean,\n",
    "        'cov': cluster_0_cov\n",
    "    },\n",
    "    1: {\n",
    "        'mean': cluster_1_mean,\n",
    "        'cov': cluster_1_cov\n",
    "    },\n",
    "    2: {\n",
    "        'mean': cluster_2_mean,\n",
    "        'cov': cluster_2_cov\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "def assign_cluster(row, clusters_stats):\n",
    "    distances = {}\n",
    "    for cluster, stats in clusters_stats.items():\n",
    "        mean = stats['mean']\n",
    "        cov = stats['cov']\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "        distance = mahalanobis(row[common_columns], mean, inv_cov)\n",
    "        distances[cluster] = distance\n",
    "    return min(distances, key=distances.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b170b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuwee site 1\n",
    "\n",
    "nuwee_site1_df['Cluster'] = -1\n",
    "for i, row in nuwee_site1_df.iterrows():\n",
    "    cluster = assign_cluster(row, clusters_stats)\n",
    "    nuwee_site1_df.at[i, 'Cluster'] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site1_df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuwee site 2\n",
    "nuwee_site2_df['Cluster'] = -1\n",
    "\n",
    "for i, row in nuwee_site2_df.iterrows():\n",
    "    cluster = assign_cluster(row, clusters_stats)\n",
    "    nuwee_site2_df.at[i, 'Cluster'] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764033cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site2_df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuwee site 3\n",
    "nuwee_site3_df['Cluster'] = -1\n",
    "\n",
    "for i, row in nuwee_site3_df.iterrows():\n",
    "    cluster = assign_cluster(row, clusters_stats)\n",
    "    nuwee_site3_df.at[i, 'Cluster'] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuwee_site3_df['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09cc7f0",
   "metadata": {},
   "source": [
    "PCA per far vedere cluster originiale, altri cluster (anche solo centroide) e i punti dei 3 siti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8418aed0",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464acf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea46cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# All the data from cluster 0 will be used for training\n",
    "\n",
    "cluster_0_df.set_index('DateTime', inplace=True)\n",
    "X, y = cluster_0_df[common_columns], cluster_0_df['TTHMs']\n",
    "\n",
    "# scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d206d5",
   "metadata": {},
   "source": [
    "## PLS\n",
    "\n",
    "The Partial Least Squares regression (PLS) is a method which reduces the variables, used to predict, to a smaller set of predictors. These predictors are then used to perform a regression.\n",
    "\n",
    "It projects the predictors (independent variables) and the response variable (dependent variable) into a new space that maximizes the covariance between them. The procedure identifies components (latent variables) that explain the most variance in the predictors while also being predictive of the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfdc344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_pls_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    n_components = params[\"n_components\"]\n",
    "    tol = params[\"tol\"]\n",
    "\n",
    "    model = PLSRegression(\n",
    "        n_components=n_components,\n",
    "        tol=tol,\n",
    "        scale=False,\n",
    "        max_iter=1000,\n",
    "    )\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fab09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"n_components\": trial.suggest_int(\"n_components\", 2, X_cv.shape[1]),\n",
    "        \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-1),\n",
    "        \n",
    "    }\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_pls_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b33105",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/PLS.sqlite3\"):\n",
    "    \n",
    "    study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - PLS\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/PLS.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"Hyperparameter Tuning - PLS\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/PLS.sqlite3\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=100, show_progress_bar=True)\n",
    "\n",
    "pls_study = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f07a2",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689dac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SVR from sklearn\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the parameters of the model\n",
    "svr = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c40579",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = [\n",
    "    \"linear\",\n",
    "    \"rbf\",\n",
    "    \"sigmoid\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b71154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_svr_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    kernel = params[\"kernel\"]\n",
    "    C = params[\"C\"]\n",
    "    epsilon = params[\"epsilon\"]\n",
    "    gamma = params[\"gamma\"]\n",
    "\n",
    "    model = SVR(\n",
    "        kernel=kernel,\n",
    "        C=C,\n",
    "        epsilon=epsilon,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ebb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"kernel\": trial.suggest_categorical(\"kernel\", kernel),\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-6, 1, log=True),\n",
    "        \"epsilon\": trial.suggest_float(\"epsilon\", 1e-6, 1, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-6, 1, log=True),\n",
    "        \n",
    "    }\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_svr_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a165ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/SVR.sqlite3\"):\n",
    "    \n",
    "    study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - SVR\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/SVR.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"Hyperparameter Tuning - SVR\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/SVR.sqlite3\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=100, show_progress_bar=True)\n",
    "\n",
    "svr_study = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f9c06",
   "metadata": {},
   "source": [
    "## QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantnn.qrnn import QRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aefa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.linspace(0.01, 0.99, 99)\n",
    "\n",
    "def fit_and_validate_qrnn_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index].to_numpy(), X.iloc[val_index].to_numpy()\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    n_layers = params[\"n_layers\"]\n",
    "    n_units = params[\"n_units\"]\n",
    "    activation = params[\"activation\"]\n",
    "\n",
    "    model = QRNN(\n",
    "        n_inputs=X_tr.shape[1],\n",
    "        quantiles=quantiles,\n",
    "        model=(n_layers, n_units, activation),\n",
    "    )\n",
    "    \n",
    "    n_epochs = 50\n",
    "    optimizer = torch.optim.AdamW(model.model.parameters())\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "    \n",
    "    model.train(\n",
    "        training_data=(np.array(X_tr), np.array(y_tr)),\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        n_epochs=n_epochs,\n",
    "        device=\"cpu\",\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        logger=None,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model.predict(X_val)\n",
    "    \n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred.mean(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181c8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\n",
    "    \"elu\",\n",
    "    \"hardshrink\",\n",
    "    \"hardtanh\",\n",
    "    \"prelu\",\n",
    "    \"relu\",\n",
    "    \"selu\",\n",
    "    \"celu\",\n",
    "    \"sigmoid\",\n",
    "    \"softplus\",\n",
    "    \"softmin\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780865ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    \n",
    "    config= {\n",
    "        \n",
    "        \"n_layers\": trial.suggest_int(\"n_layers\", 1, 3),\n",
    "        \"n_units\": trial.suggest_int(\"n_units\", 32, 512, log=True),\n",
    "        \"activation\": trial.suggest_categorical(\"activation\", activations),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16]),\n",
    "    }\n",
    "\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_qrnn_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/QRNN.sqlite3\"):\n",
    "    \n",
    "    study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - QRNN\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/QRNN.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"Hyperparameter Tuning - QRNN\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/QRNN.sqlite3\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=100, show_progress_bar=True)\n",
    "\n",
    "qrnn_study = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063e731",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_xgb_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X.iloc[train_index, :], X.iloc[val_index, :]\n",
    "    y_tr, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = XGBRegressor(random_state=42, **params)\n",
    "\n",
    "    # train model\n",
    "    _ = model.fit(X_tr, y_tr)\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719adcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    eta = trial.suggest_float(\"eta\", 1e-5, 1, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1, log=True)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 1e-8, 1, log=True)\n",
    "    learning_rate = trial.suggest_float(\n",
    "        \"learning_rate\", 1e-5, 1, log=True\n",
    "    )\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 1, 500)\n",
    "    updater = trial.suggest_categorical(\n",
    "        \"updater\", [\"shotgun\", \"coord_descent\"]\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"booster\": \"gblinear\",\n",
    "        \"eta\": eta,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"updater\": updater,\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "    }\n",
    "\n",
    "    cv = LeaveOneOut()\n",
    "    cv_rmse = np.zeros((cv.get_n_splits(X_cv)))\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_cv, y_cv)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_xgb_model(\n",
    "            X_cv,\n",
    "            y_cv,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            params,\n",
    "        )\n",
    "\n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "\n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74caed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/XGB.sqlite3\"):\n",
    "    \n",
    "    study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - XGB\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/XGB.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"Hyperparameter Tuning - XGB\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/XGB.sqlite3\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=100, show_progress_bar=True)\n",
    "\n",
    "xgb_study = study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751af02",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the studies\n",
    "\n",
    "best_studies= {\n",
    "    \"PLS\": pls_study.best_trial,\n",
    "    \"SVR\": svr_study.best_trial,\n",
    "    \"QRNN\": qrnn_study.best_trial,\n",
    "    \"XGB\": xgb_study.best_trial,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame(\n",
    "    columns=['RMSE'],\n",
    "    index=list(best_studies.keys()),\n",
    ")\n",
    "\n",
    "for model, study in best_studies.items():\n",
    "    comparison_df.loc[model, :] = np.round(study.value, 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd40062",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e93d9e",
   "metadata": {},
   "source": [
    "# Model Prediction with all common features\n",
    "\n",
    "Since all the points were associated to cluster 0, we are going to use the model that performed best on all the features for cluster 0 and we are going to use it on these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"nuwee_sqlites/XGB.sqlite3\"):\n",
    "    \n",
    "    xgb_study = optuna.load_study(\n",
    "        study_name=f\"Hyperparameter Tuning - XGB\",\n",
    "        storage=f\"sqlite:///nuwee_sqlites/XGB.sqlite3\",\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"SQLite file not found. Please check the path.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cabe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the sites df\n",
    "nuwee_site1_df[common_columns] = scaler.transform(nuwee_site1_df[common_columns])\n",
    "nuwee_site2_df[common_columns] = scaler.transform(nuwee_site2_df[common_columns])\n",
    "nuwee_site3_df[common_columns] = scaler.transform(nuwee_site3_df[common_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e385283",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50\n",
    "\n",
    "site1_preds = []\n",
    "site2_preds = []\n",
    "site3_preds = []\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "\n",
    "    xgb_best_trial = xgb_study.best_trial\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        random_state=42,\n",
    "        objective=\"reg:squarederror\",\n",
    "        booster=\"gblinear\",\n",
    "        eta=xgb_best_trial.params[\"eta\"],\n",
    "        reg_lambda=xgb_best_trial.params[\"reg_lambda\"],\n",
    "        reg_alpha=xgb_best_trial.params[\"reg_alpha\"],\n",
    "        learning_rate=xgb_best_trial.params[\"learning_rate\"],\n",
    "        updater=xgb_best_trial.params[\"updater\"],\n",
    "        n_estimators=xgb_best_trial.params[\"n_estimators\"],\n",
    "    )\n",
    "    \n",
    "    xgb.fit(X, y)\n",
    "    \n",
    "    site1_preds.append(xgb.predict(nuwee_site1_df[common_columns]))\n",
    "    site2_preds.append(xgb.predict(nuwee_site2_df[common_columns]))\n",
    "    site3_preds.append(xgb.predict(nuwee_site3_df[common_columns]))\n",
    "\n",
    "eval_preds = {\n",
    "    \"y_test1\": nuwee_site1_df['TTHMs'].values,\n",
    "    \"y_test2\": nuwee_site2_df['TTHMs'].values,\n",
    "    \"y_test3\": nuwee_site3_df['TTHMs'].values,\n",
    "    \"y_test_mean1\": np.mean(site1_preds, axis=0),\n",
    "    \"y_test_mean2\": np.mean(site2_preds, axis=0),\n",
    "    \"y_test_mean3\": np.mean(site3_preds, axis=0),\n",
    "    \"y_test_lower1\": np.quantile(site1_preds, 0.025, axis=0),\n",
    "    \"y_test_lower2\": np.quantile(site2_preds, 0.025, axis=0),\n",
    "    \"y_test_lower3\": np.quantile(site3_preds, 0.025, axis=0),\n",
    "    \"y_test_upper1\": np.quantile(site1_preds, 0.975, axis=0),\n",
    "    \"y_test_upper2\": np.quantile(site2_preds, 0.975, axis=0),\n",
    "    \"y_test_upper3\": np.quantile(site3_preds, 0.975, axis=0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06bad8",
   "metadata": {},
   "source": [
    "mettere colori diversi per i vari punti di campionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 1\n",
    "y_test1 = eval_preds[\"y_test1\"]\n",
    "y_test_mean1 = eval_preds[\"y_test_mean1\"]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test1, y_test_mean1, \"o\")\n",
    "plt.plot([0, 14], [0, 14], \"--\")\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "\n",
    "plt.title(\"Site 1 - True vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8133a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 2\n",
    "y_test2 = eval_preds[\"y_test2\"]\n",
    "y_test_mean2 = eval_preds[\"y_test_mean2\"]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test2, y_test_mean2, \"o\")\n",
    "plt.plot([0, 14], [0, 14], \"--\")\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Site 2 - True vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 3\n",
    "y_test3 = eval_preds[\"y_test3\"]\n",
    "y_test_mean3 = eval_preds[\"y_test_mean3\"]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test3, y_test_mean3, \"o\")\n",
    "plt.plot([0, 14], [0, 14], \"--\")\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Site 3 - True vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 1\n",
    "y_test = eval_preds[\"y_test1\"]\n",
    "y_pred_mean = eval_preds[\"y_test_mean1\"]\n",
    "y_pred_lower = eval_preds[\"y_test_lower1\"]\n",
    "y_pred_upper = eval_preds[\"y_test_upper1\"]\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site1_df['DateTime'],\n",
    "        y=nuwee_site1_df['TTHMs'],\n",
    "        mode=\"markers\",\n",
    "        name=\"True TTHMs\",\n",
    "        line=dict(color=\"black\"),\n",
    "        marker=dict(size=10),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site1_df['DateTime'],\n",
    "        y=y_pred_mean,\n",
    "        mode=\"markers\",\n",
    "        name=\"Predicted TTHMs (95% PI)\",\n",
    "        line=dict(color=\"green\"),\n",
    "        marker=dict(size=10),\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=y_pred_upper,\n",
    "            arrayminus=y_pred_lower,\n",
    "            thickness=2,\n",
    "            width=5,\n",
    "            color=\"green\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# get the legend inside the plot\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=30, b=10),\n",
    "    title=\"Site 1\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"TTHMs (µg/L)\")\n",
    "\n",
    "# fig.update_yaxes(range=[0, 25])\n",
    "\n",
    "# update the overall font\n",
    "fig.update_layout(font=dict(family=\"Arial\", size=18))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 2\n",
    "y_test = eval_preds[\"y_test2\"]\n",
    "y_pred_mean = eval_preds[\"y_test_mean2\"]\n",
    "y_pred_lower = eval_preds[\"y_test_lower2\"]\n",
    "y_pred_upper = eval_preds[\"y_test_upper2\"]\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site2_df['DateTime'],\n",
    "        y=nuwee_site2_df['TTHMs'],\n",
    "        mode=\"markers\",\n",
    "        name=\"True TTHMs\",\n",
    "        line=dict(color=\"black\"),\n",
    "        marker=dict(size=10),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site2_df['DateTime'],\n",
    "        y=y_pred_mean,\n",
    "        mode=\"markers\",\n",
    "        name=\"Predicted TTHMs (95% PI)\",\n",
    "        line=dict(color=\"green\"),\n",
    "        marker=dict(size=10),\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=y_pred_upper,\n",
    "            arrayminus=y_pred_lower,\n",
    "            thickness=2,\n",
    "            width=5,\n",
    "            color=\"green\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# get the legend inside the plot\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=30, b=10),\n",
    "    title=\"Site 2\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"TTHMs (µg/L)\")\n",
    "\n",
    "# fig.update_yaxes(range=[0, 25])\n",
    "\n",
    "# update the overall font\n",
    "fig.update_layout(font=dict(family=\"Arial\", size=18))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08474aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 3\n",
    "y_test3 = eval_preds[\"y_test3\"]\n",
    "y_test_mean3 = eval_preds[\"y_test_mean3\"]\n",
    "y_test_lower3 = eval_preds[\"y_test_lower3\"]\n",
    "y_test_upper3 = eval_preds[\"y_test_upper3\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site3_df['DateTime'],\n",
    "        y=nuwee_site3_df['TTHMs'],\n",
    "        mode=\"markers\",\n",
    "        name=\"True TTHMs\",\n",
    "        line=dict(color=\"black\"),\n",
    "        marker=dict(size=10),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=nuwee_site3_df['DateTime'],\n",
    "        y=y_test_mean3,\n",
    "        mode=\"markers\",\n",
    "        name=\"Predicted TTHMs (95% PI)\",\n",
    "        line=dict(color=\"green\"),\n",
    "        marker=dict(size=10),\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            symmetric=False,\n",
    "            array=y_test_upper3,\n",
    "            arrayminus=y_test_lower3,\n",
    "            thickness=2,\n",
    "            width=5,\n",
    "            color=\"green\",\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1,\n",
    "        xanchor=\"right\",\n",
    "        x=1,\n",
    "    ),\n",
    "    margin=dict(l=10, r=10, t=30, b=10),\n",
    "    title=\"Site 3\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "fig.update_yaxes(title_text=\"TTHMs (µg/L)\")\n",
    "\n",
    "# fig.update_yaxes(range=[0, 25])\n",
    "\n",
    "# update the overall font\n",
    "fig.update_layout(font=dict(family=\"Arial\", size=18))\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
