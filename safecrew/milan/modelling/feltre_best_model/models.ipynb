{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_lstm_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X[train_index], X[val_index]\n",
    "    y_tr, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(params[\"window_size\"], X_tr.shape[-1])))\n",
    "    model.add(LSTM(units=params[\"n_units_1\"], return_sequences=False, seed=42))\n",
    "    model.add(Dropout(params[\"dropout_1\"]))\n",
    "    # model.add(LSTM(units=params[\"n_units_2\"], seed=42))\n",
    "    # model.add(Dropout(params[\"dropout_2\"]))\n",
    "    model.add(Dense(params[\"n_neurons\"]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params[\"learning_rate\"]),\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError()],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "    \n",
    "    _ = model.fit(X_tr, y_tr, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0, batch_size=params[\"batch_size\"])\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.squeeze(y_val_pred)\n",
    "    \n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_units_1\": trial.suggest_categorical(\"n_units_1\", [20, 40, 60]),\n",
    "        # \"n_units_2\": trial.suggest_categorical(\"n_units_2\", [20, 40, 60]),\n",
    "        \"n_neurons\": trial.suggest_categorical(\"n_neurons\", [20, 40, 60]),        \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True),\n",
    "        \"window_size\": trial.suggest_int(\"window_size\", 1, 24, step=1),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "        \"dropout_1\": trial.suggest_float(\"dropout_1\", 0.1, 0.5),\n",
    "        # \"dropout_2\": trial.suggest_float(\"dropout_2\", 0.1, 0.5),\n",
    "    }\n",
    "    \n",
    "    window_size = config[\"window_size\"]\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(X_cv, y_cv, test_size=0.2, shuffle=False, random_state=42)\n",
    "    \n",
    "    X_train_seq, y_train_seq, _ = create_sequences(X_train, y_train, window_size)\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_train_seq, y_train_seq)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_lstm_model(\n",
    "            X_train_seq,\n",
    "            y_train_seq,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_studies = {}\n",
    "\n",
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable == 'HNAC (1_mL)':\n",
    "    \n",
    "        if os.path.exists(f\"{feltre_sqlites_folder}/LSTM - {target_variable}.sqlite3\"):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=\"Hyperparameter Tuning - LSTM - \" + target_variable,\n",
    "            storage=f\"sqlite:///{feltre_sqlites_folder}/LSTM - {target_variable}.sqlite3\",\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=f\"sqlite:///{feltre_sqlites_folder}/LSTM - {target_variable}.sqlite3\",\n",
    "                study_name=\"Hyperparameter Tuning - LSTM - \" + target_variable,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            study.optimize(lambda trial: objective(trial, X.copy(), y.copy()), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        lstm_studies[target_variable] = study  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_gru_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X[train_index], X[val_index]\n",
    "    y_tr, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(params[\"window_size\"], X_tr.shape[-1])))\n",
    "    model.add(GRU(units=params[\"n_units_1\"], return_sequences=False, seed=42))\n",
    "    model.add(Dropout(params[\"dropout_1\"]))\n",
    "    # model.add(GRU(units=params[\"n_units_2\"], seed=42))\n",
    "    # model.add(Dropout(params[\"dropout_2\"]))\n",
    "    model.add(Dense(params[\"n_neurons\"]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params[\"learning_rate\"]),\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError()],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "    \n",
    "    _ = model.fit(X_tr, y_tr, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0, batch_size=params[\"batch_size\"])\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.squeeze(y_val_pred)\n",
    "    \n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_units_1\": trial.suggest_categorical(\"n_units_1\", [20, 40, 60]),\n",
    "        # \"n_units_2\": trial.suggest_categorical(\"n_units_2\", [20, 40, 60]),\n",
    "        \"n_neurons\": trial.suggest_categorical(\"n_neurons\", [20, 40, 60]),        \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True),\n",
    "        \"window_size\": trial.suggest_int(\"window_size\", 1, 24, step=1),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "        \"dropout_1\": trial.suggest_float(\"dropout_1\", 0.1, 0.5),\n",
    "        # \"dropout_2\": trial.suggest_float(\"dropout_2\", 0.1, 0.5),\n",
    "    }\n",
    "    \n",
    "    window_size = config[\"window_size\"]\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(X_cv, y_cv, test_size=0.2, shuffle=False, random_state=42)\n",
    "    \n",
    "    X_train_seq, y_train_seq, _ = create_sequences(X_train, y_train, window_size)\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_train_seq, y_train_seq)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_gru_model(\n",
    "            X_train_seq,\n",
    "            y_train_seq,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_studies = {}\n",
    "\n",
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable == 'HNAC (1_mL)':\n",
    "    \n",
    "        if os.path.exists(f\"{feltre_sqlites_folder}/GRU - {target_variable}.sqlite3\"):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=\"Hyperparameter Tuning - GRU - \" + target_variable,\n",
    "            storage=f\"sqlite:///{feltre_sqlites_folder}/GRU - {target_variable}.sqlite3\",\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=f\"sqlite:///{feltre_sqlites_folder}/GRU - {target_variable}.sqlite3\",\n",
    "                study_name=\"Hyperparameter Tuning - GRU - \" + target_variable,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            study.optimize(lambda trial: objective(trial, X.copy(), y.copy()), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        gru_studies[target_variable] = study  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_validate_bi_lstm_model(\n",
    "    X,\n",
    "    y,\n",
    "    train_index,\n",
    "    val_index,\n",
    "    params,\n",
    "):\n",
    "    X_tr, X_val = X[train_index], X[val_index]\n",
    "    y_tr, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(params[\"window_size\"], X_tr.shape[-1])))\n",
    "    model.add(Bidirectional(LSTM(units=params[\"n_units_1\"], return_sequences=False, seed=42)))\n",
    "    model.add(Dropout(params[\"dropout_1\"]))\n",
    "    # model.add(LSTM(units=params[\"n_units_2\"], seed=42))\n",
    "    # model.add(Dropout(params[\"dropout_2\"]))\n",
    "    model.add(Dense(params[\"n_neurons\"]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=params[\"learning_rate\"]),\n",
    "        loss=MeanSquaredError(),\n",
    "        metrics=[RootMeanSquaredError()],\n",
    "    )\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "    \n",
    "    _ = model.fit(X_tr, y_tr, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0, batch_size=params[\"batch_size\"])\n",
    "\n",
    "    # obtain predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_pred = np.squeeze(y_val_pred)\n",
    "    \n",
    "    # return metrics\n",
    "    return np.sqrt(mean_squared_error(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial, X_cv, y_cv) -> float:\n",
    "    config = {\n",
    "        \"n_units_1\": trial.suggest_categorical(\"n_units_1\", [20, 40, 60]),\n",
    "        # \"n_units_2\": trial.suggest_categorical(\"n_units_2\", [20, 40, 60]),\n",
    "        \"n_neurons\": trial.suggest_categorical(\"n_neurons\", [20, 40, 60]),       \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True),\n",
    "        \"window_size\": trial.suggest_int(\"window_size\", 1, 24, step=1),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "        \"dropout_1\": trial.suggest_float(\"dropout_1\", 0.1, 0.5),\n",
    "        # \"dropout_2\": trial.suggest_float(\"dropout_2\", 0.1, 0.5),\n",
    "    }\n",
    "    \n",
    "    window_size = config[\"window_size\"]\n",
    "    \n",
    "    X_train, _, y_train, _ = train_test_split(X_cv, y_cv, test_size=0.2, shuffle=False, random_state=42)\n",
    "    \n",
    "    X_train_seq, y_train_seq, _ = create_sequences(X_train, y_train, window_size)\n",
    "\n",
    "    n_splits = 5\n",
    "    cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_rmse = [None] * n_splits\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        cv.split(X_train_seq, y_train_seq)\n",
    "    ):\n",
    "        cv_rmse[i] = fit_and_validate_bi_lstm_model(\n",
    "            X_train_seq,\n",
    "            y_train_seq,\n",
    "            train_index,\n",
    "            test_index,\n",
    "            config,\n",
    "        )\n",
    "        \n",
    "    # saving the individual fold holdout metrics\n",
    "    # uncomment this line if you don't want this\n",
    "    # trial.set_user_attr(\"split_rmse\", cv_rmse)\n",
    "    \n",
    "    return np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_lstm_studies = {}\n",
    "\n",
    "for target_variable, (X, y) in lstm_datasets.items():\n",
    "    \n",
    "    if target_variable == 'HNAC (1_mL)':\n",
    "    \n",
    "        if os.path.exists(f\"{feltre_sqlites_folder}/BI_LSTM - {target_variable}.sqlite3\"):\n",
    "                \n",
    "            study = optuna.load_study(\n",
    "            study_name=\"Hyperparameter Tuning - BI_LSTM - \" + target_variable,\n",
    "            storage=f\"sqlite:///{feltre_sqlites_folder}/BI_LSTM - {target_variable}.sqlite3\",\n",
    "            )\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                storage=f\"sqlite:///{feltre_sqlites_folder}/BI_LSTM - {target_variable}.sqlite3\",\n",
    "                study_name=\"Hyperparameter Tuning - BI_LSTM - \" + target_variable,\n",
    "                load_if_exists=True,\n",
    "            )\n",
    "            study.optimize(lambda trial: objective(trial, X.copy(), y.copy()), n_trials=100, show_progress_bar=True)\n",
    "                \n",
    "        bi_lstm_studies[target_variable] = study  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safecrew-3OLHM_8n-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
